{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from extend import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.copy(x)\n",
    "for i in range(tX.shape[1]):\n",
    "    col = tX[:,i]\n",
    "    places_999 = (col == -999.0)\n",
    "    mean_col = col[col!=-999.0].mean()\n",
    "    col[places_999] = mean_col\n",
    "    tX[:,i] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 7\n",
    "tX = build_poly(tX, degree)\n",
    "tX = tX[:,1:]\n",
    "tX = extend(tX, [logabs])\n",
    "#tX = extend(tX,[np.cos,np.sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 240)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(tX,axis = 0)\n",
    "std = np.std(tX,axis = 0)\n",
    "tX = (tX-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240,)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[(np.ones(tX.shape[0]) , tX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 241)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=476.06830313713334, w0=0.5626178017127776, gamma=0.0379906894392927\n",
      "Gradient Descent(1/499): loss=87.69235023138016, w0=0.5292892612352252, gamma=0.03765536342902759\n",
      "Gradient Descent(2/499): loss=26.138745806950695, w0=0.4975098944685961, gamma=0.08795331009766921\n",
      "Gradient Descent(3/499): loss=18.108336614916983, w0=0.42607651208037983, gamma=0.12692081944712452\n",
      "Gradient Descent(4/499): loss=9.165145030032251, w0=0.33206111929660503, gamma=0.24527641543895132\n",
      "Gradient Descent(5/499): loss=4.36334872895896, w0=0.17343470027393984, gamma=0.16683205179557561\n",
      "Gradient Descent(6/499): loss=1.824182143803345, w0=0.09200419263595842, gamma=0.039307556993257116\n",
      "Gradient Descent(7/499): loss=2.730517713969985, w0=0.07601905947819587, gamma=0.03679220433185472\n",
      "Gradient Descent(8/499): loss=1.2365505472297609, w0=0.061644968526408765, gamma=0.0451071719121877\n",
      "Gradient Descent(9/499): loss=1.1580850629421624, w0=0.044670735192827424, gamma=0.11204175325117757\n",
      "Gradient Descent(10/499): loss=1.0960693834984858, w0=0.0044102414622587335, gamma=0.41805653481152544\n",
      "Gradient Descent(11/499): loss=0.9671026814830499, w0=-0.12898083025502038, gamma=0.46362126496307254\n",
      "Gradient Descent(12/499): loss=0.6539425270598072, w0=-0.21506749627921765, gamma=0.06282077645156904\n",
      "Gradient Descent(13/499): loss=0.6938416892779414, w0=-0.22132422597304982, gamma=0.05169543875433213\n",
      "Gradient Descent(14/499): loss=0.5107764876506442, w0=-0.22614946654311568, gamma=0.03748877486849592\n",
      "Gradient Descent(15/499): loss=0.4943959289121595, w0=-0.2294677679594112, gamma=0.03750150350419514\n",
      "Gradient Descent(16/499): loss=0.4878101905939151, w0=-0.2326627547527709, gamma=0.38166798601442337\n",
      "Gradient Descent(17/499): loss=0.4831304215270798, w0=-0.2639600048662925, gamma=1.0038114832426708\n",
      "Gradient Descent(18/499): loss=0.4431576137261042, w0=-0.3148572574015606, gamma=0.18106829453221224\n",
      "Gradient Descent(19/499): loss=0.40293403460792426, w0=-0.3148222646093673, gamma=0.06479077773528887\n",
      "Gradient Descent(20/499): loss=0.4150874172665573, w0=-0.31481201052079355, gamma=0.04984594582775285\n",
      "Gradient Descent(21/499): loss=0.3916704673736866, w0=-0.3148046327952841, gamma=0.03702071952559224\n",
      "Gradient Descent(22/499): loss=0.38829779381838153, w0=-0.3147994264671909, gamma=0.036923533539699224\n",
      "Gradient Descent(23/499): loss=0.38728452498109434, w0=-0.31479442604266916, gamma=0.12407672533202752\n",
      "Gradient Descent(24/499): loss=0.3866588439297623, w0=-0.3147782432036524, gamma=0.2797048342181486\n",
      "Gradient Descent(25/499): loss=0.3846394061141447, w0=-0.31474628882116673, gamma=0.5412975094526726\n",
      "Gradient Descent(26/499): loss=0.38040352239800324, w0=-0.314701746075478, gamma=0.1316886415230353\n",
      "Gradient Descent(27/499): loss=0.3735725313882685, w0=-0.31469677534329255, gamma=0.06748530950794383\n",
      "Gradient Descent(28/499): loss=0.3720181533148448, w0=-0.31469456348768465, gamma=0.04951394457562332\n",
      "Gradient Descent(29/499): loss=0.3710606891864735, w0=-0.31469305016781085, gamma=0.03791357225857549\n",
      "Gradient Descent(30/499): loss=0.3705001227736082, w0=-0.3146919487713802, gamma=0.04434439324740218\n",
      "Gradient Descent(31/499): loss=0.3700968301349883, w0=-0.31469070939914373, gamma=1.2537050540991033\n",
      "Gradient Descent(32/499): loss=0.36963599910882683, w0=-0.31465722366426385, gamma=3.239085420547928\n",
      "Gradient Descent(33/499): loss=0.3579081112651665, w0=-0.314679172730001, gamma=0.06850490542646544\n",
      "Gradient Descent(34/499): loss=0.3501565688037898, w0=-0.31467813332231925, gamma=0.0402494367929276\n",
      "Gradient Descent(35/499): loss=0.3430819676809788, w0=-0.31467756446332856, gamma=0.03623598066411476\n",
      "Gradient Descent(36/499): loss=0.3386373288025373, w0=-0.3146770729410403, gamma=0.03846662399190758\n",
      "Gradient Descent(37/499): loss=0.3384390747725615, w0=-0.3146765700684358, gamma=0.08841770740770874\n",
      "Gradient Descent(38/499): loss=0.33830649811504426, w0=-0.31467545865020163, gamma=0.10713054566353578\n",
      "Gradient Descent(39/499): loss=0.33801853812912686, w0=-0.31467423107681575, gamma=1.7448143332584731\n",
      "Gradient Descent(40/499): loss=0.33767848652834764, w0=-0.31465637971584565, gamma=3.0812735179962463\n",
      "Gradient Descent(41/499): loss=0.33260291633750927, w0=-0.31467985984154745, gamma=0.061354210009982195\n",
      "Gradient Descent(42/499): loss=0.3375485647385217, w0=-0.3146788867724529, gamma=0.05484947712697469\n",
      "Gradient Descent(43/499): loss=0.3264049485988739, w0=-0.3146780702398483, gamma=0.03881541338680456\n",
      "Gradient Descent(44/499): loss=0.32604683568554554, w0=-0.3146775240970199, gamma=0.03660510161361917\n",
      "Gradient Descent(45/499): loss=0.32592933529180473, w0=-0.31467702904545974, gamma=0.05762465393388401\n",
      "Gradient Descent(46/499): loss=0.3258737644465257, w0=-0.314676278250257, gamma=0.1432979030277873\n",
      "Gradient Descent(47/499): loss=0.32578883475314696, w0=-0.31467451880033903, gamma=0.4217204996526099\n",
      "Gradient Descent(48/499): loss=0.32558163421877356, w0=-0.31467008279954234, gamma=0.7924838577025677\n",
      "Gradient Descent(49/499): loss=0.3249893952091371, w0=-0.31466526226588526, gamma=0.09443906518136941\n",
      "Gradient Descent(50/499): loss=0.32397785634376863, w0=-0.31466514305711335, gamma=0.0460511748694608\n",
      "Gradient Descent(51/499): loss=0.3238474865971909, w0=-0.31466509041722984, gamma=0.03638025455130909\n",
      "Gradient Descent(52/499): loss=0.3237669735473265, w0=-0.31466505074697265, gamma=0.03738179762293525\n",
      "Gradient Descent(53/499): loss=0.3237204696593923, w0=-0.3146650114675449, gamma=0.32418704739061\n",
      "Gradient Descent(54/499): loss=0.32367490950590405, w0=-0.3146646835575182, gamma=0.895897660016961\n",
      "Gradient Descent(55/499): loss=0.3232849493630816, w0=-0.314664071145195, gamma=0.1687233353889805\n",
      "Gradient Descent(56/499): loss=0.3222751976492219, w0=-0.31466405913858575, gamma=0.07718513286467772\n",
      "Gradient Descent(57/499): loss=0.3221030253664068, w0=-0.3146640545727078, gamma=0.07552701210283144\n",
      "Gradient Descent(58/499): loss=0.3219980263033772, w0=-0.3146640504497635, gamma=0.05099811490880366\n",
      "Gradient Descent(59/499): loss=0.3219194121956789, w0=-0.31466404787609015, gamma=0.04747846390360053\n",
      "Gradient Descent(60/499): loss=0.321866618287728, w0=-0.314664045602234, gamma=0.10211251724861636\n",
      "Gradient Descent(61/499): loss=0.3218177726228496, w0=-0.31466404094401335, gamma=0.6698498740095554\n",
      "Gradient Descent(62/499): loss=0.3217133820999056, w0=-0.3146640135067788, gamma=0.22919414185712056\n",
      "Gradient Descent(63/499): loss=0.32104648064220226, w0=-0.31466401040739655, gamma=0.03668197521498603\n",
      "Gradient Descent(64/499): loss=0.3208799234983898, w0=-0.3146640100250399, gamma=0.03612299403894189\n",
      "Gradient Descent(65/499): loss=0.32079392350704294, w0=-0.31466400966232216, gamma=0.11168180974870484\n",
      "Gradient Descent(66/499): loss=0.3207604545318831, w0=-0.3146640085814136, gamma=15.005185260885114\n",
      "Gradient Descent(67/499): loss=0.32065754807111646, w0=-0.31466387957365904, gamma=15.142814810614356\n",
      "Gradient Descent(68/499): loss=0.3117033053941022, w0=-0.31466570294830454, gamma=0.07461153005641213\n",
      "Gradient Descent(69/499): loss=0.31270816803209434, w0=-0.3146655758876129, gamma=0.0488905551580045\n",
      "Gradient Descent(70/499): loss=0.3098551376429731, w0=-0.3146654988408467, gamma=0.0384512614271328\n",
      "Gradient Descent(71/499): loss=0.3090398234972233, w0=-0.3146654412079428, gamma=0.036520985361619226\n",
      "Gradient Descent(72/499): loss=0.30895449071706815, w0=-0.31466538857305515, gamma=0.04706920164091644\n",
      "Gradient Descent(73/499): loss=0.3089475946641891, w0=-0.3146653232133168, gamma=0.09746132187482744\n",
      "Gradient Descent(74/499): loss=0.3089400107871944, w0=-0.31466519424972134, gamma=0.20229401016005166\n",
      "Gradient Descent(75/499): loss=0.30892505106255586, w0=-0.31466495265709243, gamma=0.36839583215380256\n",
      "Gradient Descent(76/499): loss=0.3088951845656782, w0=-0.3146646016966146, gamma=0.1683884003192381\n",
      "Gradient Descent(77/499): loss=0.30884292203827746, w0=-0.3146645003753397, gamma=0.05345730213387576\n",
      "Gradient Descent(78/499): loss=0.3088206147266036, w0=-0.3146644736258164, gamma=0.04467830083748729\n",
      "Gradient Descent(79/499): loss=0.30881250986774417, w0=-0.31466445246434505, gamma=0.03916451887149102\n",
      "Gradient Descent(80/499): loss=0.3088064601031729, w0=-0.3146644347432054, gamma=0.08498565577632576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(81/499): loss=0.3088012326329506, w0=-0.31466439779498606, gamma=2.9403271161296622\n",
      "Gradient Descent(82/499): loss=0.30878996070049797, w0=-0.3146632281032305, gamma=2.441699834632492\n",
      "Gradient Descent(83/499): loss=0.30843394258786583, w0=-0.31466511280699255, gamma=0.036436251203959\n",
      "Gradient Descent(84/499): loss=0.3095413503997215, w0=-0.31466507225993595, gamma=0.03614306563822563\n",
      "Gradient Descent(85/499): loss=0.3082014752090866, w0=-0.3146650335046389, gamma=0.05201039119542219\n",
      "Gradient Descent(86/499): loss=0.3081971432715887, w0=-0.3146649797508873, gamma=0.058521112235359614\n",
      "Gradient Descent(87/499): loss=0.30819254046958877, w0=-0.31466492241390814, gamma=0.16160505238817877\n",
      "Gradient Descent(88/499): loss=0.30818757467727337, w0=-0.3146647733447649, gamma=2.7367363921330723\n",
      "Gradient Descent(89/499): loss=0.3081739195918566, w0=-0.3146626568634569, gamma=6.600781193075881\n",
      "Gradient Descent(90/499): loss=0.30795050615390185, w0=-0.31467152251676966, gamma=0.07718453895750121\n",
      "Gradient Descent(91/499): loss=0.30773241911925764, w0=-0.3146709418936617, gamma=0.05322312954927454\n",
      "Gradient Descent(92/499): loss=0.3075273001856725, w0=-0.3146705724235814, gamma=0.03818941995487134\n",
      "Gradient Descent(93/499): loss=0.3074854352546829, w0=-0.3146703214259823, gamma=0.03685888055738401\n",
      "Gradient Descent(94/499): loss=0.30747445573476384, w0=-0.3146700884247615, gamma=0.1173541187098103\n",
      "Gradient Descent(95/499): loss=0.3074708328537305, w0=-0.3146693739213344, gamma=0.2309539074484521\n",
      "Gradient Descent(96/499): loss=0.30746115600646706, w0=-0.31466813278984995, gamma=0.23817062965119387\n",
      "Gradient Descent(97/499): loss=0.3074461271076758, w0=-0.3146671484772321, gamma=0.05979104971626918\n",
      "Gradient Descent(98/499): loss=0.30743175867679945, w0=-0.31466696022560586, gamma=0.04088470267155658\n",
      "Gradient Descent(99/499): loss=0.3074280768779466, w0=-0.31466683919706895, gamma=0.037320233030674374\n",
      "Gradient Descent(100/499): loss=0.3074255939572505, w0=-0.3146667332370313, gamma=0.0865847909272589\n",
      "Gradient Descent(101/499): loss=0.30742334753473544, w0=-0.31466649657901835, gamma=1.6238799595234754\n",
      "Gradient Descent(102/499): loss=0.30741814334824297, w0=-0.3146624424108291, gamma=8.925710247074539\n",
      "Gradient Descent(103/499): loss=0.3073216557906014, w0=-0.3146763448712845, gamma=0.04819585679767536\n",
      "Gradient Descent(104/499): loss=0.3069314750151236, w0=-0.31467574989894415, gamma=0.036589802413408694\n",
      "Gradient Descent(105/499): loss=0.3068421727086537, w0=-0.3146753199719385, gamma=0.036289574547570876\n",
      "Gradient Descent(106/499): loss=0.306829973006181, w0=-0.3146749091744522, gamma=0.10946578829413647\n",
      "Gradient Descent(107/499): loss=0.3068279271716681, w0=-0.3146737149915004, gamma=0.2061702113191031\n",
      "Gradient Descent(108/499): loss=0.306822144589658, w0=-0.31467171204669203, gamma=0.20492736488779384\n",
      "Gradient Descent(109/499): loss=0.3068120527923726, w0=-0.31467013163434704, gamma=0.06589852021219104\n",
      "Gradient Descent(110/499): loss=0.3068024771800393, w0=-0.3146697275677724, gamma=0.0566023053175816\n",
      "Gradient Descent(111/499): loss=0.30679929970917885, w0=-0.31466940337342136, gamma=0.07053389891601766\n",
      "Gradient Descent(112/499): loss=0.30679665623996843, w0=-0.3146690222514158, gamma=0.5012730423193098\n",
      "Gradient Descent(113/499): loss=0.30679337193245054, w0=-0.3146665047249858, gamma=0.5631810085317542\n",
      "Gradient Descent(114/499): loss=0.30677013977045325, w0=-0.31466509410337656, gamma=0.039379017649128434\n",
      "Gradient Descent(115/499): loss=0.306744987237881, w0=-0.3146650510180967, gamma=0.03612299298400501\n",
      "Gradient Descent(116/499): loss=0.3067425240133117, w0=-0.31466501305166034, gamma=0.03796721481969285\n",
      "Gradient Descent(117/499): loss=0.30674087620338586, w0=-0.3146649745883669, gamma=2.436726234739708\n",
      "Gradient Descent(118/499): loss=0.30673915136049884, w0=-0.31466259974844735, gamma=7.678144876389258\n",
      "Gradient Descent(119/499): loss=0.3066297569702248, w0=-0.3146733509730219, gamma=0.09058884257549774\n",
      "Gradient Descent(120/499): loss=0.30638802373405094, w0=-0.3146725038779125, gamma=0.07197531393148689\n",
      "Gradient Descent(121/499): loss=0.30631160035484833, w0=-0.3146718918076058, gamma=0.05684030980269927\n",
      "Gradient Descent(122/499): loss=0.30630274014007836, w0=-0.31467144323400825, gamma=0.058904167788557\n",
      "Gradient Descent(123/499): loss=0.306299773646073, w0=-0.3146710047956655, gamma=0.19277047273454928\n",
      "Gradient Descent(124/499): loss=0.30629731419523243, w0=-0.3146696544751517, gamma=0.23168955340485914\n",
      "Gradient Descent(125/499): loss=0.3062896699896774, w0=-0.3146683443890339, gamma=0.0427071633311061\n",
      "Gradient Descent(126/499): loss=0.30628101863465, w0=-0.3146681588518946, gamma=0.036176848960878076\n",
      "Gradient Descent(127/499): loss=0.306279274638471, w0=-0.31466800839722336, gamma=0.03790393499045982\n",
      "Gradient Descent(128/499): loss=0.30627791006451227, w0=-0.31466785646265666, gamma=1.2720283415056939\n",
      "Gradient Descent(129/499): loss=0.30627648633488214, w0=-0.31466295091477386, gamma=2.0717113358264503\n",
      "Gradient Descent(130/499): loss=0.30622900065196534, w0=-0.3146651242871007, gamma=0.14968291943478124\n",
      "Gradient Descent(131/499): loss=0.30615498122453605, w0=-0.31466495599840133, gamma=0.08537871564842812\n",
      "Gradient Descent(132/499): loss=0.3061488023862736, w0=-0.3146648743752739, gamma=0.07720769163564105\n",
      "Gradient Descent(133/499): loss=0.3061450230398225, w0=-0.3146648068656817, gamma=0.0513516366566576\n",
      "Gradient Descent(134/499): loss=0.30614223617951924, w0=-0.31466476543107963, gamma=0.04289442813654501\n",
      "Gradient Descent(135/499): loss=0.3061403944924993, w0=-0.3146647325977425, gamma=0.057034500280524515\n",
      "Gradient Descent(136/499): loss=0.3061388640764189, w0=-0.31466469081358706, gamma=0.28298741926721377\n",
      "Gradient Descent(137/499): loss=0.30613683393074814, w0=-0.3146644953180174, gamma=0.42810761615226456\n",
      "Gradient Descent(138/499): loss=0.3061267968935602, w0=-0.3146642832625281, gamma=0.04992817068835805\n",
      "Gradient Descent(139/499): loss=0.3061117509719691, w0=-0.31466426911904005, gamma=0.03621797971789737\n",
      "Gradient Descent(140/499): loss=0.30610995524228823, w0=-0.3146642593715784, gamma=0.036883079561301824\n",
      "Gradient Descent(141/499): loss=0.3061086775019954, w0=-0.3146642498046327, gamma=33.361590044614324\n",
      "Gradient Descent(142/499): loss=0.3061073845632117, w0=-0.3146559154517067, gamma=61.513506062841955\n",
      "Gradient Descent(143/499): loss=0.3050757472892225, w0=-0.31515322350285097, gamma=0.19232955477596755\n",
      "Gradient Descent(144/499): loss=0.30771098461507934, w0=-0.31505913136167124, gamma=0.06318333156625645\n",
      "Gradient Descent(145/499): loss=0.30737489131249374, w0=-0.3150341656449345, gamma=0.0391650772559685\n",
      "Gradient Descent(146/499): loss=0.3052542962883613, w0=-0.31501966807829473, gamma=0.036549728535656315\n",
      "Gradient Descent(147/499): loss=0.3042178928065851, w0=-0.31500666850606485, gamma=0.0565059544576822\n",
      "Gradient Descent(148/499): loss=0.30414068641007924, w0=-0.3149873056942644, gamma=0.16441796253175792\n",
      "Gradient Descent(149/499): loss=0.30406640604988466, w0=-0.3149341484284055, gamma=0.2759808580567714\n",
      "Gradient Descent(150/499): loss=0.303945602202229, w0=-0.31485959262942714, gamma=0.21466528405937202\n",
      "Gradient Descent(151/499): loss=0.30388703622451463, w0=-0.31481760567904676, gamma=0.0685803415858152\n",
      "Gradient Descent(152/499): loss=0.3038826873660645, w0=-0.3148070713481443, gamma=0.04237535552441189\n",
      "Gradient Descent(153/499): loss=0.3038713885100563, w0=-0.3148010086483063, gamma=0.03657833003608718\n",
      "Gradient Descent(154/499): loss=0.30386626735400685, w0=-0.31479599710023726, gamma=0.04362327108092902\n",
      "Gradient Descent(155/499): loss=0.303864915935663, w0=-0.3147902389543395, gamma=0.288207523482422\n",
      "Gradient Descent(156/499): loss=0.30386355367364226, w0=-0.31475385593389893, gamma=0.49671022161664086\n",
      "Gradient Descent(157/499): loss=0.3038560080295532, w0=-0.31470922356610465, gamma=0.1488313087834761\n",
      "Gradient Descent(158/499): loss=0.3038479660620628, w0=-0.3147024928814947, gamma=0.07921877456947596\n",
      "Gradient Descent(159/499): loss=0.30384565593421436, w0=-0.314699443521487, gamma=0.05278468138851296\n",
      "Gradient Descent(160/499): loss=0.30384417925173984, w0=-0.31469757264576104, gamma=0.03632754959010323\n",
      "Gradient Descent(161/499): loss=0.3038433851064008, w0=-0.3146963530333001, gamma=0.036847127422213535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(162/499): loss=0.3038428704164282, w0=-0.3146951609164452, gamma=0.722566005043041\n",
      "Gradient Descent(163/499): loss=0.3038423691268678, w0=-0.3146726450874497, gamma=0.8862098412750722\n",
      "Gradient Descent(164/499): loss=0.3038326945573769, w0=-0.3146649837135132, gamma=0.103698646122124\n",
      "Gradient Descent(165/499): loss=0.30382119073615427, w0=-0.3146648817023077, gamma=0.08352745050041964\n",
      "Gradient Descent(166/499): loss=0.3038197569681896, w0=-0.31466480805479713, gamma=0.06857909395837335\n",
      "Gradient Descent(167/499): loss=0.30381866609228236, w0=-0.31466475263817495, gamma=0.038747138872731006\n",
      "Gradient Descent(168/499): loss=0.3038177748601296, w0=-0.31466472347505875, gamma=0.039263549039240515\n",
      "Gradient Descent(169/499): loss=0.3038172712500029, w0=-0.3146646950683128, gamma=0.6106608066039989\n",
      "Gradient Descent(170/499): loss=0.3038167613560694, w0=-0.3146642706088208, gamma=25.789244107505265\n",
      "Gradient Descent(171/499): loss=0.30380884008195597, w0=-0.31465729145225213, gamma=0.3242770742848004\n",
      "Gradient Descent(172/499): loss=0.3034901689768016, w0=-0.3146594668759571, gamma=0.03633594708519916\n",
      "Gradient Descent(173/499): loss=0.3035884457362267, w0=-0.31465963159080484, gamma=0.03617172110580975\n",
      "Gradient Descent(174/499): loss=0.30348466509160166, w0=-0.3146597896031783, gamma=0.05540508616736057\n",
      "Gradient Descent(175/499): loss=0.30348410492234035, w0=-0.3146600228798026, gamma=0.062288757046139724\n",
      "Gradient Descent(176/499): loss=0.30348345906383617, w0=-0.3146602706088057, gamma=0.1378180325354968\n",
      "Gradient Descent(177/499): loss=0.30348276632705806, w0=-0.31466078458423635, gamma=0.15765276916934964\n",
      "Gradient Descent(178/499): loss=0.30348124483986877, w0=-0.3146612915012319, gamma=2.044295772044767\n",
      "Gradient Descent(179/499): loss=0.3034795089682219, w0=-0.31466682844523625, gamma=1.8998800994007305\n",
      "Gradient Descent(180/499): loss=0.30345711255645547, w0=-0.31466145471184703, gamma=0.04135096668345569\n",
      "Gradient Descent(181/499): loss=0.303438595061556, w0=-0.3146615599613945, gamma=0.03693897733999467\n",
      "Gradient Descent(182/499): loss=0.3034362143102322, w0=-0.3146616500934087, gamma=0.04351976814344166\n",
      "Gradient Descent(183/499): loss=0.3034357241597765, w0=-0.3146617523601901, gamma=0.08307220478546486\n",
      "Gradient Descent(184/499): loss=0.3034352369406045, w0=-0.31466193907542295, gamma=0.08731679933085319\n",
      "Gradient Descent(185/499): loss=0.3034343387698731, w0=-0.3146621190275396, gamma=0.21520080343409909\n",
      "Gradient Descent(186/499): loss=0.3034334017662021, w0=-0.31466252381131476, gamma=0.26668390991999485\n",
      "Gradient Descent(187/499): loss=0.30343109456338296, w0=-0.3146629174833555, gamma=0.047618040680739014\n",
      "Gradient Descent(188/499): loss=0.3034282421449457, w0=-0.3146629690300112, gamma=0.036690696612158395\n",
      "Gradient Descent(189/499): loss=0.30342773029177084, w0=-0.3146630068565051, gamma=0.03874935808001872\n",
      "Gradient Descent(190/499): loss=0.30342733776107683, w0=-0.31466304533963607, gamma=2.9424345045048494\n",
      "Gradient Descent(191/499): loss=0.3034269235954305, w0=-0.3146658543240785, gamma=15.609508483587767\n",
      "Gradient Descent(192/499): loss=0.30339561198650905, w0=-0.3146369090182703, gamma=0.08536881870059948\n",
      "Gradient Descent(193/499): loss=0.3032500227465169, w0=-0.3146392217421853, gamma=0.05620061808426651\n",
      "Gradient Descent(194/499): loss=0.30323814073290045, w0=-0.31464061429480333, gamma=0.04483408457765376\n",
      "Gradient Descent(195/499): loss=0.3032343032777642, w0=-0.3146416627708612, gamma=0.03718536165722782\n",
      "Gradient Descent(196/499): loss=0.30323351179648117, w0=-0.3146424933882852, gamma=0.04352691505749171\n",
      "Gradient Descent(197/499): loss=0.3032330963744538, w0=-0.3146434295041379, gamma=0.1370436239190785\n",
      "Gradient Descent(198/499): loss=0.3032326479938494, w0=-0.31464624855752044, gamma=0.15155527858329948\n",
      "Gradient Descent(199/499): loss=0.3032312826297769, w0=-0.3146489388802114, gamma=0.3989609765781085\n",
      "Gradient Descent(200/499): loss=0.30322979942136363, w0=-0.314654947673691, gamma=0.10464393244734385\n",
      "Gradient Descent(201/499): loss=0.30322593177692697, w0=-0.3146558949432499, gamma=0.03661236445555379\n",
      "Gradient Descent(202/499): loss=0.30322493669838707, w0=-0.3146561916880294, gamma=0.03619535032879731\n",
      "Gradient Descent(203/499): loss=0.3032245664992158, w0=-0.3146564743121103, gamma=0.30220801606999087\n",
      "Gradient Descent(204/499): loss=0.3032242184812884, w0=-0.31465874863108917, gamma=1.472480129405017\n",
      "Gradient Descent(205/499): loss=0.30322131655171375, w0=-0.3146664811468607, gamma=0.3854485459754045\n",
      "Gradient Descent(206/499): loss=0.3032072663433265, w0=-0.31466552478701615, gamma=0.06316439227623752\n",
      "Gradient Descent(207/499): loss=0.30320378808506954, w0=-0.31466542847388684, gamma=0.0603240047912455\n",
      "Gradient Descent(208/499): loss=0.30320300949852896, w0=-0.314665342301777, gamma=0.059245742677704175\n",
      "Gradient Descent(209/499): loss=0.30320243563029, w0=-0.3146652627752822, gamma=0.052535972560723\n",
      "Gradient Descent(210/499): loss=0.30320187512323526, w0=-0.31466519643341934, gamma=0.06743466646766022\n",
      "Gradient Descent(211/499): loss=0.3032013783192183, w0=-0.314665115751387, gamma=0.18485888873318068\n",
      "Gradient Descent(212/499): loss=0.3032007408436865, w0=-0.31466490949223835, gamma=0.23389363092427767\n",
      "Gradient Descent(213/499): loss=0.30319899429890607, w0=-0.3146646967645231, gamma=0.04771843067161702\n",
      "Gradient Descent(214/499): loss=0.30319678825421004, w0=-0.31466466351534567, gamma=0.036487751348452155\n",
      "Gradient Descent(215/499): loss=0.3031963369475806, w0=-0.31466463930465205, gamma=0.03981214355044819\n",
      "Gradient Descent(216/499): loss=0.3031959928377474, w0=-0.3146646138520063, gamma=24.091764707170523\n",
      "Gradient Descent(217/499): loss=0.3031956176250847, w0=-0.31464982473674513, gamma=94.10577516726632\n",
      "Gradient Descent(218/499): loss=0.3029758934969416, w0=-0.3159837975577174, gamma=0.06375143834023642\n",
      "Gradient Descent(219/499): loss=0.30326361155229953, w0=-0.31589965856416086, gamma=0.03643645967554305\n",
      "Gradient Descent(220/499): loss=0.30293989568505847, w0=-0.3158546355401811, gamma=0.036173376140785626\n",
      "Gradient Descent(221/499): loss=0.3025789853578541, w0=-0.3158115662324111, gamma=0.18754929848399793\n",
      "Gradient Descent(222/499): loss=0.30255844918606695, w0=-0.3155963409878225, gamma=0.3771972641645875\n",
      "Gradient Descent(223/499): loss=0.30247031948393893, w0=-0.3152446645124856, gamma=0.3271103427017269\n",
      "Gradient Descent(224/499): loss=0.3023656020850258, w0=-0.3150547231401305, gamma=0.09294554856215223\n",
      "Gradient Descent(225/499): loss=0.3023487359520755, w0=-0.315018407162215, gamma=0.07829535541058387\n",
      "Gradient Descent(226/499): loss=0.3023164193078446, w0=-0.31499065872637844, gamma=0.07616161040490829\n",
      "Gradient Descent(227/499): loss=0.30231045229285297, w0=-0.31496577987064517, gamma=0.0633726227580005\n",
      "Gradient Descent(228/499): loss=0.3023064924652807, w0=-0.3149466552878495, gamma=0.05507470682111497\n",
      "Gradient Descent(229/499): loss=0.30230357149348613, w0=-0.3149310881299605, gamma=0.06516194753139014\n",
      "Gradient Descent(230/499): loss=0.3023012801498638, w0=-0.31491368414632837, gamma=0.1250913011502999\n",
      "Gradient Descent(231/499): loss=0.3022988340120154, w0=-0.31488245082981997, gamma=0.15363485469040425\n",
      "Gradient Descent(232/499): loss=0.30229474418503816, w0=-0.3148488891661554, gamma=0.05286028608606952\n",
      "Gradient Descent(233/499): loss=0.3022909433728573, w0=-0.3148391158711933, gamma=0.03728296649274797\n",
      "Gradient Descent(234/499): loss=0.30228956180837346, w0=-0.31483258703151, gamma=0.041037577452462747\n",
      "Gradient Descent(235/499): loss=0.3022887440725942, w0=-0.314825668627569, gamma=0.8768374948524695\n",
      "Gradient Descent(236/499): loss=0.3022879582830491, w0=-0.31468391150083036, gamma=1.075139849254558\n",
      "Gradient Descent(237/499): loss=0.3022765642686378, w0=-0.3146625038377856, gamma=0.05216416018896487\n",
      "Gradient Descent(238/499): loss=0.3022713948067267, w0=-0.3146625818831024, gamma=0.03610473720093428\n",
      "Gradient Descent(239/499): loss=0.3022706654010197, w0=-0.3146626330833361, gamma=0.03611005042809825\n",
      "Gradient Descent(240/499): loss=0.30227036864459966, w0=-0.3146626824422614, gamma=5.331820075019831\n",
      "Gradient Descent(241/499): loss=0.302270187147186, w0=-0.3146697073485974, gamma=5.800325026432001\n",
      "Gradient Descent(242/499): loss=0.30224385223547884, w0=-0.3146366027908312, gamma=0.1607237157080071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(243/499): loss=0.30221779416040373, w0=-0.31464100616984786, gamma=0.10886383031021148\n",
      "Gradient Descent(244/499): loss=0.30221592463456415, w0=-0.31464350936475394, gamma=0.1092293591304246\n",
      "Gradient Descent(245/499): loss=0.3022150496380905, w0=-0.3146457475421873, gamma=0.5343586019801181\n",
      "Gradient Descent(246/499): loss=0.30221448390351585, w0=-0.3146555008925781, gamma=0.6361482945156726\n",
      "Gradient Descent(247/499): loss=0.3022118556903099, w0=-0.31466090757640447, gamma=0.051492453246366386\n",
      "Gradient Descent(248/499): loss=0.30220893317619735, w0=-0.3146610668121644, gamma=0.0401289853718489\n",
      "Gradient Descent(249/499): loss=0.30220863688491517, w0=-0.314661184517457, gamma=0.03720567135267009\n",
      "Gradient Descent(250/499): loss=0.3022084427045082, w0=-0.3146612892688568, gamma=0.05159422858319775\n",
      "Gradient Descent(251/499): loss=0.3022082687431477, w0=-0.31466142912622014, gamma=0.1086263466995507\n",
      "Gradient Descent(252/499): loss=0.30220802790676965, w0=-0.3146617083893331, gamma=0.30944998615147373\n",
      "Gradient Descent(253/499): loss=0.3022075211746196, w0=-0.31466241752391055, gamma=1.0963643926022455\n",
      "Gradient Descent(254/499): loss=0.3022060787232564, w0=-0.31466415247907165, gamma=0.1023557514479805\n",
      "Gradient Descent(255/499): loss=0.3022009840564625, w0=-0.31466413687053574, gamma=0.04004234415743118\n",
      "Gradient Descent(256/499): loss=0.3022005132863143, w0=-0.3146641313893608, gamma=0.03630859006753399\n",
      "Gradient Descent(257/499): loss=0.3022003192889698, w0=-0.3146641266182925, gamma=0.04098780540166386\n",
      "Gradient Descent(258/499): loss=0.3022001509130372, w0=-0.31466412142791556, gamma=0.1775786205568487\n",
      "Gradient Descent(259/499): loss=0.30219996102085384, w0=-0.31466409986244, gamma=0.5815186272544094\n",
      "Gradient Descent(260/499): loss=0.3021991386285413, w0=-0.31466404178247004, gamma=0.3445339827510988\n",
      "Gradient Descent(261/499): loss=0.3021964487887109, w0=-0.3146640273821899, gamma=0.07944525245407243\n",
      "Gradient Descent(262/499): loss=0.3021948621656369, w0=-0.3146640252056983, gamma=0.07216191235267555\n",
      "Gradient Descent(263/499): loss=0.3021944916040924, w0=-0.31466402338580174, gamma=0.050590815942106585\n",
      "Gradient Descent(264/499): loss=0.30219415887672374, w0=-0.31466402220199025, gamma=0.046431770287302\n",
      "Gradient Descent(265/499): loss=0.30219392568593473, w0=-0.31466402117046577, gamma=0.09770158644761355\n",
      "Gradient Descent(266/499): loss=0.30219371172346615, w0=-0.3146640191007168, gamma=0.787303367643222\n",
      "Gradient Descent(267/499): loss=0.3021932616162786, w0=-0.3146640040516918, gamma=0.2946964752809507\n",
      "Gradient Descent(268/499): loss=0.3021896387364714, w0=-0.314664002853568, gamma=0.03670007104266737\n",
      "Gradient Descent(269/499): loss=0.30218830394121876, w0=-0.31466400274833073, gamma=0.03611544644071049\n",
      "Gradient Descent(270/499): loss=0.3021881173947698, w0=-0.31466400264857053, gamma=0.07827625251982746\n",
      "Gradient Descent(271/499): loss=0.30218795178935737, w0=-0.3146640024401602, gamma=27.590875377702332\n",
      "Gradient Descent(272/499): loss=0.3021875929346313, w0=-0.31466393472973647, gamma=133.47374019053078\n",
      "Gradient Descent(273/499): loss=0.3020645502054936, w0=-0.3146726447418296, gamma=0.057404344262528925\n",
      "Gradient Descent(274/499): loss=0.3066738632086812, w0=-0.3146721484953145, gamma=0.056389083011787126\n",
      "Gradient Descent(275/499): loss=0.3015996973653492, w0=-0.314671689008361, gamma=0.04746472470664227\n",
      "Gradient Descent(276/499): loss=0.3015884202978216, w0=-0.31467132405104414, gamma=0.044150814210618325\n",
      "Gradient Descent(277/499): loss=0.30158595393019016, w0=-0.3146710006876211, gamma=0.062397873128877075\n",
      "Gradient Descent(278/499): loss=0.3015851065691593, w0=-0.3146705638587466, gamma=0.10995749017490714\n",
      "Gradient Descent(279/499): loss=0.3015845641236718, w0=-0.3146698421118037, gamma=0.09733613114679134\n",
      "Gradient Descent(280/499): loss=0.30158415506130276, w0=-0.3146692734619071, gamma=0.037294245434661824\n",
      "Gradient Descent(281/499): loss=0.3015839265456007, w0=-0.3146690767916126, gamma=0.0361582889193524\n",
      "Gradient Descent(282/499): loss=0.301583814432845, w0=-0.31466889322301844, gamma=0.0687725398449318\n",
      "Gradient Descent(283/499): loss=0.3015837260495061, w0=-0.3146685567026996, gamma=0.39932760687714985\n",
      "Gradient Descent(284/499): loss=0.3015835582063125, w0=-0.3146667370800349, gamma=0.8388949405401557\n",
      "Gradient Descent(285/499): loss=0.3015825866838972, w0=-0.31466444094592944, gamma=0.30556605316464996\n",
      "Gradient Descent(286/499): loss=0.3015805612584982, w0=-0.3146643062036293, gamma=0.13008432993751973\n",
      "Gradient Descent(287/499): loss=0.30157982825600155, w0=-0.31466426636955047, gamma=0.0730891519729389\n",
      "Gradient Descent(288/499): loss=0.3015795129862785, w0=-0.3146642468998231, gamma=0.03625345840715496\n",
      "Gradient Descent(289/499): loss=0.30157933759297734, w0=-0.3146642379483532, gamma=0.036236217299620364\n",
      "Gradient Descent(290/499): loss=0.30157924974583644, w0=-0.31466422932550775, gamma=3.6765393756063265\n",
      "Gradient Descent(291/499): loss=0.30157916280498553, w0=-0.3146633861508048, gamma=9.165253208348414\n",
      "Gradient Descent(292/499): loss=0.30157037468720016, w0=-0.3146690121084154, gamma=0.12440515954627539\n",
      "Gradient Descent(293/499): loss=0.30154912378219695, w0=-0.3146683885745636, gamma=0.05993734870947284\n",
      "Gradient Descent(294/499): loss=0.30154879126779416, w0=-0.31466812553421847, gamma=0.05559003452583996\n",
      "Gradient Descent(295/499): loss=0.3015484109563755, w0=-0.3146678961948671, gamma=0.04784055588788412\n",
      "Gradient Descent(296/499): loss=0.3015482787715807, w0=-0.31466770979808334, gamma=0.04454499704616517\n",
      "Gradient Descent(297/499): loss=0.3015481665308015, w0=-0.31466754454452833, gamma=0.10196255054492624\n",
      "Gradient Descent(298/499): loss=0.30154806224213687, w0=-0.3146671831323306, gamma=0.6104200594598059\n",
      "Gradient Descent(299/499): loss=0.30154782392302076, w0=-0.3146652400761402, gamma=0.28778701934518697\n",
      "Gradient Descent(300/499): loss=0.30154640496108626, w0=-0.3146648831943812, gamma=0.0365793625508645\n",
      "Gradient Descent(301/499): loss=0.30154576376307557, w0=-0.3146648508871926, gamma=0.03611115329178323\n",
      "Gradient Descent(302/499): loss=0.3015456562289749, w0=-0.31466482016018005, gamma=0.07541198176496486\n",
      "Gradient Descent(303/499): loss=0.3015455730610927, w0=-0.31466475830924234, gamma=1.4806787752971349\n",
      "Gradient Descent(304/499): loss=0.30154539943621633, w0=-0.314663635476557, gamma=2.431919211625068\n",
      "Gradient Descent(305/499): loss=0.3015419983613526, w0=-0.31466452193480954, gamma=0.23487686131707786\n",
      "Gradient Descent(306/499): loss=0.30153645385832456, w0=-0.3146643993411832, gamma=0.12313916568384256\n",
      "Gradient Descent(307/499): loss=0.3015359172457403, w0=-0.31466435016495675, gamma=0.0831877716393794\n",
      "Gradient Descent(308/499): loss=0.3015356312507627, w0=-0.31466432103437514, gamma=0.0466417648353381\n",
      "Gradient Descent(309/499): loss=0.3015354417237451, w0=-0.3146643060601266, gamma=0.03794636671954699\n",
      "Gradient Descent(310/499): loss=0.3015353352712993, w0=-0.3146642944457371, gamma=0.03877053621585603\n",
      "Gradient Descent(311/499): loss=0.30153524910720625, w0=-0.3146642830293871, gamma=0.3031128131790315\n",
      "Gradient Descent(312/499): loss=0.3015351611270794, w0=-0.31466419723540257, gamma=2.6505419489358526\n",
      "Gradient Descent(313/499): loss=0.3015344735004425, w0=-0.3146636744183992, gamma=0.2458257358158288\n",
      "Gradient Descent(314/499): loss=0.3015284788671521, w0=-0.3146637544513704, gamma=0.054452151006786915\n",
      "Gradient Descent(315/499): loss=0.30152795870959886, w0=-0.31466376782127603, gamma=0.04868771651345255\n",
      "Gradient Descent(316/499): loss=0.3015278015593289, w0=-0.31466377912486143, gamma=0.036672337786393956\n",
      "Gradient Descent(317/499): loss=0.30152769068164587, w0=-0.3146637872243671, gamma=0.03690276136754671\n",
      "Gradient Descent(318/499): loss=0.301527608046885, w0=-0.31466379507587033, gamma=0.28074688488448235\n",
      "Gradient Descent(319/499): loss=0.30152752508329, w0=-0.31466385260383806, gamma=0.6056177957345174\n",
      "Gradient Descent(320/499): loss=0.3015268941419691, w0=-0.3146639418612863, gamma=0.2845577999842799\n",
      "Gradient Descent(321/499): loss=0.3015255346331033, w0=-0.31466395840121564, gamma=0.06982488349369624\n",
      "Gradient Descent(322/499): loss=0.3015248978020895, w0=-0.31466396130489016, gamma=0.05836360768679462\n",
      "Gradient Descent(323/499): loss=0.3015247401589485, w0=-0.3146639635624775, gamma=0.053679377649732095\n",
      "Gradient Descent(324/499): loss=0.3015246093491015, w0=-0.3146639655176863, gamma=0.05941501858372546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(325/499): loss=0.30152448911605345, w0=-0.3146639675656403, gamma=0.11851840398346684\n",
      "Gradient Descent(326/499): loss=0.30152435606387284, w0=-0.3146639714080866, gamma=0.2254854114798765\n",
      "Gradient Descent(327/499): loss=0.30152409071830544, w0=-0.31466397785205963, gamma=0.07351830667661086\n",
      "Gradient Descent(328/499): loss=0.30152358614219005, w0=-0.31466397947933245, gamma=0.037461875160260644\n",
      "Gradient Descent(329/499): loss=0.3015234217001863, w0=-0.31466398024756237, gamma=0.03736774362310257\n",
      "Gradient Descent(330/499): loss=0.3015233378629166, w0=-0.3146639809851549, gamma=0.9271136354158375\n",
      "Gradient Descent(331/499): loss=0.30152325430208093, w0=-0.31466399860138755, gamma=277.67961192266034\n",
      "Gradient Descent(332/499): loss=0.30152118260192484, w0=-0.3146643831672407, gamma=1.2196234840648579\n",
      "Gradient Descent(333/499): loss=0.3010240206157481, w0=-0.3146639158312168, gamma=0.03612892576569708\n",
      "Gradient Descent(334/499): loss=0.3048834683660811, w0=-0.31466391887165995, gamma=0.036105018158073536\n",
      "Gradient Descent(335/499): loss=0.3010207770534093, w0=-0.31466392180031866, gamma=0.12578032248394497\n",
      "Gradient Descent(336/499): loss=0.3010199456095039, w0=-0.31466393163462253, gamma=0.13116738852157112\n",
      "Gradient Descent(337/499): loss=0.3010189403365616, w0=-0.31466394060018177, gamma=0.10137173033185809\n",
      "Gradient Descent(338/499): loss=0.30101880468272896, w0=-0.3146639466202926, gamma=0.08299271512593027\n",
      "Gradient Descent(339/499): loss=0.30101870147857873, w0=-0.314663951049313, gamma=0.0820477882810251\n",
      "Gradient Descent(340/499): loss=0.3010186171920024, w0=-0.3146639550645147, gamma=0.16191791928195495\n",
      "Gradient Descent(341/499): loss=0.30101853389086436, w0=-0.31466396233821636, gamma=0.5334117945704919\n",
      "Gradient Descent(342/499): loss=0.30101836953094524, w0=-0.31466398242034477, gamma=0.17803342191127097\n",
      "Gradient Descent(343/499): loss=0.30101782836009117, w0=-0.3146639855477376, gamma=0.05261466884035612\n",
      "Gradient Descent(344/499): loss=0.30101764806474773, w0=-0.3146639863074372, gamma=0.0460158369492929\n",
      "Gradient Descent(345/499): loss=0.3010175945003519, w0=-0.3146639869368985, gamma=0.038212689862135364\n",
      "Gradient Descent(346/499): loss=0.3010175478520628, w0=-0.31466398743556534, gamma=0.059753910850951206\n",
      "Gradient Descent(347/499): loss=0.30101750912547787, w0=-0.3146639881855429, gamma=5.0272449734002285\n",
      "Gradient Descent(348/499): loss=0.30101744857380863, w0=-0.3146640475126972, gamma=12.260885619285263\n",
      "Gradient Descent(349/499): loss=0.30101236542598425, w0=-0.3146634648015764, gamma=0.03687334200972445\n",
      "Gradient Descent(350/499): loss=0.3010028376751316, w0=-0.31466348453564, gamma=0.03610984610904827\n",
      "Gradient Descent(351/499): loss=0.3010000647628022, w0=-0.31466350314849806, gamma=0.0376459679089542\n",
      "Gradient Descent(352/499): loss=0.3010000275548361, w0=-0.31466352185245267, gamma=0.05762427501788282\n",
      "Gradient Descent(353/499): loss=0.3009999903713842, w0=-0.3146635494045914, gamma=0.06977848352949104\n",
      "Gradient Descent(354/499): loss=0.3009999335207818, w0=-0.31466358084552687, gamma=3.6487386107728614\n",
      "Gradient Descent(355/499): loss=0.3009998646925331, w0=-0.31466511018206894, gamma=14.734870518061925\n",
      "Gradient Descent(356/499): loss=0.30099627119079203, w0=-0.3146487515969596, gamma=0.1112513914481717\n",
      "Gradient Descent(357/499): loss=0.3009820989448958, w0=-0.3146504480015368, gamma=0.060014577736601536\n",
      "Gradient Descent(358/499): loss=0.3009819107701182, w0=-0.3146512613182043, gamma=0.05579329034938548\n",
      "Gradient Descent(359/499): loss=0.30098174610894696, w0=-0.3146519720504349, gamma=0.05587791012481022\n",
      "Gradient Descent(360/499): loss=0.30098169033444866, w0=-0.314652644146377, gamma=0.06581220719206166\n",
      "Gradient Descent(361/499): loss=0.3009816355727623, w0=-0.3146533914992938, gamma=0.09524687889643435\n",
      "Gradient Descent(362/499): loss=0.300981571317555, w0=-0.3146544019246099, gamma=0.10566133702923218\n",
      "Gradient Descent(363/499): loss=0.3009814785862401, w0=-0.31465541606868425, gamma=0.05844174960283891\n",
      "Gradient Descent(364/499): loss=0.30098137601069175, w0=-0.3146559177278721, gamma=0.041141796282578344\n",
      "Gradient Descent(365/499): loss=0.3009813193482685, w0=-0.3146562502465187, gamma=0.047968197213992564\n",
      "Gradient Descent(366/499): loss=0.3009812795019314, w0=-0.3146566219875846, gamma=0.4265787074523555\n",
      "Gradient Descent(367/499): loss=0.30098123309662667, w0=-0.3146597692849151, gamma=1.0239733832041853\n",
      "Gradient Descent(368/499): loss=0.3009808213382349, w0=-0.31466410141094603, gamma=0.055514493914558476\n",
      "Gradient Descent(369/499): loss=0.30097984151978935, w0=-0.31466409578043103, gamma=0.036110360725820016\n",
      "Gradient Descent(370/499): loss=0.30097978625932237, w0=-0.3146640923212853, gamma=0.036126134593671054\n",
      "Gradient Descent(371/499): loss=0.3009797508398454, w0=-0.31466408898559406, gamma=3.8233280469204383\n",
      "Gradient Descent(372/499): loss=0.30097971627854536, w0=-0.31466374871367225, gamma=5.169042978529729\n",
      "Gradient Descent(373/499): loss=0.3009760647150277, w0=-0.3146650475548348, gamma=0.165057649263999\n",
      "Gradient Descent(374/499): loss=0.30097123875939286, w0=-0.3146648746457049, gamma=0.1350496347129252\n",
      "Gradient Descent(375/499): loss=0.3009710088267976, w0=-0.3146647565233287, gamma=0.1334994836063045\n",
      "Gradient Descent(376/499): loss=0.30097087610371914, w0=-0.31466465552608236, gamma=0.14333239672582743\n",
      "Gradient Descent(377/499): loss=0.3009707494362068, w0=-0.3146645615660547, gamma=0.08522083508850452\n",
      "Gradient Descent(378/499): loss=0.3009706137161244, w0=-0.314664513707795, gamma=0.041981376956913806\n",
      "Gradient Descent(379/499): loss=0.3009705331162894, w0=-0.314664492141077, gamma=0.03818126426284054\n",
      "Gradient Descent(380/499): loss=0.30097049339568327, w0=-0.31466447335000153, gamma=0.104139757850292\n",
      "Gradient Descent(381/499): loss=0.30097045730772076, w0=-0.31466442405406425, gamma=1.135976615418698\n",
      "Gradient Descent(382/499): loss=0.3009703589099685, w0=-0.3146639423234806, gamma=1.0672515679436851\n",
      "Gradient Descent(383/499): loss=0.3009692874087158, w0=-0.3146640038646671, gamma=0.03868642933062118\n",
      "Gradient Descent(384/499): loss=0.30096829078874443, w0=-0.3146640037146433, gamma=0.03612120775713951\n",
      "Gradient Descent(385/499): loss=0.30096824670750577, w0=-0.3146640035799864, gamma=0.03737500395589983\n",
      "Gradient Descent(386/499): loss=0.30096821270747287, w0=-0.31466400344568823, gamma=0.18476639254029148\n",
      "Gradient Descent(387/499): loss=0.3009681775744603, w0=-0.314664002806588, gamma=0.4007812103913571\n",
      "Gradient Descent(388/499): loss=0.3009680039246924, w0=-0.3146640016764399, gamma=0.24971904049500745\n",
      "Gradient Descent(389/499): loss=0.30096762742608263, w0=-0.314664001254486, gamma=0.08400602376582048\n",
      "Gradient Descent(390/499): loss=0.30096739302603626, w0=-0.3146640011479865, gamma=0.07675800632172357\n",
      "Gradient Descent(391/499): loss=0.3009673140868628, w0=-0.31466400105885045, gamma=0.19048892071377965\n",
      "Gradient Descent(392/499): loss=0.30096724204459424, w0=-0.3146640008546226, gamma=9.166828454391352\n",
      "Gradient Descent(393/499): loss=0.3009670632886962, w0=-0.3146639928987636, gamma=0.5679681150349455\n",
      "Gradient Descent(394/499): loss=0.3009584930315155, w0=-0.3146639969245056, gamma=0.036158819312568445\n",
      "Gradient Descent(395/499): loss=0.3009583768691715, w0=-0.31466399703523223, gamma=0.03611272576303167\n",
      "Gradient Descent(396/499): loss=0.30095793207462296, w0=-0.3146639971418191, gamma=0.07395721619562784\n",
      "Gradient Descent(397/499): loss=0.3009578984922892, w0=-0.3146639973522212, gamma=0.07713917672679384\n",
      "Gradient Descent(398/499): loss=0.3009578300119847, w0=-0.31466399755544555, gamma=2.0465387099084187\n",
      "Gradient Descent(399/499): loss=0.30095775865851687, w0=-0.31466400253117605, gamma=19.991038889694416\n",
      "Gradient Descent(400/499): loss=0.30095586715678885, w0=-0.3146639516652233, gamma=0.07695794307408145\n",
      "Gradient Descent(401/499): loss=0.30093795555250746, w0=-0.31466395538394937, gamma=0.057630460859118486\n",
      "Gradient Descent(402/499): loss=0.30093753755647235, w0=-0.31466395795442975, gamma=0.05729510547809475\n",
      "Gradient Descent(403/499): loss=0.30093744151752305, w0=-0.31466396036267646, gamma=0.0421246873825277\n",
      "Gradient Descent(404/499): loss=0.3009373899281806, w0=-0.3146639620318285, gamma=0.045769126814644935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(405/499): loss=0.3009373520495949, w0=-0.3146639637689925, gamma=0.21072419296425282\n",
      "Gradient Descent(406/499): loss=0.30093731093044485, w0=-0.31466397140095204, gamma=0.4585620527946076\n",
      "Gradient Descent(407/499): loss=0.300937121780254, w0=-0.31466398450931826, gamma=0.09660826495441133\n",
      "Gradient Descent(408/499): loss=0.300936710889008, w0=-0.31466398600456696, gamma=0.03630619738893386\n",
      "Gradient Descent(409/499): loss=0.30093662454876463, w0=-0.3146639865122072, gamma=0.0361718033094067\n",
      "Gradient Descent(410/499): loss=0.3009365918008041, w0=-0.31466398699960607, gamma=1.5711713855195353\n",
      "Gradient Descent(411/499): loss=0.30093655942055825, w0=-0.31466400740464995, gamma=6.3939160700385615\n",
      "Gradient Descent(412/499): loss=0.3009351538646066, w0=-0.3146639599752916, gamma=0.7198758707582867\n",
      "Gradient Descent(413/499): loss=0.3009294590623015, w0=-0.31466398877858714, gamma=0.062117045904626494\n",
      "Gradient Descent(414/499): loss=0.30092893659492526, w0=-0.31466398947480617, gamma=0.05621400222901836\n",
      "Gradient Descent(415/499): loss=0.3009287696769269, w0=-0.31466399006572565, gamma=0.05591497303920138\n",
      "Gradient Descent(416/499): loss=0.3009287157797378, w0=-0.31466399062046047, gamma=0.05865995680245024\n",
      "Gradient Descent(417/499): loss=0.30092866531012136, w0=-0.3146639911698877, gamma=0.0631952518142025\n",
      "Gradient Descent(418/499): loss=0.300928613040959, w0=-0.3146639917270727, gamma=0.06266544734784277\n",
      "Gradient Descent(419/499): loss=0.3009285570076052, w0=-0.31466399224467023, gamma=0.05552208776466808\n",
      "Gradient Descent(420/499): loss=0.3009285015470098, w0=-0.31466399267452766, gamma=0.05308065274424495\n",
      "Gradient Descent(421/499): loss=0.30092845244209104, w0=-0.3146639930626662, gamma=0.0700735056646571\n",
      "Gradient Descent(422/499): loss=0.30092840551172734, w0=-0.3146639935478623, gamma=0.13250485334085185\n",
      "Gradient Descent(423/499): loss=0.3009283435700905, w0=-0.31466399440104864, gamma=0.13261631643888636\n",
      "Gradient Descent(424/499): loss=0.3009282264664708, w0=-0.31466399514180626, gamma=0.04837364918473372\n",
      "Gradient Descent(425/499): loss=0.30092810929844294, w0=-0.3146639953761748, gamma=0.03770817798862485\n",
      "Gradient Descent(426/499): loss=0.3009280665558018, w0=-0.3146639955500319, gamma=0.04981177898920945\n",
      "Gradient Descent(427/499): loss=0.3009280332446423, w0=-0.31466399577103366, gamma=4.27926033157228\n",
      "Gradient Descent(428/499): loss=0.30092798924520614, w0=-0.3146640138112622, gamma=11.257184984493472\n",
      "Gradient Descent(429/499): loss=0.3009242151998038, w0=-0.3146638581864305, gamma=0.04792772985160189\n",
      "Gradient Descent(430/499): loss=0.3009148824738007, w0=-0.3146638649825995, gamma=0.03619115046650128\n",
      "Gradient Descent(431/499): loss=0.30091444914010296, w0=-0.31466386986855627, gamma=0.036710189648560716\n",
      "Gradient Descent(432/499): loss=0.30091435872914735, w0=-0.3146638746452211, gamma=0.3474080415596785\n",
      "Gradient Descent(433/499): loss=0.300914314574226, w0=-0.314663918189888, gamma=0.3494720590179578\n",
      "Gradient Descent(434/499): loss=0.300913963869212, w0=-0.3146639467756177, gamma=0.2618893527202749\n",
      "Gradient Descent(435/499): loss=0.3009136615342526, w0=-0.31466396071105573, gamma=0.12961780158418112\n",
      "Gradient Descent(436/499): loss=0.3009134352796697, w0=-0.3146639658018894, gamma=0.03898820085323058\n",
      "Gradient Descent(437/499): loss=0.3009133234291059, w0=-0.31466396713469696, gamma=0.03625290138542272\n",
      "Gradient Descent(438/499): loss=0.3009132896619141, w0=-0.31466396832568044, gamma=0.05863125525565326\n",
      "Gradient Descent(439/499): loss=0.30091325836445265, w0=-0.31466397018201075, gamma=3.8912811580583804\n",
      "Gradient Descent(440/499): loss=0.30091320775191915, w0=-0.3146640861607672, gamma=6.097064213975227\n",
      "Gradient Descent(441/499): loss=0.3009098537232902, w0=-0.3146635607524876, gamma=0.20616935874003828\n",
      "Gradient Descent(442/499): loss=0.30090469538534553, w0=-0.31466365130914226, gamma=0.072089301053512\n",
      "Gradient Descent(443/499): loss=0.3009044996142351, w0=-0.3146636764450706, gamma=0.036423342258460556\n",
      "Gradient Descent(444/499): loss=0.30090443210996143, w0=-0.31466368822954155, gamma=0.03620077328326844\n",
      "Gradient Descent(445/499): loss=0.30090436727159275, w0=-0.3146636995153952, gamma=0.1592358049301628\n",
      "Gradient Descent(446/499): loss=0.300904335517395, w0=-0.31466374736120034, gamma=0.1975692250278572\n",
      "Gradient Descent(447/499): loss=0.300904197527566, w0=-0.3146637972722432, gamma=0.6358648935961574\n",
      "Gradient Descent(448/499): loss=0.30090402806758904, w0=-0.31466392617131, gamma=0.6679899745301031\n",
      "Gradient Descent(449/499): loss=0.300903485787502, w0=-0.3146639754793143, gamma=0.06706206099683043\n",
      "Gradient Descent(450/499): loss=0.3009029188544509, w0=-0.31466397712283656, gamma=0.04056357294017096\n",
      "Gradient Descent(451/499): loss=0.30090286132821037, w0=-0.31466397805028046, gamma=0.03655443174704887\n",
      "Gradient Descent(452/499): loss=0.3009028265905306, w0=-0.31466397885215736, gamma=0.04681602290511699\n",
      "Gradient Descent(453/499): loss=0.3009027955358714, w0=-0.3146639798415971, gamma=0.16079392893298847\n",
      "Gradient Descent(454/499): loss=0.3009027557767295, w0=-0.31466398308082283, gamma=0.3170004218721349\n",
      "Gradient Descent(455/499): loss=0.3009026192457251, w0=-0.31466398844002363, gamma=0.6214017049771761\n",
      "Gradient Descent(456/499): loss=0.3009023501563877, w0=-0.31466399561520825, gamma=0.1461394304227104\n",
      "Gradient Descent(457/499): loss=0.30090182303623575, w0=-0.3146639962540698, gamma=0.054607626869393966\n",
      "Gradient Descent(458/499): loss=0.3009016991752108, w0=-0.3146639964579052, gamma=0.03760172578348901\n",
      "Gradient Descent(459/499): loss=0.3009016527776775, w0=-0.3146639965905977, gamma=0.03726669740579782\n",
      "Gradient Descent(460/499): loss=0.3009016208749313, w0=-0.31466399671716283, gamma=0.1563351553579884\n",
      "Gradient Descent(461/499): loss=0.30090158928600574, w0=-0.3146639972283217, gamma=0.4609907567590025\n",
      "Gradient Descent(462/499): loss=0.30090145678539887, w0=-0.3146639984999537, gamma=0.6147637031555225\n",
      "Gradient Descent(463/499): loss=0.3009010662035052, w0=-0.31466399941401174, gamma=0.10121889631343324\n",
      "Gradient Descent(464/499): loss=0.30090054593096893, w0=-0.31466399947198864, gamma=0.06100515431213067\n",
      "Gradient Descent(465/499): loss=0.3009004600888946, w0=-0.31466399950339474, gamma=0.037045577244054176\n",
      "Gradient Descent(466/499): loss=0.300900408432309, w0=-0.31466399952130275, gamma=0.0366707848974456\n",
      "Gradient Descent(467/499): loss=0.30090037706244444, w0=-0.3146639995383729, gamma=0.3967497663893297\n",
      "Gradient Descent(468/499): loss=0.30090034604003335, w0=-0.31466399971628584, gamma=2.2171701554870458\n",
      "Gradient Descent(469/499): loss=0.30090001046071874, w0=-0.31466400031606045, gamma=0.23258058141910845\n",
      "Gradient Descent(470/499): loss=0.3008981379474543, w0=-0.3146640002394812, gamma=0.08178050217038851\n",
      "Gradient Descent(471/499): loss=0.3008979435240994, w0=-0.31466400021881696, gamma=0.07270605306008052\n",
      "Gradient Descent(472/499): loss=0.3008978723564941, w0=-0.31466400020194807, gamma=0.0364152365533448\n",
      "Gradient Descent(473/499): loss=0.3008978110907766, w0=-0.31466400019411345, gamma=0.036258759615398345\n",
      "Gradient Descent(474/499): loss=0.30089778032333925, w0=-0.31466400018659657, gamma=1.8639893500973972\n",
      "Gradient Descent(475/499): loss=0.30089774977434147, w0=-0.31466399981418147, gamma=42.91725308149279\n",
      "Gradient Descent(476/499): loss=0.3008961803383877, w0=-0.31466400722265003, gamma=0.20358506090643858\n",
      "Gradient Descent(477/499): loss=0.3008610159217567, w0=-0.3146640057495479, gamma=0.08603567833092521\n",
      "Gradient Descent(478/499): loss=0.30086114861241114, w0=-0.31466400525374966, gamma=0.07697277292336599\n",
      "Gradient Descent(479/499): loss=0.3008604153520945, w0=-0.3146640048483413, gamma=0.0371285354784961\n",
      "Gradient Descent(480/499): loss=0.30086035382473464, w0=-0.314664004667841, gamma=0.036140869340313186\n",
      "Gradient Descent(481/499): loss=0.3008603017616964, w0=-0.31466400449866566, gamma=0.05177580176163819\n",
      "Gradient Descent(482/499): loss=0.3008602729888498, w0=-0.31466400426506247, gamma=0.11212972366899726\n",
      "Gradient Descent(483/499): loss=0.30086023185862226, w0=-0.314664003785347, gamma=0.12749226853028678\n",
      "Gradient Descent(484/499): loss=0.30086014286554813, w0=-0.31466400330106714, gamma=6.698782466476824\n",
      "Gradient Descent(485/499): loss=0.3008600417114946, w0=-0.3146639810998053, gamma=30.86764180100931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(486/499): loss=0.30085473867534734, w0=-0.3146645640983219, gamma=0.051808830236319076\n",
      "Gradient Descent(487/499): loss=0.30083630322806176, w0=-0.314664534872368, gamma=0.040366720352851514\n",
      "Gradient Descent(488/499): loss=0.30083147033151575, w0=-0.3146645132807949, gamma=0.038420558597514386\n",
      "Gradient Descent(489/499): loss=0.3008307219427431, w0=-0.31466449355975584, gamma=0.060435468822054764\n",
      "Gradient Descent(490/499): loss=0.3008306230439822, w0=-0.3146644637304475, gamma=0.0876296662140516\n",
      "Gradient Descent(491/499): loss=0.30083055040995726, w0=-0.31466442309275316, gamma=0.09255621789342029\n",
      "Gradient Descent(492/499): loss=0.30083048023598946, w0=-0.31466438393167345, gamma=0.04638819696705783\n",
      "Gradient Descent(493/499): loss=0.3008304100970502, w0=-0.3146643661211666, gamma=0.03651084162064072\n",
      "Gradient Descent(494/499): loss=0.30083037492289366, w0=-0.3146643527532955, gamma=0.04008912746849228\n",
      "Gradient Descent(495/499): loss=0.3008303472827601, w0=-0.3146643386111976, gamma=1.064390923816822\n",
      "Gradient Descent(496/499): loss=0.30083031694206086, w0=-0.3146639781825439, gamma=1.7573539157130307\n",
      "Gradient Descent(497/499): loss=0.30082951187839885, w0=-0.3146640165004753, gamma=0.13365462632590605\n",
      "Gradient Descent(498/499): loss=0.3008281871544881, w0=-0.3146640142933568, gamma=0.09092804176195818\n",
      "Gradient Descent(499/499): loss=0.3008280846305007, w0=-0.3146640129924968, gamma=0.04307875379383272\n"
     ]
    }
   ],
   "source": [
    "w = np.array(np.random.rand((tX.shape[1])))\n",
    "max_iters = 500 #choosing number of iterations\n",
    "gamma = 5 *10**(-2)\n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)\n",
    "(w, loss) =adaptative_step_gradient_descent(y, tX, w, max_iters, gamma)\n",
    "#gamma = 5/max_iters #choosing gamma \n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tX.shape[1]):\n",
    "    col = tX[:,i]\n",
    "    places_999 = (col == -999.0)\n",
    "    mean_col = col[col!=-999.0].mean()\n",
    "    col[places_999] = mean_col\n",
    "    tX[:,i] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = build_poly(tX_test, degree)\n",
    "tX_test = tX_test[:,1:]\n",
    "tX_test = (tX_test-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = np.c_[(np.ones(tX_test.shape[0]) , tX_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
