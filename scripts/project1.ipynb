{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from extend import *\n",
    "from gradient_descent import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tX.shape[1]):\n",
    "    col = tX[:,i]\n",
    "    places_999 = (col == -999.0)\n",
    "    mean_col = col[col!=-999.0].mean()\n",
    "    col[places_999] = mean_col\n",
    "    tX[:,i] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.38470000e+02,  5.16550000e+01,  9.78270000e+01, ...,\n",
       "         1.24000000e+00, -2.47500000e+00,  1.13497000e+02],\n",
       "       [ 1.60937000e+02,  6.87680000e+01,  1.03235000e+02, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  4.62260000e+01],\n",
       "       [ 1.21858528e+02,  1.62172000e+02,  1.25953000e+02, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  4.42510000e+01],\n",
       "       ...,\n",
       "       [ 1.05457000e+02,  6.05260000e+01,  7.58390000e+01, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  4.19920000e+01],\n",
       "       [ 9.49510000e+01,  1.93620000e+01,  6.88120000e+01, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  0.00000000e+00],\n",
       "       [ 1.21858528e+02,  7.27560000e+01,  7.08310000e+01, ...,\n",
       "        -1.18452642e-02, -1.58228913e-03,  0.00000000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 5\n",
    "tX = build_poly(tX, degree)\n",
    "tX = tX[:,1:]\n",
    "#tX = extend(tX,[np.cos,np.sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 150)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(tX,axis = 0)\n",
    "std = np.std(tX,axis = 0)\n",
    "tX = (tX-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[(np.ones(tX.shape[0]) , tX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 151)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=198.05642109855407, w0=0.4902548638091157, gamma=0.0525260003859785\n",
      "Gradient Descent(1/499): loss=28.806248494337805, w0=0.4479756952584541, gamma=0.1150194423120764\n",
      "Gradient Descent(2/499): loss=15.936560472466512, w0=0.36025730282552026, gamma=0.12811015948479823\n",
      "Gradient Descent(3/499): loss=7.148366483996095, w0=0.2737930270813364, gamma=0.29516482187698884\n",
      "Gradient Descent(4/499): loss=4.25922250906486, w0=0.10010121350160991, gamma=0.36813530201080485\n",
      "Gradient Descent(5/499): loss=1.5352950651672612, w0=-0.05258850363339243, gamma=0.10754137912181486\n",
      "Gradient Descent(6/499): loss=1.1030595421548373, w0=-0.08077246394649505, gamma=0.06257140225212193\n",
      "Gradient Descent(7/499): loss=0.8442563515388866, w0=-0.09540738533214858, gamma=0.05152011525197287\n",
      "Gradient Descent(8/499): loss=0.6883593257406577, w0=-0.10670351138951178, gamma=0.05569680562177828\n",
      "Gradient Descent(9/499): loss=0.6578433298645694, w0=-0.11828624630057359, gamma=0.4522989466553068\n",
      "Gradient Descent(10/499): loss=0.6354561014472885, w0=-0.20710769744469135, gamma=0.7376208519024414\n",
      "Gradient Descent(11/499): loss=0.5072606558829104, w0=-0.28644346896237316, gamma=0.1574473673816919\n",
      "Gradient Descent(12/499): loss=0.4586774528374283, w0=-0.2908867172803287, gamma=0.08605907012218363\n",
      "Gradient Descent(13/499): loss=0.4452653918255067, w0=-0.29293296812119896, gamma=0.07578762165246276\n",
      "Gradient Descent(14/499): loss=0.4291068849288778, w0=-0.29457991134333744, gamma=0.05186032070786291\n",
      "Gradient Descent(15/499): loss=0.42607092950715314, w0=-0.29562147862219074, gamma=0.05490107162705219\n",
      "Gradient Descent(16/499): loss=0.4242279657274433, w0=-0.2966669334523079, gamma=1.438724476288872\n",
      "Gradient Descent(17/499): loss=0.42248376637176355, w0=-0.32255975359574507, gamma=1.9498218554521276\n",
      "Gradient Descent(18/499): loss=0.38687466297871, w0=-0.30716444067005555, gamma=0.2624547657812031\n",
      "Gradient Descent(19/499): loss=0.3675939874300587, w0=-0.3091327357575482, gamma=0.0624832861588188\n",
      "Gradient Descent(20/499): loss=0.37462195463644615, w0=-0.3094783473240572, gamma=0.05137223719659138\n",
      "Gradient Descent(21/499): loss=0.36163138856382304, w0=-0.3097447459033653, gamma=0.05204504793665293\n",
      "Gradient Descent(22/499): loss=0.3605077777818505, w0=-0.31000076871865895, gamma=0.11620268842532894\n",
      "Gradient Descent(23/499): loss=0.3601321574757008, w0=-0.3105426487303492, gamma=0.18877348303066963\n",
      "Gradient Descent(24/499): loss=0.35936113828779476, w0=-0.31132065056439706, gamma=0.5291010564718754\n",
      "Gradient Descent(25/499): loss=0.3581814743609079, w0=-0.31308962028317394, gamma=0.6058691637605829\n",
      "Gradient Descent(26/499): loss=0.35512276597221837, w0=-0.3140434884059652, gamma=0.1034911072657851\n",
      "Gradient Descent(27/499): loss=0.35241897685801205, w0=-0.31410770583796194, gamma=0.08088941284831251\n",
      "Gradient Descent(28/499): loss=0.3516010338089365, w0=-0.3141527041461474, gamma=0.055937840568689244\n",
      "Gradient Descent(29/499): loss=0.35121840977685825, w0=-0.3141813049321364, gamma=0.05511269807075625\n",
      "Gradient Descent(30/499): loss=0.35096711228324473, w0=-0.3142079075597044, gamma=0.4126605806388216\n",
      "Gradient Descent(31/499): loss=0.35072882271519173, w0=-0.31439611893118735, gamma=5.501540697516368\n",
      "Gradient Descent(32/499): loss=0.34899868636890397, w0=-0.3158698775367898, gamma=0.7278624674344276\n",
      "Gradient Descent(33/499): loss=0.333768766628945, w0=-0.31499216453815154, gamma=0.05199837675576879\n",
      "Gradient Descent(34/499): loss=0.39911229837295153, w0=-0.31497510051491706, gamma=0.05101043498843301\n",
      "Gradient Descent(35/499): loss=0.3330249267344406, w0=-0.314959231142377, gamma=0.07735028400601501\n",
      "Gradient Descent(36/499): loss=0.3324001608705016, w0=-0.31493639492974423, gamma=0.08899615195717152\n",
      "Gradient Descent(37/499): loss=0.33219426326964424, w0=-0.3149121528292736, gamma=0.11518309541078162\n",
      "Gradient Descent(38/499): loss=0.3320844605323858, w0=-0.31488356981837856, gamma=0.41712564322964385\n",
      "Gradient Descent(39/499): loss=0.33194785870786764, w0=-0.3147919816170733, gamma=0.9802841289195683\n",
      "Gradient Descent(40/499): loss=0.33146559219454197, w0=-0.3146665232700564, gamma=0.13184562924007295\n",
      "Gradient Descent(41/499): loss=0.33040835659035445, w0=-0.3146661905880638, gamma=0.05125614946849628\n",
      "Gradient Descent(42/499): loss=0.3302945942181337, w0=-0.31466607830700766, gamma=0.050695325047454914\n",
      "Gradient Descent(43/499): loss=0.330204351864363, w0=-0.31466597294661075, gamma=0.3810016707321628\n",
      "Gradient Descent(44/499): loss=0.3301532081119813, w0=-0.31466522125105006, gamma=1.5739778262571338\n",
      "Gradient Descent(45/499): loss=0.3297738990174599, w0=-0.3146632990306163, gamma=1.6362169247227574\n",
      "Gradient Descent(46/499): loss=0.3282988524914357, w0=-0.31466444597033444, gamma=0.13633088357505704\n",
      "Gradient Descent(47/499): loss=0.327304255079316, w0=-0.31466438517095446, gamma=0.09497424932125917\n",
      "Gradient Descent(48/499): loss=0.3269199718158024, w0=-0.31466434858973635, gamma=0.08302614804072916\n",
      "Gradient Descent(49/499): loss=0.3267797434149162, w0=-0.3146643196477646, gamma=0.05853863914593702\n",
      "Gradient Descent(50/499): loss=0.32671335385888467, w0=-0.31466430093608394, gamma=0.05780911166646279\n",
      "Gradient Descent(51/499): loss=0.32666859874956516, w0=-0.3146642835393, gamma=0.16417341833495805\n",
      "Gradient Descent(52/499): loss=0.3266257833449069, w0=-0.31466423698986495, gamma=0.40146349493337424\n",
      "Gradient Descent(53/499): loss=0.3265058138972288, w0=-0.3146641418475294, gamma=0.7013052823101695\n",
      "Gradient Descent(54/499): loss=0.3262182287411565, w0=-0.3146640423698877, gamma=0.06634387587189189\n",
      "Gradient Descent(55/499): loss=0.3257478522382882, w0=-0.31466403955897954, gamma=0.05070239193452158\n",
      "Gradient Descent(56/499): loss=0.3256887053118648, w0=-0.3146640375533017, gamma=0.0515507790253015\n",
      "Gradient Descent(57/499): loss=0.3256533135225121, w0=-0.3146640356174577, gamma=3.006695942327551\n",
      "Gradient Descent(58/499): loss=0.3256190517896528, w0=-0.3146639285299764, gamma=4.745210453275519\n",
      "Gradient Descent(59/499): loss=0.3237252868212617, w0=-0.31466426767583794, gamma=0.22572383139909993\n",
      "Gradient Descent(60/499): loss=0.3226656425723992, w0=-0.31466420725530253, gamma=0.17189802170995616\n",
      "Gradient Descent(61/499): loss=0.3213319798192672, w0=-0.31466417162873844, gamma=0.0869030149739576\n",
      "Gradient Descent(62/499): loss=0.3212452552998876, w0=-0.3146641567137921, gamma=0.07954061187694082\n",
      "Gradient Descent(63/499): loss=0.3211105450266579, w0=-0.31466414424878003, gamma=0.05133953564674407\n",
      "Gradient Descent(64/499): loss=0.3210763575440078, w0=-0.31466413684317857, gamma=0.05148373434474026\n",
      "Gradient Descent(65/499): loss=0.32105473346074165, w0=-0.3146641297980448, gamma=1.1296910735093046\n",
      "Gradient Descent(66/499): loss=0.3210339066950737, w0=-0.3146639831677597, gamma=2.4922742606062576\n",
      "Gradient Descent(67/499): loss=0.3205840876795056, w0=-0.31466402512145686, gamma=0.3198579688657608\n",
      "Gradient Descent(68/499): loss=0.31969199983023905, w0=-0.3146640170865703, gamma=0.1799032426226261\n",
      "Gradient Descent(69/499): loss=0.319560071065004, w0=-0.314664014012873, gamma=0.07154856953127867\n",
      "Gradient Descent(70/499): loss=0.31949701303555555, w0=-0.3146640130103645, gamma=0.052868051493495695\n",
      "Gradient Descent(71/499): loss=0.3194610641274224, w0=-0.3146640123226002, gamma=0.05141235379795057\n",
      "Gradient Descent(72/499): loss=0.31944141122334857, w0=-0.3146640116891328, gamma=0.13520918286370626\n",
      "Gradient Descent(73/499): loss=0.31942413431936845, w0=-0.31466401010882966, gamma=0.5581839592490689\n",
      "Gradient Descent(74/499): loss=0.3193789024636289, w0=-0.314664004466966, gamma=1.8739060793079223\n",
      "Gradient Descent(75/499): loss=0.3191940575276874, w0=-0.3146639960987286, gamma=0.1632408920514515\n",
      "Gradient Descent(76/499): loss=0.31860626891353994, w0=-0.314663996735791, gamma=0.08343315586288633\n",
      "Gradient Descent(77/499): loss=0.3185547071013942, w0=-0.3146639970082444, gamma=0.06861530086725114\n",
      "Gradient Descent(78/499): loss=0.31852065721823586, w0=-0.31466399721361527, gamma=0.05135866514465676\n",
      "Gradient Descent(79/499): loss=0.31849942258307756, w0=-0.3146639973567881, gamma=0.054745463588689626\n",
      "Gradient Descent(80/499): loss=0.31848394895515103, w0=-0.3146639975015644, gamma=3.4330899356566627\n",
      "Gradient Descent(81/499): loss=0.31846758174524115, w0=-0.31466400608345946, gamma=9.41336428871629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(82/499): loss=0.3174748211328191, w0=-0.31466394883037985, gamma=0.15382222514920346\n",
      "Gradient Descent(83/499): loss=0.31629853640155126, w0=-0.3146639567016233, gamma=0.09080048373337093\n",
      "Gradient Descent(84/499): loss=0.31554345067990025, w0=-0.31466396063326707, gamma=0.06468620437477494\n",
      "Gradient Descent(85/499): loss=0.315209998776045, w0=-0.3146639631798438, gamma=0.0514035414081953\n",
      "Gradient Descent(86/499): loss=0.31513720051377103, w0=-0.3146639650726037, gamma=0.05556341505990139\n",
      "Gradient Descent(87/499): loss=0.31511875330613254, w0=-0.3146639670133685, gamma=0.22560023865746132\n",
      "Gradient Descent(88/499): loss=0.3151052617266086, w0=-0.31466397445548283, gamma=0.23209849982251224\n",
      "Gradient Descent(89/499): loss=0.31505817882397347, w0=-0.31466398038465887, gamma=0.32881757842491505\n",
      "Gradient Descent(90/499): loss=0.31501386403814957, w0=-0.31466398683499847, gamma=0.13004233273743102\n",
      "Gradient Descent(91/499): loss=0.3149515619360665, w0=-0.3146639885471923, gamma=0.05697458677728274\n",
      "Gradient Descent(92/499): loss=0.3149271064906374, w0=-0.31466398919979294, gamma=0.05210133062091754\n",
      "Gradient Descent(93/499): loss=0.31491628177141007, w0=-0.3146639897625728, gamma=0.12649510123768196\n",
      "Gradient Descent(94/499): loss=0.3149065021995447, w0=-0.31466399105773857, gamma=6.41493090693276\n",
      "Gradient Descent(95/499): loss=0.31488280424538556, w0=-0.31466404843093065, gamma=14.491954538585393\n",
      "Gradient Descent(96/499): loss=0.31372957562932846, w0=-0.3146633465933153, gamma=0.06039993291583913\n",
      "Gradient Descent(97/499): loss=0.31866821223397823, w0=-0.31466338605912497, gamma=0.05381341320980994\n",
      "Gradient Descent(98/499): loss=0.31198565571929304, w0=-0.31466341909746076, gamma=0.05804397571604947\n",
      "Gradient Descent(99/499): loss=0.31160476932326153, w0=-0.3146634528154415, gamma=0.08472874017274137\n",
      "Gradient Descent(100/499): loss=0.3115657390191588, w0=-0.3146634991778281, gamma=0.08842009255531957\n",
      "Gradient Descent(101/499): loss=0.31155139545949034, w0=-0.3146635434607048, gamma=0.08707402741636593\n",
      "Gradient Descent(102/499): loss=0.31154091087368196, w0=-0.31466358321355176, gamma=0.0814507856056413\n",
      "Gradient Descent(103/499): loss=0.3115306594959528, w0=-0.3146636171612588, gamma=0.08954941324005257\n",
      "Gradient Descent(104/499): loss=0.31152109747334367, w0=-0.3146636514443791, gamma=0.13664319717531084\n",
      "Gradient Descent(105/499): loss=0.3115106048151312, w0=-0.3146636990723405, gamma=0.18770698879550612\n",
      "Gradient Descent(106/499): loss=0.311494624722536, w0=-0.3146637555588497, gamma=0.09784034148369217\n",
      "Gradient Descent(107/499): loss=0.31147273473348686, w0=-0.3146637794752035, gamma=0.05698446436985452\n",
      "Gradient Descent(108/499): loss=0.31146135002852104, w0=-0.31466379204177725, gamma=0.057906193052452\n",
      "Gradient Descent(109/499): loss=0.3114547212459438, w0=-0.31466380408393396, gamma=0.5061951919481238\n",
      "Gradient Descent(110/499): loss=0.3114479997416855, w0=-0.31466390325647164, gamma=17.305052823914917\n",
      "Gradient Descent(111/499): loss=0.3113894063492817, w0=-0.3146655774345715, gamma=0.3169305124350424\n",
      "Gradient Descent(112/499): loss=0.3095642141318007, w0=-0.3146650774979104, gamma=0.05060025093742456\n",
      "Gradient Descent(113/499): loss=0.3102371437522091, w0=-0.3146650229763226, gamma=0.05059870668616404\n",
      "Gradient Descent(114/499): loss=0.3095063475054957, w0=-0.31466497121512116, gamma=11.898399074843507\n",
      "Gradient Descent(115/499): loss=0.309502006992328, w0=-0.3146534153282364, gamma=24.036573557126946\n",
      "Gradient Descent(116/499): loss=0.3085244055719259, w0=-0.3149078346062885, gamma=0.21325429766609627\n",
      "Gradient Descent(117/499): loss=0.3105477033932223, w0=-0.31485583582889914, gamma=0.19771730604266774\n",
      "Gradient Descent(118/499): loss=0.30697952193805106, w0=-0.3148179065659014, gamma=0.11902632544953941\n",
      "Gradient Descent(119/499): loss=0.30692875611838893, w0=-0.31479958763307825, gamma=0.11370105589201672\n",
      "Gradient Descent(120/499): loss=0.30689603858451886, w0=-0.3147841711762015, gamma=0.13096592965199982\n",
      "Gradient Descent(121/499): loss=0.30688999157632485, w0=-0.3147684328465889, gamma=0.262959555512416\n",
      "Gradient Descent(122/499): loss=0.30688318379659146, w0=-0.31474097123206257, gamma=0.3645354818041873\n",
      "Gradient Descent(123/499): loss=0.30686961724033224, w0=-0.31471291248744315, gamma=0.1243133208726956\n",
      "Gradient Descent(124/499): loss=0.3068509593487208, w0=-0.31470683201388305, gamma=0.06958284903424485\n",
      "Gradient Descent(125/499): loss=0.3068446086958677, w0=-0.3147038516404313, gamma=0.05753997371829124\n",
      "Gradient Descent(126/499): loss=0.30684105036155973, w0=-0.31470155857817433, gamma=0.06751538350759823\n",
      "Gradient Descent(127/499): loss=0.30683812168812086, w0=-0.31469902279646594, gamma=0.7840912723718819\n",
      "Gradient Descent(128/499): loss=0.3068346895813853, w0=-0.3146715617285962, gamma=4.312036725120274\n",
      "Gradient Descent(129/499): loss=0.3067949582594412, w0=-0.3146389552836327, gamma=0.08397459344845136\n",
      "Gradient Descent(130/499): loss=0.30658489209957684, w0=-0.3146410584036328, gamma=0.05068419453519278\n",
      "Gradient Descent(131/499): loss=0.306578534780681, w0=-0.3146422211800415, gamma=0.050609771645526797\n",
      "Gradient Descent(132/499): loss=0.30657434331438327, w0=-0.31464332340122164, gamma=1.6452846206600513\n",
      "Gradient Descent(133/499): loss=0.30657192791450927, w0=-0.31467734229364924, gamma=9.827953593703137\n",
      "Gradient Descent(134/499): loss=0.3064938260382225, w0=-0.3145462148654388, gamma=0.16026797299056145\n",
      "Gradient Descent(135/499): loss=0.30606637627677696, w0=-0.31456509205044025, gamma=0.0815863954055576\n",
      "Gradient Descent(136/499): loss=0.30605560203745774, w0=-0.31457316159364146, gamma=0.07969725606121635\n",
      "Gradient Descent(137/499): loss=0.30603713869165355, w0=-0.3145804011654905, gamma=0.15458464960605608\n",
      "Gradient Descent(138/499): loss=0.30603368235293477, w0=-0.31459332426225817, gamma=0.2569709582590353\n",
      "Gradient Descent(139/499): loss=0.3060271653899472, w0=-0.3146114858746895, gamma=0.33038757491756826\n",
      "Gradient Descent(140/499): loss=0.3060164945210483, w0=-0.3146288358896858, gamma=0.09506552314474603\n",
      "Gradient Descent(141/499): loss=0.3060028506770873, w0=-0.31463217878436855, gamma=0.0519656729817126\n",
      "Gradient Descent(142/499): loss=0.3059989318920139, w0=-0.3146338323953303, gamma=0.05234717096187725\n",
      "Gradient Descent(143/499): loss=0.30599678451329787, w0=-0.31463541158416647, gamma=2.212203838751208\n",
      "Gradient Descent(144/499): loss=0.30599463147539385, w0=-0.31469865499066985, gamma=25.699866174298403\n",
      "Gradient Descent(145/499): loss=0.3059042598181076, w0=-0.3138080264058779, gamma=0.9649913245504046\n",
      "Gradient Descent(146/499): loss=0.30495624176639585, w0=-0.3146340334996352, gamma=0.05198242557811798\n",
      "Gradient Descent(147/499): loss=0.3061237823487002, w0=-0.314635591231084, gamma=0.050741105708369795\n",
      "Gradient Descent(148/499): loss=0.30493021722311836, w0=-0.3146370327235032, gamma=0.07518009298385511\n",
      "Gradient Descent(149/499): loss=0.30492165285796646, w0=-0.31463906012596515, gamma=0.150942510266764\n",
      "Gradient Descent(150/499): loss=0.3049156026336361, w0=-0.3146428246133733, gamma=0.2167985567356286\n",
      "Gradient Descent(151/499): loss=0.30490871699120403, w0=-0.31464741540694185, gamma=0.2219021650823557\n",
      "Gradient Descent(152/499): loss=0.3049020404523443, w0=-0.3146510955643653, gamma=0.10873111637340524\n",
      "Gradient Descent(153/499): loss=0.3048955289566462, w0=-0.31465249867821327, gamma=0.07591666121681005\n",
      "Gradient Descent(154/499): loss=0.3048923407215199, w0=-0.31465337182027125, gamma=0.06450039148004112\n",
      "Gradient Descent(155/499): loss=0.30489011988893266, w0=-0.31465405734211654, gamma=0.09236533952819201\n",
      "Gradient Descent(156/499): loss=0.3048882358014849, w0=-0.31465497569921963, gamma=0.5448933160197896\n",
      "Gradient Descent(157/499): loss=0.30488554036238824, w0=-0.31465989298117436, gamma=0.7466636584494423\n",
      "Gradient Descent(158/499): loss=0.3048696772149753, w0=-0.31466295954394136, gamma=0.057938047344023004\n",
      "Gradient Descent(159/499): loss=0.30484831656479133, w0=-0.31466301982601635, gamma=0.050636777791340695\n",
      "Gradient Descent(160/499): loss=0.30484641059755574, w0=-0.31466306945894074, gamma=0.05240324115764601\n",
      "Gradient Descent(161/499): loss=0.3048449495913574, w0=-0.31466311822238296, gamma=97.99536308968973\n",
      "Gradient Descent(162/499): loss=0.30484344370624444, w0=-0.3147495284798266, gamma=122.72158250519279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(163/499): loss=0.3027152820586401, w0=-0.3042533382457529, gamma=0.157063874688173\n",
      "Gradient Descent(164/499): loss=0.36954296424979205, w0=-0.3058884771191214, gamma=0.09621180219133109\n",
      "Gradient Descent(165/499): loss=0.32548298896052036, w0=-0.3067327859907942, gamma=0.07946843582077621\n",
      "Gradient Descent(166/499): loss=0.3072584001596011, w0=-0.30736306716236156, gamma=0.06741271456429804\n",
      "Gradient Descent(167/499): loss=0.30509874206034, w0=-0.30785524286388183, gamma=0.06590406770691352\n",
      "Gradient Descent(168/499): loss=0.30435191320941196, w0=-0.3083039676552627, gamma=0.11501139672685112\n",
      "Gradient Descent(169/499): loss=0.3038987961156997, w0=-0.3090354438586024, gamma=0.23969492062825803\n",
      "Gradient Descent(170/499): loss=0.3033612187655977, w0=-0.31038458017646664, gamma=0.16889368874048513\n",
      "Gradient Descent(171/499): loss=0.3027287949854301, w0=-0.3111073471763445, gamma=0.05481919344626945\n",
      "Gradient Descent(172/499): loss=0.30256821864613176, w0=-0.311302320015575, gamma=0.05113746671531849\n",
      "Gradient Descent(173/499): loss=0.30238561618377063, w0=-0.3114742278139505, gamma=0.11590597125424205\n",
      "Gradient Descent(174/499): loss=0.3023308293415341, w0=-0.3118439414574004, gamma=1.0835350472755694\n",
      "Gradient Descent(175/499): loss=0.3022223813505471, w0=-0.31489957372504157, gamma=1.1043200633513128\n",
      "Gradient Descent(176/499): loss=0.3017586603629358, w0=-0.3146394249354684, gamma=0.09206667489504268\n",
      "Gradient Descent(177/499): loss=0.3017711007139106, w0=-0.3146416874800583, gamma=0.05073414637940793\n",
      "Gradient Descent(178/499): loss=0.30175923125701487, w0=-0.3146428194867737, gamma=0.05070816127316007\n",
      "Gradient Descent(179/499): loss=0.30173918426111646, w0=-0.3146438935117167, gamma=0.36522135438699005\n",
      "Gradient Descent(180/499): loss=0.30173840645991956, w0=-0.3146512368310495, gamma=0.38670390208164795\n",
      "Gradient Descent(181/499): loss=0.30173490017163557, w0=-0.31465617239876303, gamma=0.4096665129968766\n",
      "Gradient Descent(182/499): loss=0.30173298125603054, w0=-0.31465937910537317, gamma=0.3532508760441243\n",
      "Gradient Descent(183/499): loss=0.30173116833564123, w0=-0.3146610114408849, gamma=0.14537715050375807\n",
      "Gradient Descent(184/499): loss=0.3017296412534202, w0=-0.31466144590927286, gamma=0.0545886773210903\n",
      "Gradient Descent(185/499): loss=0.3017290194336216, w0=-0.3146615853337749, gamma=0.0510357014769449\n",
      "Gradient Descent(186/499): loss=0.30172877789617325, w0=-0.31466170856802267, gamma=0.08272082814247542\n",
      "Gradient Descent(187/499): loss=0.30172855873890103, w0=-0.31466189811727574, gamma=0.6720369700418873\n",
      "Gradient Descent(188/499): loss=0.3017282038333455, w0=-0.31466331066100406, gamma=1.6413122790422154\n",
      "Gradient Descent(189/499): loss=0.30172532627960186, w0=-0.3146644420835923, gamma=0.6383958239322566\n",
      "Gradient Descent(190/499): loss=0.30171833886160476, w0=-0.3146641598600629, gamma=0.08480302794366329\n",
      "Gradient Descent(191/499): loss=0.3017156830562278, w0=-0.31466414630355044, gamma=0.05436535868602163\n",
      "Gradient Descent(192/499): loss=0.3017152909937977, w0=-0.3146641383497727, gamma=0.05164074973297363\n",
      "Gradient Descent(193/499): loss=0.3017150472384272, w0=-0.3146641312053506, gamma=0.11447879688710379\n",
      "Gradient Descent(194/499): loss=0.3017148279412444, w0=-0.3146641161852616, gamma=0.2267568750642549\n",
      "Gradient Descent(195/499): loss=0.3017143435098342, w0=-0.3146640898397353, gamma=0.3329450415142332\n",
      "Gradient Descent(196/499): loss=0.3017133866311657, w0=-0.3146640599284529, gamma=0.9860266713230826\n",
      "Gradient Descent(197/499): loss=0.3017119835795006, w0=-0.3146640008386203, gamma=0.08496651148210528\n",
      "Gradient Descent(198/499): loss=0.3017078428778065, w0=-0.3146640007674708, gamma=0.05079693566192966\n",
      "Gradient Descent(199/499): loss=0.30170748352735616, w0=-0.3146640007285485, gamma=0.050821819928428116\n",
      "Gradient Descent(200/499): loss=0.3017072680074915, w0=-0.31466400069158523, gamma=4.628552580121974\n",
      "Gradient Descent(201/499): loss=0.3017070550823774, w0=-0.3146639974962767, gamma=12.168871050311825\n",
      "Gradient Descent(202/499): loss=0.30168776190056706, w0=-0.3146640279788447, gamma=0.21524975406690547\n",
      "Gradient Descent(203/499): loss=0.3016433747448164, w0=-0.3146640219566727, gamma=0.16042357844936314\n",
      "Gradient Descent(204/499): loss=0.3016380315574312, w0=-0.3146640184345042, gamma=0.08277221902878339\n",
      "Gradient Descent(205/499): loss=0.3016373240842408, w0=-0.3146640169087422, gamma=0.07700195139379734\n",
      "Gradient Descent(206/499): loss=0.3016366145935625, w0=-0.31466401560683177, gamma=0.05168413963900482\n",
      "Gradient Descent(207/499): loss=0.3016363077332023, w0=-0.31466401480027034, gamma=0.05104139156629848\n",
      "Gradient Descent(208/499): loss=0.3016361041697224, w0=-0.3146640140449074, gamma=0.3374608497042918\n",
      "Gradient Descent(209/499): loss=0.3016359052283736, w0=-0.3146640093057205, gamma=0.896787141403505\n",
      "Gradient Descent(210/499): loss=0.30163459079236093, w0=-0.314664000961585, gamma=0.7912739257820933\n",
      "Gradient Descent(211/499): loss=0.3016311048856048, w0=-0.31466400020169183, gamma=0.10752052140944568\n",
      "Gradient Descent(212/499): loss=0.30162805445023233, w0=-0.31466400018013957, gamma=0.08747764424170092\n",
      "Gradient Descent(213/499): loss=0.30162762472880084, w0=-0.3146640001644902, gamma=0.07046538958535953\n",
      "Gradient Descent(214/499): loss=0.3016272857904527, w0=-0.31466400015298696, gamma=0.05890697302444908\n",
      "Gradient Descent(215/499): loss=0.3016270135266713, w0=-0.3146640001440482, gamma=0.09137542148501067\n",
      "Gradient Descent(216/499): loss=0.3016267860687077, w0=-0.3146640001309994, gamma=0.7509064383583022\n",
      "Gradient Descent(217/499): loss=0.3016264333772524, w0=-0.3146640000335651, gamma=1.4213593055703804\n",
      "Gradient Descent(218/499): loss=0.30162353821824406, w0=-0.3146639999876253, gamma=0.06342307885462108\n",
      "Gradient Descent(219/499): loss=0.30161811520057863, w0=-0.31466399998848904, gamma=0.050753020519182554\n",
      "Gradient Descent(220/499): loss=0.3016178383850191, w0=-0.3146639999891364, gamma=0.051833613824257795\n",
      "Gradient Descent(221/499): loss=0.30161764158627397, w0=-0.314663999989764, gamma=0.2576559761696336\n",
      "Gradient Descent(222/499): loss=0.30161744315257794, w0=-0.31466399999272204, gamma=0.30403053332189506\n",
      "Gradient Descent(223/499): loss=0.30161645811659954, w0=-0.31466399999531314, gamma=1.5011912730541754\n",
      "Gradient Descent(224/499): loss=0.30161529733614406, w0=-0.3146640000042174, gamma=4.425799063173251\n",
      "Gradient Descent(225/499): loss=0.3016095798131223, w0=-0.3146639999910621, gamma=0.167665852252166\n",
      "Gradient Descent(226/499): loss=0.30159304325012964, w0=-0.31466399999276967, gamma=0.06210202640333235\n",
      "Gradient Descent(227/499): loss=0.3015924331184903, w0=-0.31466399999329603, gamma=0.05104413803207161\n",
      "Gradient Descent(228/499): loss=0.30159202576303973, w0=-0.3146639999937018, gamma=0.05174220622700706\n",
      "Gradient Descent(229/499): loss=0.3015918252665822, w0=-0.31466399999409217, gamma=0.1444932068862857\n",
      "Gradient Descent(230/499): loss=0.30159163208993234, w0=-0.3146639999951259, gamma=0.18942616295608983\n",
      "Gradient Descent(231/499): loss=0.30159109391185057, w0=-0.31466399999628525, gamma=0.3938128556934242\n",
      "Gradient Descent(232/499): loss=0.30159038943526545, w0=-0.31466399999823896, gamma=0.3273688031563543\n",
      "Gradient Descent(233/499): loss=0.30158892639426915, w0=-0.3146639999992235, gamma=0.09571991742113432\n",
      "Gradient Descent(234/499): loss=0.3015877128714685, w0=-0.31466399999941713, gamma=0.08005421389497862\n",
      "Gradient Descent(235/499): loss=0.30158735709168005, w0=-0.31466399999956357, gamma=0.08762997387186078\n",
      "Gradient Descent(236/499): loss=0.3015870604645042, w0=-0.31466399999971106, gamma=0.18634837450255656\n",
      "Gradient Descent(237/499): loss=0.3015867359245424, w0=-0.3146639999999972, gamma=0.30399251619828616\n",
      "Gradient Descent(238/499): loss=0.30158604609412304, w0=-0.31466400000037703, gamma=0.08579187525629085\n",
      "Gradient Descent(239/499): loss=0.3015849220066418, w0=-0.31466400000045164, gamma=0.051999628469563956\n",
      "Gradient Descent(240/499): loss=0.30158460481624005, w0=-0.314664000000493, gamma=0.053493034759826294\n",
      "Gradient Descent(241/499): loss=0.30158441256394725, w0=-0.3146640000005333, gamma=3.7837366813381297\n",
      "Gradient Descent(242/499): loss=0.3015842149950196, w0=-0.3146640000032327, gamma=264.30371359011417\n",
      "Gradient Descent(243/499): loss=0.3015702937317089, w0=-0.3146639994785898, gamma=0.2575882853390581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(244/499): loss=0.30091289874364857, w0=-0.3146639996132378, gamma=0.05060742807672624\n",
      "Gradient Descent(245/499): loss=0.30185185692286737, w0=-0.3146639996328766, gamma=0.050599384048460365\n",
      "Gradient Descent(246/499): loss=0.30085273538076385, w0=-0.31466399965151925, gamma=0.11636121380451225\n",
      "Gradient Descent(247/499): loss=0.30085263784126204, w0=-0.31466399969222164, gamma=0.11829037233465063\n",
      "Gradient Descent(248/499): loss=0.3008524674723559, w0=-0.3146639997287841, gamma=0.13008299907190035\n",
      "Gradient Descent(249/499): loss=0.30085230898284004, w0=-0.3146639997642354, gamma=0.305234246666079\n",
      "Gradient Descent(250/499): loss=0.30085213484853307, w0=-0.3146639998365995, gamma=0.6414416906885817\n",
      "Gradient Descent(251/499): loss=0.30085172655613746, w0=-0.31466399994225336, gamma=0.14906776546001227\n",
      "Gradient Descent(252/499): loss=0.300850870372904, w0=-0.3146639999510572, gamma=0.08085190429482284\n",
      "Gradient Descent(253/499): loss=0.3008506715927976, w0=-0.31466399995512045, gamma=0.08126121547842625\n",
      "Gradient Descent(254/499): loss=0.30085056352049216, w0=-0.31466399995887406, gamma=5.571861901778413\n",
      "Gradient Descent(255/499): loss=0.30085045531473037, w0=-0.3146640001953355, gamma=71.58476514125466\n",
      "Gradient Descent(256/499): loss=0.300843079073674, w0=-0.31466398630626213, gamma=0.19653370236371612\n",
      "Gradient Descent(257/499): loss=0.30075784266776556, w0=-0.3146639889978023, gamma=0.05145215854219483\n",
      "Gradient Descent(258/499): loss=0.30077187222658086, w0=-0.31466398956395697, gamma=0.05081822418787649\n",
      "Gradient Descent(259/499): loss=0.3007554811925112, w0=-0.31466399009436513, gamma=0.0730261013557811\n",
      "Gradient Descent(260/499): loss=0.3007553684436796, w0=-0.3146639908178313, gamma=0.08007641055462415\n",
      "Gradient Descent(261/499): loss=0.30075527930277657, w0=-0.314663991553212, gamma=0.09370849707514392\n",
      "Gradient Descent(262/499): loss=0.30075519010426477, w0=-0.31466399234487136, gamma=0.36562752591688036\n",
      "Gradient Descent(263/499): loss=0.30075508584965305, w0=-0.31466399514427923, gamma=4.410312577038948\n",
      "Gradient Descent(264/499): loss=0.30075467926883737, w0=-0.31466401656534176, gamma=0.4895881459499694\n",
      "Gradient Descent(265/499): loss=0.3007497949838778, w0=-0.3146640084557935, gamma=0.05506331175656865\n",
      "Gradient Descent(266/499): loss=0.30074935498313615, w0=-0.3146640079902623, gamma=0.05123693102412855\n",
      "Gradient Descent(267/499): loss=0.30074919809659256, w0=-0.3146640075809335, gamma=0.06750540283234795\n",
      "Gradient Descent(268/499): loss=0.30074913916178964, w0=-0.3146640070692687, gamma=0.1175709338063527\n",
      "Gradient Descent(269/499): loss=0.30074906433104676, w0=-0.31466400623828356, gamma=0.1240384662050741\n",
      "Gradient Descent(270/499): loss=0.30074893517840057, w0=-0.31466400546466033, gamma=1.2044226879079702\n",
      "Gradient Descent(271/499): loss=0.3007487991361776, w0=-0.3146639988844909, gamma=1.690556917127731\n",
      "Gradient Descent(272/499): loss=0.30074747940793284, w0=-0.3146640007725574, gamma=0.05450888364628422\n",
      "Gradient Descent(273/499): loss=0.3007456578874147, w0=-0.31466400073051826, gamma=0.050674816920274354\n",
      "Gradient Descent(274/499): loss=0.30074557344677183, w0=-0.3146640006935664, gamma=0.05400700123856794\n",
      "Gradient Descent(275/499): loss=0.30074551802838734, w0=-0.31466400065618044, gamma=0.2013149471490774\n",
      "Gradient Descent(276/499): loss=0.3007454591733293, w0=-0.3146640005243478, gamma=0.2943280588840745\n",
      "Gradient Descent(277/499): loss=0.30074523992791535, w0=-0.31466400037040687, gamma=0.5900731571210172\n",
      "Gradient Descent(278/499): loss=0.3007449196006998, w0=-0.3146640001526204, gamma=0.1608552686620363\n",
      "Gradient Descent(279/499): loss=0.3007442780597109, w0=-0.3146640001282835, gamma=0.08265666785367062\n",
      "Gradient Descent(280/499): loss=0.300744103308257, w0=-0.3146640001177894, gamma=0.08157615454941322\n",
      "Gradient Descent(281/499): loss=0.3007440133899945, w0=-0.31466400010828854, gamma=0.5812086976739743\n",
      "Gradient Descent(282/499): loss=0.3007439248110232, w0=-0.3146640000461195, gamma=2.7577040228559193\n",
      "Gradient Descent(283/499): loss=0.3007432939915224, w0=-0.3146639999225861, gamma=0.07702495532907296\n",
      "Gradient Descent(284/499): loss=0.3007403161539916, w0=-0.31466399992865096, gamma=0.050699495900800695\n",
      "Gradient Descent(285/499): loss=0.30074022713146537, w0=-0.3146639999323355, gamma=0.05066299894582146\n",
      "Gradient Descent(286/499): loss=0.3007401702712985, w0=-0.3146639999358307, gamma=0.3081574060501676\n",
      "Gradient Descent(287/499): loss=0.30074011566077063, w0=-0.3146639999560133, gamma=0.7603700600609635\n",
      "Gradient Descent(288/499): loss=0.3007397835881916, w0=-0.31466399999046707, gamma=0.336932451164477\n",
      "Gradient Descent(289/499): loss=0.3007389650017662, w0=-0.31466399999412564, gamma=0.1205852557422453\n",
      "Gradient Descent(290/499): loss=0.300738603018611, w0=-0.31466399999499384, gamma=0.1151255802724782\n",
      "Gradient Descent(291/499): loss=0.3007384729877848, w0=-0.3146639999957228, gamma=0.35852751623770485\n",
      "Gradient Descent(292/499): loss=0.300738349287586, w0=-0.3146639999977316, gamma=2.895144349978989\n",
      "Gradient Descent(293/499): loss=0.3007379641968131, w0=-0.3146640000081375, gamma=0.2132720846627976\n",
      "Gradient Descent(294/499): loss=0.3007348623146383, w0=-0.31466400000668504, gamma=0.057080533734498624\n",
      "Gradient Descent(295/499): loss=0.30073464396103083, w0=-0.31466400000637923, gamma=0.052507200381971104\n",
      "Gradient Descent(296/499): loss=0.30073457233925577, w0=-0.31466400000611394, gamma=0.05911780033002344\n",
      "Gradient Descent(297/499): loss=0.3007345159851405, w0=-0.31466400000583095, gamma=0.09107813070863162\n",
      "Gradient Descent(298/499): loss=0.3007344528873111, w0=-0.3146640000054208, gamma=0.20504689064615597\n",
      "Gradient Descent(299/499): loss=0.3007343557274997, w0=-0.31466400000458145, gamma=6.201777936316525\n",
      "Gradient Descent(300/499): loss=0.30073413704268803, w0=-0.31466399998440076, gamma=1.3350676366881855\n",
      "Gradient Descent(301/499): loss=0.3007275445258343, w0=-0.31466400000700234, gamma=0.05513664982446296\n",
      "Gradient Descent(302/499): loss=0.30072655152331085, w0=-0.31466400000668965, gamma=0.0524805851022153\n",
      "Gradient Descent(303/499): loss=0.30072609006688306, w0=-0.3146640000064084, gamma=0.06373186876851429\n",
      "Gradient Descent(304/499): loss=0.30072602483001737, w0=-0.31466400000608474, gamma=0.07979994684456482\n",
      "Gradient Descent(305/499): loss=0.300725956854735, w0=-0.3146640000057054, gamma=0.08562645792526316\n",
      "Gradient Descent(306/499): loss=0.3007258729047559, w0=-0.3146640000053308, gamma=0.5898276746719204\n",
      "Gradient Descent(307/499): loss=0.3007257828836016, w0=-0.31466400000297134, gamma=1.4558184079219032\n",
      "Gradient Descent(308/499): loss=0.3007251630131381, w0=-0.314664000000583, gamma=0.06386565004815044\n",
      "Gradient Descent(309/499): loss=0.3007236385725209, w0=-0.3146640000006308, gamma=0.05068115770020507\n",
      "Gradient Descent(310/499): loss=0.3007235684507073, w0=-0.3146640000006663, gamma=0.05105071840699137\n",
      "Gradient Descent(311/499): loss=0.3007235151228211, w0=-0.3146640000007003, gamma=0.35587061823224825\n",
      "Gradient Descent(312/499): loss=0.3007234616517026, w0=-0.31466400000092487, gamma=0.8167481938896081\n",
      "Gradient Descent(313/499): loss=0.30072308900604044, w0=-0.314664000001257, gamma=0.3095311453790381\n",
      "Gradient Descent(314/499): loss=0.3007222345497535, w0=-0.31466400000128014, gamma=0.1259110072407311\n",
      "Gradient Descent(315/499): loss=0.30072191124136455, w0=-0.3146640000012867, gamma=0.12080639196636564\n",
      "Gradient Descent(316/499): loss=0.3007217793780271, w0=-0.3146640000012922, gamma=0.28557010152664436\n",
      "Gradient Descent(317/499): loss=0.30072165319702787, w0=-0.31466400000130357, gamma=0.9245230493149077\n",
      "Gradient Descent(318/499): loss=0.3007213550081302, w0=-0.31466400000133, gamma=0.10386692277669027\n",
      "Gradient Descent(319/499): loss=0.30072039067233247, w0=-0.31466400000133027, gamma=0.051799680566388215\n",
      "Gradient Descent(320/499): loss=0.3007202824122838, w0=-0.3146640000013304, gamma=0.050817142302843975\n",
      "Gradient Descent(321/499): loss=0.3007202281006175, w0=-0.3146640000013305, gamma=0.2668980810326642\n",
      "Gradient Descent(322/499): loss=0.300720175150276, w0=-0.31466400000133105, gamma=4.786919589503534\n",
      "Gradient Descent(323/499): loss=0.30071989709826813, w0=-0.3146640000013387, gamma=1.0490958791748604\n",
      "Gradient Descent(324/499): loss=0.30071492217628865, w0=-0.31466400000133415, gamma=0.0823309835979041\n",
      "Gradient Descent(325/499): loss=0.3007139465484568, w0=-0.31466400000133415, gamma=0.08147883820769002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(326/499): loss=0.300713752998885, w0=-0.3146640000013342, gamma=0.06408781927525825\n",
      "Gradient Descent(327/499): loss=0.3007136686365772, w0=-0.31466400000133427, gamma=0.05255033061185371\n",
      "Gradient Descent(328/499): loss=0.300713602498468, w0=-0.3146640000013343, gamma=0.07377418713604704\n",
      "Gradient Descent(329/499): loss=0.3007135482951211, w0=-0.3146640000013344, gamma=5.572259549983891\n",
      "Gradient Descent(330/499): loss=0.30071347221046363, w0=-0.31466400000133693, gamma=36.078619778698666\n",
      "Gradient Descent(331/499): loss=0.3007077383400166, w0=-0.3146640000013311, gamma=0.0636590631170841\n",
      "Gradient Descent(332/499): loss=0.3006770364068391, w0=-0.3146640000013322, gamma=0.052455311740432345\n",
      "Gradient Descent(333/499): loss=0.3006718283917397, w0=-0.31466400000133304, gamma=0.05530160167621443\n",
      "Gradient Descent(334/499): loss=0.3006713097515439, w0=-0.3146640000013339, gamma=0.10915173964028334\n",
      "Gradient Descent(335/499): loss=0.30067120139991865, w0=-0.3146640000013355, gamma=0.11330695721180861\n",
      "Gradient Descent(336/499): loss=0.3006710762060937, w0=-0.314664000001337, gamma=0.08852520208213122\n",
      "Gradient Descent(337/499): loss=0.30067096621839257, w0=-0.31466400000133804, gamma=0.05804930765657431\n",
      "Gradient Descent(338/499): loss=0.30067088034719935, w0=-0.31466400000133865, gamma=0.06134817033839089\n",
      "Gradient Descent(339/499): loss=0.300670824043503, w0=-0.31466400000133926, gamma=0.541144007028952\n"
     ]
    }
   ],
   "source": [
    "w = np.array(np.random.rand((tX.shape[1])))\n",
    "max_iters = 500 #choosing number of iterations\n",
    "gamma = 5 *10**(-2)\n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)\n",
    "(w, loss) =adaptative_step_gradient_descent(y, tX, w, max_iters, gamma)\n",
    "#gamma = 5/max_iters #choosing gamma \n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tX.shape[1]):\n",
    "    col = tX[:,i]\n",
    "    places_999 = (col == -999.0)\n",
    "    mean_col = col[col!=-999.0].mean()\n",
    "    col[places_999] = mean_col\n",
    "    tX[:,i] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = build_poly(tX_test, degree)\n",
    "tX_test = tX_test[:,1:]\n",
    "tX_test = (tX_test-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = np.c_[(np.ones(tX_test.shape[0]) , tX_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
