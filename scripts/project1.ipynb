{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from extend import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree = 4\n",
    "#tX = build_poly(tX, degree)\n",
    "#tX = tX[:,1:]\n",
    "tX = extend(tX,[np.cos,np.sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 90)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = (tX - np.mean(tX,axis = 0))/(np.std(tX,axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[(np.ones(tX.shape[0]) , tX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 91)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=31.178392724618256, w0=0.6796479937999426, w1=0.4767322451524528\n",
      "Gradient Descent(1/499): loss=14.104898507360678, w0=0.6299323941096968, w1=0.47526595322765697\n",
      "Gradient Descent(2/499): loss=11.870437080509808, w0=0.5827025744039789, w1=0.4751740823181678\n",
      "Gradient Descent(3/499): loss=10.13218509217815, w0=0.5378342456835647, w1=0.4742030570689656\n",
      "Gradient Descent(4/499): loss=8.745431757216368, w0=0.4952093333991877, w1=0.47243869698271523\n",
      "Gradient Descent(5/499): loss=7.625554268617974, w0=0.454715666729043, w1=0.47004123056673996\n",
      "Gradient Descent(6/499): loss=6.7104888928581365, w0=0.41624668339241716, w1=0.46714765107772926\n",
      "Gradient Descent(7/499): loss=5.9543104108315426, w0=0.37970114922263243, w1=0.46387102762798643\n",
      "Gradient Descent(8/499): loss=5.322745710549761, w0=0.3449828917613458, w1=0.46030381858378544\n",
      "Gradient Descent(9/499): loss=4.789991850788796, w0=0.3120005471731306, w1=0.4565210048649859\n",
      "Gradient Descent(10/499): loss=4.336445794089638, w0=0.28066731981433213, w1=0.4525828623006769\n",
      "Gradient Descent(11/499): loss=3.9470749647417653, w0=0.2509007538234792, w1=0.4485373830241017\n",
      "Gradient Descent(12/499): loss=3.6102407688237212, w0=0.22262251613217315, w1=0.4444223711006987\n",
      "Gradient Descent(13/499): loss=3.3168440486988993, w0=0.19575819032543643, w1=0.44026724166355374\n",
      "Gradient Descent(14/499): loss=3.0597005897102987, w0=0.17023708080904, w1=0.4360945541643247\n",
      "Gradient Descent(15/499): loss=2.8330819061408246, w0=0.14599202676846632, w1=0.4319213100480584\n",
      "Gradient Descent(16/499): loss=2.6323754081417934, w0=0.12295922542992366, w1=0.4277600438144741\n",
      "Gradient Descent(17/499): loss=2.4538312612148934, w0=0.10107806415831028, w1=0.42361973444957474\n",
      "Gradient Descent(18/499): loss=2.2943725419923977, w0=0.08029096095027918, w1=0.4195065618929877\n",
      "Gradient Descent(19/499): loss=2.1514518626110473, w0=0.06054321290265116, w1=0.4154245307523967\n",
      "Gradient Descent(20/499): loss=2.022942301486974, w0=0.0417828522574058, w1=0.41137598102534445\n",
      "Gradient Descent(21/499): loss=1.907053807607019, w0=0.0239605096444238, w1=0.40736200323108473\n",
      "Gradient Descent(22/499): loss=1.8022686321087966, w0=0.007029284162091861, w1=0.40338277314684656\n",
      "Gradient Descent(23/499): loss=1.7072910595180046, w0=-0.00905538004612269, w1=0.39943781931486827\n",
      "Gradient Descent(24/499): loss=1.621007954046291, w0=-0.02433581104392578, w1=0.39552623465253794\n",
      "Gradient Descent(25/499): loss=1.54245753939486, w0=-0.03885222049183806, w1=0.3916468418597476\n",
      "Gradient Descent(26/499): loss=1.4708044894551928, w0=-0.05264280946735427, w1=0.38779832086909405\n",
      "Gradient Descent(27/499): loss=1.4053198902722945, w0=-0.0657438689940943, w1=0.3839793053148905\n",
      "Gradient Descent(28/499): loss=1.345364989231261, w0=-0.07818987554449694, w1=0.38018845389230455\n",
      "Gradient Descent(29/499): loss=1.290377910445753, w0=-0.09001358176737928, w1=0.37642450152319135\n",
      "Gradient Descent(30/499): loss=1.2398627107876687, w0=-0.10124610267911721, w1=0.37268629442486945\n",
      "Gradient Descent(31/499): loss=1.1933802969585035, w0=-0.11191699754526802, w1=0.36897281247708746\n",
      "Gradient Descent(32/499): loss=1.150540833546055, w0=-0.12205434766811107, w1=0.36528318168639823\n",
      "Gradient Descent(33/499): loss=1.1109973546473233, w0=-0.1316848302848119, w1=0.361616679042798\n",
      "Gradient Descent(34/499): loss=1.0744403543121142, w0=-0.14083378877067748, w1=0.3579727316386511\n",
      "Gradient Descent(35/499): loss=1.0405931788580354, w0=-0.1495252993322497, w1=0.35435091156367504\n",
      "Gradient Descent(36/499): loss=1.0092080807663855, w0=-0.15778223436574312, w1=0.3507509277923652\n",
      "Gradient Descent(37/499): loss=0.9800628221509011, w0=-0.16562632264756175, w1=0.3471726160330926\n",
      "Gradient Descent(38/499): loss=0.9529577377436409, w0=-0.17307820651528935, w1=0.34361592730367335\n",
      "Gradient Descent(39/499): loss=0.9277131844870261, w0=-0.18015749618963045, w1=0.34008091582992955\n",
      "Gradient Descent(40/499): loss=0.9041673182958122, w0=-0.18688282138025442, w1=0.3365677267259533\n",
      "Gradient Descent(41/499): loss=0.8821741492111367, w0=-0.19327188031134707, w1=0.3330765838025697\n",
      "Gradient Descent(42/499): loss=0.8616018346540862, w0=-0.19934148629588497, w1=0.329607777759671\n",
      "Gradient Descent(43/499): loss=0.8423311772853813, w0=-0.20510761198119587, w1=0.3261616549450964\n",
      "Gradient Descent(44/499): loss=0.8242542994612332, w0=-0.21058543138224114, w1=0.32273860680451977\n",
      "Gradient Descent(45/499): loss=0.8072734707261694, w0=-0.21578935981323397, w1=0.31933906010080515\n",
      "Gradient Descent(46/499): loss=0.7913000684190291, w0=-0.22073309182267706, w1=0.31596346794533725\n",
      "Gradient Descent(47/499): loss=0.7762536544561653, w0=-0.22542963723164786, w1=0.3126123016560964\n",
      "Gradient Descent(48/499): loss=0.7620611538262939, w0=-0.22989135537016991, w1=0.3092860434362084\n",
      "Gradient Descent(49/499): loss=0.7486561223859146, w0=-0.2341299876017658, w1=0.3059851798510771\n",
      "Gradient Descent(50/499): loss=0.7359780932624164, w0=-0.23815668822178168, w1=0.3027101960709457\n",
      "Gradient Descent(51/499): loss=0.7239719926165601, w0=-0.24198205381079663, w1=0.29946157083795427\n",
      "Gradient Descent(52/499): loss=0.7125876167368269, w0=-0.24561615112036067, w1=0.2962397721117356\n",
      "Gradient Descent(53/499): loss=0.7017791634747095, w0=-0.24906854356444633, w1=0.29304525334473147\n",
      "Gradient Descent(54/499): loss=0.6915048119142584, w0=-0.2523483163863275, w1=0.2898784503372246\n",
      "Gradient Descent(55/499): loss=0.6817263449266386, w0=-0.2554641005671144, w1=0.2867397786221685\n",
      "Gradient Descent(56/499): loss=0.6724088099119119, w0=-0.25842409553886175, w1=0.2836296313309451\n",
      "Gradient Descent(57/499): loss=0.6635202135926266, w0=-0.26123609076202153, w1=0.2805483774929195\n",
      "Gradient Descent(58/499): loss=0.6550312472109499, w0=-0.2639074862240231, w1=0.27749636072389205\n",
      "Gradient Descent(59/499): loss=0.6469150389044345, w0=-0.26644531191292437, w1=0.27447389826110274\n",
      "Gradient Descent(60/499): loss=0.6391469304045033, w0=-0.2688562463173803, w1=0.2714812803051935\n",
      "Gradient Descent(61/499): loss=0.6317042755242668, w0=-0.27114663400161326, w1=0.26851876963237786\n",
      "Gradient Descent(62/499): loss=0.6245662581848836, w0=-0.2733225023016343, w1=0.2655866014429314\n",
      "Gradient Descent(63/499): loss=0.6177137279778975, w0=-0.275389577186654, w1=0.26268498341493507\n",
      "Gradient Descent(64/499): loss=0.6111290514794658, w0=-0.2773532983274225, w1=0.2598140959349439\n",
      "Gradient Descent(65/499): loss=0.6047959777251055, w0=-0.2792188334111523, w1=0.2569740924798758\n",
      "Gradient Descent(66/499): loss=0.5986995164238452, w0=-0.2809910917406953, w1=0.25416510012690197\n",
      "Gradient Descent(67/499): loss=0.5928258276414032, w0=-0.2826747371537609, w1=0.25138722017046355\n",
      "Gradient Descent(68/499): loss=0.587162121815616, w0=-0.28427420029617295, w1=0.2486405288277189\n",
      "Gradient Descent(69/499): loss=0.5816965690859777, w0=-0.2857936902814641, w1=0.24592507801575628\n",
      "Gradient Descent(70/499): loss=0.5764182170246125, w0=-0.2872372057674904, w1=0.24324089618577321\n",
      "Gradient Descent(71/499): loss=0.5713169159498844, w0=-0.2886085454792151, w1=0.24058798920114033\n",
      "Gradient Descent(72/499): loss=0.566383251087515, w0=-0.28991131820535326, w1=0.23796634124783642\n",
      "Gradient Descent(73/499): loss=0.5616084809187215, w0=-0.2911489522951842, w1=0.23537591576716707\n",
      "Gradient Descent(74/499): loss=0.5569844811215554, w0=-0.2923247046805233, w1=0.2328166564019749\n",
      "Gradient Descent(75/499): loss=0.5525036935712087, w0=-0.2934416694465951, w1=0.23028848794871695\n",
      "Gradient Descent(76/499): loss=0.5481590799183722, w0=-0.294502785974363, w1=0.2277913173088391\n",
      "Gradient Descent(77/499): loss=0.5439440793124776, w0=-0.29551084667574223, w1=0.22532503443382101\n",
      "Gradient Descent(78/499): loss=0.5398525698794338, w0=-0.29646850434205213, w1=0.22288951325911144\n",
      "Gradient Descent(79/499): loss=0.5358788336018432, w0=-0.2973782791250462, w1=0.2204846126229272\n",
      "Gradient Descent(80/499): loss=0.5320175242841215, w0=-0.29824256516889025, w1=0.21811017716656028\n",
      "Gradient Descent(81/499): loss=0.5282636383158585, w0=-0.2990636369105418, w1=0.2157660382134312\n",
      "Gradient Descent(82/499): loss=0.5246124879745596, w0=-0.2998436550651104, w1=0.21345201462465263\n",
      "Gradient Descent(83/499): loss=0.5210596770338813, w0=-0.30058467231195024, w1=0.21116791362932943\n",
      "Gradient Descent(84/499): loss=0.5176010784659583, w0=-0.30128863869644773, w1=0.20891353162822723\n",
      "Gradient Descent(85/499): loss=0.5142328140466513, w0=-0.30195740676172, w1=0.20668865496979652\n",
      "Gradient Descent(86/499): loss=0.5109512356907654, w0=-0.30259273642372836, w1=0.20449306069784848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(87/499): loss=0.5077529083607047, w0=-0.30319629960263594, w1=0.20232651727044665\n",
      "Gradient Descent(88/499): loss=0.5046345944068397, w0=-0.3037696846225978, w1=0.20018878524980974\n",
      "Gradient Descent(89/499): loss=0.5015932392111957, w0=-0.3043144003915612, w1=0.19807961796321935\n",
      "Gradient Descent(90/499): loss=0.49862595801813503, w0=-0.3048318803720761, w1=0.19599876213509607\n",
      "Gradient Descent(91/499): loss=0.4957300238465588, w0=-0.3053234863535649, w1=0.1939459584905501\n",
      "Gradient Descent(92/499): loss=0.49290285638798675, w0=-0.3057905120359789, w1=0.19192094233083365\n",
      "Gradient Descent(93/499): loss=0.49014201180373906, w0=-0.30623418643427186, w1=0.1899234440812222\n",
      "Gradient Descent(94/499): loss=0.4874451733424563, w0=-0.30665567711264985, w1=0.18795318981193404\n",
      "Gradient Descent(95/499): loss=0.4848101427064512, w0=-0.3070560932571086, w1=0.18600990173276433\n",
      "Gradient Descent(96/499): loss=0.4822348321019359, w0=-0.307436488594344, w1=0.18409329866216215\n",
      "Gradient Descent(97/499): loss=0.47971725691410183, w0=-0.3077978641647173, w1=0.18220309647152064\n",
      "Gradient Descent(98/499): loss=0.47725552895340095, w0=-0.3081411709565716, w1=0.18033900850547965\n",
      "Gradient Descent(99/499): loss=0.4748478502242373, w0=-0.30846731240883285, w1=0.17850074597906254\n",
      "Gradient Descent(100/499): loss=0.4724925071716855, w0=-0.30877714678848067, w1=0.17668801835248088\n",
      "Gradient Descent(101/499): loss=0.47018786536583945, w0=-0.30907148944914575, w1=0.1749005336844487\n",
      "Gradient Descent(102/499): loss=0.4679323645870158, w0=-0.3093511149767772, w1=0.17313799896484755\n",
      "Gradient Descent(103/499): loss=0.46572451427831535, w0=-0.30961675922802673, w1=0.1714001204275802\n",
      "Gradient Descent(104/499): loss=0.46356288933502415, w0=-0.30986912126671345, w1=0.16968660384444204\n",
      "Gradient Descent(105/499): loss=0.4614461262030318, w0=-0.3101088652034655, w1=0.16799715480082753\n",
      "Gradient Descent(106/499): loss=0.4593729192609002, w0=-0.3103366219433796, w1=0.16633147895407463\n",
      "Gradient Descent(107/499): loss=0.45734201746243724, w0=-0.3105529908462976, w1=0.16468928227523247\n",
      "Gradient Descent(108/499): loss=0.45535222121865837, w0=-0.3107585413040694, w1=0.1630702712750187\n",
      "Gradient Descent(109/499): loss=0.45340237949984447, w0=-0.3109538142389522, w1=0.16147415321471248\n",
      "Gradient Descent(110/499): loss=0.45149138714008974, w0=-0.3111393235270905, w1=0.1599006363027074\n",
      "Gradient Descent(111/499): loss=0.44961818232823614, w0=-0.31131555735082156, w1=0.15834942987742526\n",
      "Gradient Descent(112/499): loss=0.44778174427048384, w0=-0.31148297948336573, w1=0.1568202445772696\n",
      "Gradient Descent(113/499): loss=0.445981091011217, w0=-0.31164203050928235, w1=0.1553127924982728\n",
      "Gradient Descent(114/499): loss=0.44421527739972944, w0=-0.3117931289839028, w1=0.15382678734006797\n",
      "Gradient Descent(115/499): loss=0.44248339319158053, w0=-0.3119366725347919, w1=0.15236194454079216\n",
      "Gradient Descent(116/499): loss=0.440784561274251, w0=-0.3120730389081362, w1=0.15091798140150375\n",
      "Gradient Descent(117/499): loss=0.439117936007646, w0=-0.31220258696281294, w1=0.1494946172006737\n",
      "Gradient Descent(118/499): loss=0.4374827016707659, w0=-0.3123256576147555, w1=0.1480915732992866\n",
      "Gradient Descent(119/499): loss=0.4358780710065974, w0=-0.3124425747341006, w1=0.1467085732370651\n",
      "Gradient Descent(120/499): loss=0.43430328385792283, w0=-0.3125536459974781, w1=0.1453453428203085\n",
      "Gradient Descent(121/499): loss=0.43275760588735035, w0=-0.31265916369768637, w1=0.14400161020181482\n",
      "Gradient Descent(122/499): loss=0.43124032737541074, w0=-0.31275940551288395, w1=0.14267710595333413\n",
      "Gradient Descent(123/499): loss=0.42975076209106255, w0=-0.3128546352373213, w1=0.14137156313098062\n",
      "Gradient Descent(124/499): loss=0.4282882462294127, w0=-0.31294510347553645, w1=0.14008471733401057\n",
      "Gradient Descent(125/499): loss=0.42685213741186134, w0=-0.31303104830184053, w1=0.13881630675735412\n",
      "Gradient Descent(126/499): loss=0.42544181374427475, w0=-0.31311269588682905, w1=0.13756607223827022\n",
      "Gradient Descent(127/499): loss=0.42405667292912835, w0=-0.3131902610925678, w1=0.13633375729747585\n",
      "Gradient Descent(128/499): loss=0.4226961314278849, w0=-0.3132639480380193, w1=0.13511910817508346\n",
      "Gradient Descent(129/499): loss=0.42135962367016566, w0=-0.31333395063619796, w1=0.1339218738616638\n",
      "Gradient Descent(130/499): loss=0.42004660130653454, w0=-0.31340045310446735, w1=0.1327418061247352\n",
      "Gradient Descent(131/499): loss=0.41875653250196504, w0=-0.3134636304493229, w1=0.1315786595309655\n",
      "Gradient Descent(132/499): loss=0.41748890126728233, w0=-0.3135236489269354, w1=0.13043219146435706\n",
      "Gradient Descent(133/499): loss=0.4162432068260779, w0=-0.31358066648066696, w1=0.12930216214067236\n",
      "Gradient Descent(134/499): loss=0.415018963014781, w0=-0.3136348331567116, w1=0.12818833461834367\n",
      "Gradient Descent(135/499): loss=0.4138156977137529, w0=-0.3136862914989537, w1=0.12709047480609686\n",
      "Gradient Descent(136/499): loss=0.4126329523074153, w0=-0.3137351769240834, w1=0.12600835146750844\n",
      "Gradient Descent(137/499): loss=0.41147028117158374, w0=-0.31378161807795635, w1=0.1249417362227016\n",
      "Gradient Descent(138/499): loss=0.41032725118630103, w0=-0.3138257371741353, w1=0.1238904035473774\n",
      "Gradient Descent(139/499): loss=0.40920344127259883, w0=-0.313867650315505, w1=0.12285413076936504\n",
      "Gradient Descent(140/499): loss=0.4080984419517174, w0=-0.3139074677998059, w1=0.12183269806286628\n",
      "Gradient Descent(141/499): loss=0.4070118549254279, w0=-0.31394529440989144, w1=0.1208258884405587\n",
      "Gradient Descent(142/499): loss=0.4059432926761973, w0=-0.3139812296894724, w1=0.11983348774371334\n",
      "Gradient Descent(143/499): loss=0.4048923780860144, w0=-0.31401536820507403, w1=0.1188552846304738\n",
      "Gradient Descent(144/499): loss=0.40385874407279304, w0=-0.3140477997948953, w1=0.11789107056243527\n",
      "Gradient Descent(145/499): loss=0.40284203324332923, w0=-0.3140786098052252, w1=0.11694063978965423\n",
      "Gradient Descent(146/499): loss=0.40184189756187083, w0=-0.31410787931503825, w1=0.11600378933421203\n",
      "Gradient Descent(147/499): loss=0.4008579980334092, w0=-0.3141356853493604, w1=0.1150803189724483\n",
      "Gradient Descent(148/499): loss=0.3998900044008797, w0=-0.3141621010819662, w1=0.11417003121597367\n",
      "Gradient Descent(149/499): loss=0.39893759485549185, w0=-0.3141871960279414, w1=0.11327273129156455\n",
      "Gradient Descent(150/499): loss=0.39800045575947907, w0=-0.3142110362266175, w1=0.11238822712003693\n",
      "Gradient Descent(151/499): loss=0.3970782813805932, w0=-0.31423368441535954, w1=0.11151632929419017\n",
      "Gradient Descent(152/499): loss=0.3961707736377178, w0=-0.3142552001946642, w1=0.11065685105590647\n",
      "Gradient Descent(153/499): loss=0.395277641857011, w0=-0.3142756401850033, w1=0.10980960827248652\n",
      "Gradient Descent(154/499): loss=0.3943986025380335, w0=-0.3142950581758252, w1=0.10897441941229676\n",
      "Gradient Descent(155/499): loss=0.39353337912933867, w0=-0.3143135052671057, w1=0.10815110551979931\n",
      "Gradient Descent(156/499): loss=0.3926817018130474, w0=-0.31433103000382195, w1=0.10733949019003113\n",
      "Gradient Descent(157/499): loss=0.3918433072979543, w0=-0.3143476785037021, w1=0.10653939954259477\n",
      "Gradient Descent(158/499): loss=0.3910179386207351, w0=-0.314363494578588, w1=0.10575066219521936\n",
      "Gradient Descent(159/499): loss=0.3902053449548613, w0=-0.31437851984972925, w1=0.10497310923694643\n",
      "Gradient Descent(160/499): loss=0.3894052814268419, w0=-0.3143927938573132, w1=0.10420657420099201\n",
      "Gradient Descent(161/499): loss=0.38861750893944036, w0=-0.31440635416451773, w1=0.10345089303733308\n",
      "Gradient Descent(162/499): loss=0.3878417940015341, w0=-0.3144192364563617, w1=0.10270590408506311\n",
      "Gradient Descent(163/499): loss=0.3870779085643037, w0=-0.31443147463361326, w1=0.10197144804455865\n",
      "Gradient Descent(164/499): loss=0.38632562986345287, w0=-0.31444310090200195, w1=0.10124736794949628\n",
      "Gradient Descent(165/499): loss=0.3855847402671858, w0=-0.31445414585697096, w1=0.10053350913875615\n",
      "Gradient Descent(166/499): loss=0.3848550271296761, w0=-0.31446463856419127, w1=0.0998297192282465\n",
      "Gradient Descent(167/499): loss=0.38413628264977706, w0=-0.3144746066360503, w1=0.09913584808268062\n",
      "Gradient Descent(168/499): loss=0.3834283037347439, w0=-0.31448407630431613, w1=0.09845174778733591\n",
      "Gradient Descent(169/499): loss=0.38273089186874193, w0=-0.3144930724891684, w1=0.0977772726198223\n",
      "Gradient Descent(170/499): loss=0.3820438529859326, w0=-0.3145016188647778, w1=0.09711227902188581\n",
      "Gradient Descent(171/499): loss=0.38136699734793844, w0=-0.31450973792160647, w1=0.09645662557127051\n",
      "Gradient Descent(172/499): loss=0.38070013942550074, w0=-0.3145174510255935, w1=0.09581017295366105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(173/499): loss=0.3800430977841484, w0=-0.3145247784743809, w1=0.09517278393472596\n",
      "Gradient Descent(174/499): loss=0.3793956949737125, w0=-0.31453173955072866, w1=0.09454432333228029\n",
      "Gradient Descent(175/499): loss=0.3787577574215243, w0=-0.3145383525732588, w1=0.09392465798858508\n",
      "Gradient Descent(176/499): loss=0.37812911532914567, w0=-0.31454463494466217, w1=0.09331365674279951\n",
      "Gradient Descent(177/499): loss=0.3775096025724876, w0=-0.31455060319749517, w1=0.09271119040360015\n",
      "Gradient Descent(178/499): loss=0.3768990566051785, w0=-0.3145562730376863, w1=0.09211713172198112\n",
      "Gradient Descent(179/499): loss=0.3762973183650533, w0=-0.3145616593858676, w1=0.09153135536424703\n",
      "Gradient Descent(180/499): loss=0.3757042321836393, w0=-0.3145667764166396, w1=0.09095373788521031\n",
      "Gradient Descent(181/499): loss=0.37511964569851897, w0=-0.31457163759587276, w1=0.09038415770160291\n",
      "Gradient Descent(182/499): loss=0.3745434097684601, w0=-0.314576255716144, w1=0.08982249506571169\n",
      "Gradient Descent(183/499): loss=0.3739753783912032, w0=-0.3145806429304015, w1=0.08926863203924609\n",
      "Gradient Descent(184/499): loss=0.3734154086238081, w0=-0.31458481078394585, w1=0.08872245246744545\n",
      "Gradient Descent(185/499): loss=0.37286336050545826, w0=-0.31458877024481274, w1=0.08818384195343289\n",
      "Gradient Descent(186/499): loss=0.3723190969826324, w0=-0.31459253173263607, w1=0.08765268783282193\n",
      "Gradient Descent(187/499): loss=0.37178248383655654, w0=-0.314596105146068, w1=0.08712887914858111\n",
      "Gradient Descent(188/499): loss=0.3712533896128473, w0=-0.3145994998888281, w1=0.08661230662616147\n",
      "Gradient Descent(189/499): loss=0.37073168555327074, w0=-0.31460272489445, w1=0.08610286264889107\n",
      "Gradient Descent(190/499): loss=0.37021724552953644, w0=-0.3146057886497906, w1=0.08560044123364018\n",
      "Gradient Descent(191/499): loss=0.3697099459790549, w0=-0.31460869921736395, w1=0.08510493800676021\n",
      "Gradient Descent(192/499): loss=0.36920966584258536, w0=-0.3146114642565584, w1=0.08461625018029907\n",
      "Gradient Descent(193/499): loss=0.36871628650371113, w0=-0.3146140910437929, w1=0.08413427652849513\n",
      "Gradient Descent(194/499): loss=0.3682296917300727, w0=-0.31461658649166546, w1=0.08365891736455137\n",
      "Gradient Descent(195/499): loss=0.36774976761630035, w0=-0.31461895716714416, w1=0.08319007451769138\n",
      "Gradient Descent(196/499): loss=0.3672764025285856, w0=-0.31462120930884874, w1=0.08272765131049786\n",
      "Gradient Descent(197/499): loss=0.36680948705083544, w0=-0.3146233488434679, w1=0.08227155253653456\n",
      "Gradient Descent(198/499): loss=0.36634891393235486, w0=-0.3146253814013558, w1=0.08182168443825179\n",
      "Gradient Descent(199/499): loss=0.36589457803700537, w0=-0.31462731233134916, w1=0.08137795468517556\n",
      "Gradient Descent(200/499): loss=0.3654463762937892, w0=-0.31462914671484266, w1=0.08094027235238035\n",
      "Gradient Descent(201/499): loss=0.36500420764881253, w0=-0.3146308893791612, w1=0.08050854789924465\n",
      "Gradient Descent(202/499): loss=0.36456797301858046, w0=-0.31463254491026366, w1=0.08008269314848877\n",
      "Gradient Descent(203/499): loss=0.36413757524458, w0=-0.3146341176648108, w1=0.07966262126549399\n",
      "Gradient Descent(204/499): loss=0.3637129190491086, w0=-0.31463561178163035, w1=0.07924824673790182\n",
      "Gradient Descent(205/499): loss=0.36329391099230735, w0=-0.3146370311926087, w1=0.07883948535549208\n",
      "Gradient Descent(206/499): loss=0.36288045943036007, w0=-0.314638379633038, w1=0.0784362541903383\n",
      "Gradient Descent(207/499): loss=0.3624724744748204, w0=-0.31463966065144555, w1=0.0780384715772388\n",
      "Gradient Descent(208/499): loss=0.3620698679530297, w0=-0.31464087761893256, w1=0.07764605709442163\n",
      "Gradient Descent(209/499): loss=0.3616725533695933, w0=-0.31464203373804506, w1=0.07725893154452146\n",
      "Gradient Descent(210/499): loss=0.3612804458688794, w0=-0.3146431320512017, w1=0.0768770169358264\n",
      "Gradient Descent(211/499): loss=0.36089346219850804, w0=-0.3146441754487003, w1=0.0765002364637925\n",
      "Gradient Descent(212/499): loss=0.3605115206738026, w0=-0.3146451666763238, w1=0.07612851449282382\n",
      "Gradient Descent(213/499): loss=0.3601345411431687, w0=-0.31464610834256596, w1=0.07576177653831553\n",
      "Gradient Descent(214/499): loss=0.3597624449543756, w0=-0.3146470029254958, w1=0.0753999492489579\n",
      "Gradient Descent(215/499): loss=0.3593951549217135, w0=-0.3146478527792789, w1=0.07504296038929835\n",
      "Gradient Descent(216/499): loss=0.3590325952939927, w0=-0.31464866014037274, w1=0.07469073882255922\n",
      "Gradient Descent(217/499): loss=0.35867469172336813, w0=-0.31464942713341165, w1=0.0743432144937086\n",
      "Gradient Descent(218/499): loss=0.35832137123495844, w0=-0.31465015577679845, w1=0.07400031841278153\n",
      "Gradient Descent(219/499): loss=0.35797256219723783, w0=-0.3146508479880157, w1=0.0736619826384488\n",
      "Gradient Descent(220/499): loss=0.35762819429317616, w0=-0.3146515055886719, w1=0.0733281402618307\n",
      "Gradient Descent(221/499): loss=0.35728819849210697, w0=-0.3146521303092951, w1=0.07299872539055285\n",
      "Gradient Descent(222/499): loss=0.3569525070222998, w0=-0.314652723793887, w1=0.0726736731330414\n",
      "Gradient Descent(223/499): loss=0.35662105334421784, w0=-0.3146532876042491, w1=0.07235291958305455\n",
      "Gradient Descent(224/499): loss=0.35629377212444063, w0=-0.3146538232240929, w1=0.07203640180444781\n",
      "Gradient Descent(225/499): loss=0.355970599210229, w0=-0.31465433206294435, w1=0.07172405781616992\n",
      "Gradient Descent(226/499): loss=0.3556514716047209, w0=-0.314654815459853, w1=0.07141582657748662\n",
      "Gradient Descent(227/499): loss=0.35533632744273097, w0=-0.3146552746869161, w1=0.07111164797342942\n",
      "Gradient Descent(228/499): loss=0.3550251059671445, w0=-0.31465571095262584, w1=0.07081146280046641\n",
      "Gradient Descent(229/499): loss=0.3547177475058829, w0=-0.31465612540504995, w1=0.07051521275239221\n",
      "Gradient Descent(230/499): loss=0.3544141934494287, w0=-0.31465651913485265, w1=0.0702228404064343\n",
      "Gradient Descent(231/499): loss=0.3541143862288912, w0=-0.31465689317816503, w1=0.06993428920957262\n",
      "Gradient Descent(232/499): loss=0.3538182692945992, w0=-0.3146572485193116, w1=0.06964950346506982\n",
      "Gradient Descent(233/499): loss=0.3535257870952057, w0=-0.31465758609340067, w1=0.06936842831920903\n",
      "Gradient Descent(234/499): loss=0.3532368850572896, w0=-0.31465790678878514, w1=0.06909100974823652\n",
      "Gradient Descent(235/499): loss=0.3529515095654424, w0=-0.3146582114494002, w1=0.06881719454550632\n",
      "Gradient Descent(236/499): loss=0.3526696079428242, w0=-0.3146585008769844, w1=0.06854693030882383\n",
      "Gradient Descent(237/499): loss=0.352391128432177, w0=-0.31465877583318913, w1=0.0682801654279859\n",
      "Gradient Descent(238/499): loss=0.3521160201772846, w0=-0.3146590370415835, w1=0.06801684907251435\n",
      "Gradient Descent(239/499): loss=0.3518442332048639, w0=-0.314659285189558, w1=0.06775693117958023\n",
      "Gradient Descent(240/499): loss=0.3515757184068771, w0=-0.3146595209301336, w1=0.0675003624421161\n",
      "Gradient Descent(241/499): loss=0.3513104275232552, w0=-0.31465974488368026, w1=0.0672470942971136\n",
      "Gradient Descent(242/499): loss=0.3510483131250185, w0=-0.3146599576395494, w1=0.06699707891410353\n",
      "Gradient Descent(243/499): loss=0.35078932859778683, w0=-0.3146601597576249, w1=0.06675026918381587\n",
      "Gradient Descent(244/499): loss=0.3505334281256652, w0=-0.3146603517697965, w1=0.06650661870701696\n",
      "Gradient Descent(245/499): loss=0.3502805666754996, w0=-0.31466053418135936, w1=0.06626608178352139\n",
      "Gradient Descent(246/499): loss=0.35003069998149, w0=-0.3146607074723439, w1=0.06602861340137577\n",
      "Gradient Descent(247/499): loss=0.3497837845301502, w0=-0.3146608720987791, w1=0.06579416922621197\n",
      "Gradient Descent(248/499): loss=0.34953977754560983, w0=-0.3146610284938923, w1=0.06556270559076727\n",
      "Gradient Descent(249/499): loss=0.3492986369752447, w0=-0.31466117706924973, w1=0.06533417948456882\n",
      "Gradient Descent(250/499): loss=0.3490603214756311, w0=-0.31466131821583915, w1=0.06510854854377997\n",
      "Gradient Descent(251/499): loss=0.34882479039881126, w0=-0.3146614523050989, w1=0.06488577104120599\n",
      "Gradient Descent(252/499): loss=0.34859200377886707, w0=-0.3146615796898955, w1=0.06466580587645683\n",
      "Gradient Descent(253/499): loss=0.3483619223187897, w0=-0.31466170070545213, w1=0.06444861256626436\n",
      "Gradient Descent(254/499): loss=0.34813450737764, w0=-0.31466181567023077, w1=0.06423415123495183\n",
      "Gradient Descent(255/499): loss=0.34790972095799144, w0=-0.3146619248867703, w1=0.0640223826050532\n",
      "Gradient Descent(256/499): loss=0.3476875256936493, w0=-0.3146620286424827, w1=0.06381326798808003\n",
      "Gradient Descent(257/499): loss=0.34746788483763663, w0=-0.31466212721040937, w1=0.0636067692754335\n",
      "Gradient Descent(258/499): loss=0.34725076225044504, w0=-0.31466222084993956, w1=0.06340284892945959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(259/499): loss=0.34703612238853765, w0=-0.31466230980749305, w1=0.06320146997464493\n",
      "Gradient Descent(260/499): loss=0.34682393029310304, w0=-0.31466239431716875, w1=0.06300259598895122\n",
      "Gradient Descent(261/499): loss=0.346614151579051, w0=-0.3146624746013605, w1=0.06280619109528615\n",
      "Gradient Descent(262/499): loss=0.34640675242424585, w0=-0.3146625508713425, w1=0.06261221995310852\n",
      "Gradient Descent(263/499): loss=0.3462016995589688, w0=-0.31466262332782524, w1=0.06242064775016554\n",
      "Gradient Descent(264/499): loss=0.345998960255608, w0=-0.31466269216148374, w1=0.062231440194360305\n",
      "Gradient Descent(265/499): loss=0.3457985023185662, w0=-0.3146627575534591, w1=0.062044563505747226\n",
      "Gradient Descent(266/499): loss=0.345600294074384, w0=-0.31466281967583565, w1=0.061859984408653555\n",
      "Gradient Descent(267/499): loss=0.3454043043620712, w0=-0.3146628786920932, w1=0.06167767012392496\n",
      "Gradient Descent(268/499): loss=0.34521050252364344, w0=-0.3146629347575377, w1=0.06149758836129315\n",
      "Gradient Descent(269/499): loss=0.3450188583948568, w0=-0.3146629880197098, w1=0.0613197073118637\n",
      "Gradient Descent(270/499): loss=0.3448293422961367, w0=-0.3146630386187732, w1=0.06114399564072212\n",
      "Gradient Descent(271/499): loss=0.344641925023698, w0=-0.3146630866878833, w1=0.0609704224796563\n",
      "Gradient Descent(272/499): loss=0.3444565778408479, w0=-0.3146631323535377, w1=0.060798957419993485\n",
      "Gradient Descent(273/499): loss=0.344273272469472, w0=-0.3146631757359093, w1=0.06062957050555002\n",
      "Gradient Descent(274/499): loss=0.3440919810816946, w0=-0.31466321694916216, w1=0.06046223222569194\n",
      "Gradient Descent(275/499): loss=0.3439126762917134, w0=-0.3146632561017522, w1=0.060296913508504815\n",
      "Gradient Descent(276/499): loss=0.3437353311477999, w0=-0.31466329329671267, w1=0.060133585714070924\n",
      "Gradient Descent(277/499): loss=0.3435599191244669, w0=-0.3146633286319249, w1=0.05997222062785224\n",
      "Gradient Descent(278/499): loss=0.34338641411479365, w0=-0.31466336220037644, w1=0.05981279045417738\n",
      "Gradient Descent(279/499): loss=0.34321479042291037, w0=-0.3146633940904052, w1=0.059655267809830964\n",
      "Gradient Descent(280/499): loss=0.34304502275663346, w0=-0.3146634243859325, w1=0.05949962571774369\n",
      "Gradient Descent(281/499): loss=0.3428770862202516, w0=-0.3146634531666832, w1=0.05934583760078157\n",
      "Gradient Descent(282/499): loss=0.34271095630745774, w0=-0.31466348050839626, w1=0.05919387727563267\n",
      "Gradient Descent(283/499): loss=0.3425466088944226, w0=-0.31466350648302355, w1=0.0590437189467899\n",
      "Gradient Descent(284/499): loss=0.3423840202330102, w0=-0.3146635311589193, w1=0.0588953372006282\n",
      "Gradient Descent(285/499): loss=0.34222316694412636, w0=-0.3146635546010202, w1=0.05874870699957473\n",
      "Gradient Descent(286/499): loss=0.34206402601120267, w0=-0.31466357687101587, w1=0.0586038036763705\n",
      "Gradient Descent(287/499): loss=0.3419065747738096, w0=-0.31466359802751165, w1=0.058460602928421944\n",
      "Gradient Descent(288/499): loss=0.3417507909213976, w0=-0.3146636181261825, w1=0.05831908081224118\n",
      "Gradient Descent(289/499): loss=0.3415966524871611, w0=-0.31466363721991963, w1=0.058179213737973294\n",
      "Gradient Descent(290/499): loss=0.34144413784202465, w0=-0.3146636553589698, w1=0.05804097846400943\n",
      "Gradient Descent(291/499): loss=0.3412932256887488, w0=-0.3146636725910673, w1=0.05790435209168424\n",
      "Gradient Descent(292/499): loss=0.34114389505615106, w0=-0.31466368896155983, w1=0.057769312060056394\n",
      "Gradient Descent(293/499): loss=0.34099612529343964, w0=-0.3146637045135276, w1=0.05763583614077072\n",
      "Gradient Descent(294/499): loss=0.3408498960646601, w0=-0.3146637192878969, w1=0.057503902433000804\n",
      "Gradient Descent(295/499): loss=0.34070518734324967, w0=-0.3146637333235476, w1=0.057373489358470676\n",
      "Gradient Descent(296/499): loss=0.34056197940669675, w0=-0.3146637466574156, w1=0.05724457565655431\n",
      "Gradient Descent(297/499): loss=0.3404202528313055, w0=-0.31466375932459006, w1=0.05711714037945174\n",
      "Gradient Descent(298/499): loss=0.34027998848706204, w0=-0.3146637713584057, w1=0.05699116288744054\n",
      "Gradient Descent(299/499): loss=0.3401411675325987, w0=-0.3146637827905304, w1=0.05686662284420147\n",
      "Gradient Descent(300/499): loss=0.3400037714102567, w0=-0.3146637936510488, w1=0.05674350021221708\n",
      "Gradient Descent(301/499): loss=0.3398677818412434, w0=-0.3146638039685411, w1=0.05662177524824214\n",
      "Gradient Descent(302/499): loss=0.3397331808208813, w0=-0.3146638137701587, w1=0.05650142849884474\n",
      "Gradient Descent(303/499): loss=0.33959995061395115, w0=-0.31466382308169527, w1=0.05638244079601689\n",
      "Gradient Descent(304/499): loss=0.33946807375012067, w0=-0.3146638319276549, w1=0.056264793252853544\n",
      "Gradient Descent(305/499): loss=0.339337533019462, w0=-0.31466384033131645, w1=0.05614846725929899\n",
      "Gradient Descent(306/499): loss=0.3392083114680538, w0=-0.31466384831479477, w1=0.05603344447795943\n",
      "Gradient Descent(307/499): loss=0.3390803923936682, w0=-0.31466385589909907, w1=0.05591970683998078\n",
      "Gradient Descent(308/499): loss=0.33895375934153693, w0=-0.31466386310418804, w1=0.05580723654099064\n",
      "Gradient Descent(309/499): loss=0.33882839610019916, w0=-0.3146638699490224, w1=0.05569601603710334\n",
      "Gradient Descent(310/499): loss=0.3387042866974271, w0=-0.3146638764516149, w1=0.05558602804098715\n",
      "Gradient Descent(311/499): loss=0.33858141539622755, w0=-0.31466388262907774, w1=0.05547725551799256\n",
      "Gradient Descent(312/499): loss=0.3384597666909194, w0=-0.3146638884976673, w1=0.0553696816823408\n",
      "Gradient Descent(313/499): loss=0.3383393253032833, w0=-0.31466389407282724, w1=0.05526328999337143\n",
      "Gradient Descent(314/499): loss=0.33822007617878425, w0=-0.31466389936922906, w1=0.05515806415184828\n",
      "Gradient Descent(315/499): loss=0.33810200448286365, w0=-0.31466390440081066, w1=0.05505398809632264\n",
      "Gradient Descent(316/499): loss=0.3379850955973004, w0=-0.31466390918081305, w1=0.054951045999552924\n",
      "Gradient Descent(317/499): loss=0.33786933511663986, w0=-0.31466391372181524, w1=0.054849222264979736\n",
      "Gradient Descent(318/499): loss=0.337754708844688, w0=-0.3146639180357672, w1=0.05474850152325566\n",
      "Gradient Descent(319/499): loss=0.33764120279107124, w0=-0.31466392213402145, w1=0.05464886862882875\n",
      "Gradient Descent(320/499): loss=0.3375288031678581, w0=-0.3146639260273629, w1=0.05455030865657891\n",
      "Gradient Descent(321/499): loss=0.3374174963862448, w0=-0.3146639297260371, w1=0.054452806898506345\n",
      "Gradient Descent(322/499): loss=0.3373072690532995, w0=-0.3146639332397775, w1=0.054356348860471256\n",
      "Gradient Descent(323/499): loss=0.3371981079687677, w0=-0.31466393657783076, w1=0.054260920258983894\n",
      "Gradient Descent(324/499): loss=0.337090000121935, w0=-0.31466393974898127, w1=0.05416650701804427\n",
      "Gradient Descent(325/499): loss=0.33698293268854757, w0=-0.3146639427615741, w1=0.05407309526603066\n",
      "Gradient Descent(326/499): loss=0.33687689302778695, w0=-0.3146639456235372, w1=0.053980671332636176\n",
      "Gradient Descent(327/499): loss=0.3367718686793028, w0=-0.314663948342402, w1=0.05388922174585263\n",
      "Gradient Descent(328/499): loss=0.3366678473602956, w0=-0.3146639509253235, w1=0.05379873322900088\n",
      "Gradient Descent(329/499): loss=0.33656481696265417, w0=-0.3146639533790988, w1=0.053709192697807016\n",
      "Gradient Descent(330/499): loss=0.33646276555014515, w0=-0.31466395571018524, w1=0.053620587257523604\n",
      "Gradient Descent(331/499): loss=0.33636168135565075, w0=-0.31466395792471724, w1=0.05353290420009528\n",
      "Gradient Descent(332/499): loss=0.3362615527784583, w0=-0.3146639600285225, w1=0.053446131001368004\n",
      "Gradient Descent(333/499): loss=0.33616236838159697, w0=-0.31466396202713737, w1=0.053360255318341265\n",
      "Gradient Descent(334/499): loss=0.3360641168892223, w0=-0.3146639639258214, w1=0.053275264986462546\n",
      "Gradient Descent(335/499): loss=0.33596678718404754, w0=-0.3146639657295711, w1=0.05319114801696346\n",
      "Gradient Descent(336/499): loss=0.3358703683048204, w0=-0.31466396744313324, w1=0.05310789259423676\n",
      "Gradient Descent(337/499): loss=0.3357748494438451, w0=-0.3146639690710172, w1=0.05302548707325371\n",
      "Gradient Descent(338/499): loss=0.33568021994454667, w0=-0.3146639706175068, w1=0.052943919977021116\n",
      "Gradient Descent(339/499): loss=0.3355864692990811, w0=-0.3146639720866718, w1=0.052863179994077365\n",
      "Gradient Descent(340/499): loss=0.335493587145985, w0=-0.31466397348237846, w1=0.05278325597602692\n",
      "Gradient Descent(341/499): loss=0.3354015632678675, w0=-0.31466397480829966, w1=0.05270413693511263\n",
      "Gradient Descent(342/499): loss=0.3353103875891433, w0=-0.31466397606792473, w1=0.05262581204182524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(343/499): loss=0.3352200501738048, w0=-0.31466397726456846, w1=0.05254827062254955\n",
      "Gradient Descent(344/499): loss=0.33513054122323316, w0=-0.3146639784013799, w1=0.05247150215724663\n",
      "Gradient Descent(345/499): loss=0.33504185107404794, w0=-0.3146639794813506, w1=0.05239549627717152\n",
      "Gradient Descent(346/499): loss=0.334953970195994, w0=-0.3146639805073227, w1=0.05232024276262587\n",
      "Gradient Descent(347/499): loss=0.334866889189865, w0=-0.3146639814819961, w1=0.052245731540744904\n",
      "Gradient Descent(348/499): loss=0.33478059878546335, w0=-0.3146639824079357, w1=0.05217195268331831\n",
      "Gradient Descent(349/499): loss=0.33469508983959506, w0=-0.3146639832875782, w1=0.05209889640464438\n",
      "Gradient Descent(350/499): loss=0.3346103533340997, w0=-0.31466398412323854, w1=0.05202655305941691\n",
      "Gradient Descent(351/499): loss=0.33452638037391375, w0=-0.3146639849171157, w1=0.05195491314064444\n",
      "Gradient Descent(352/499): loss=0.33444316218516773, w0=-0.31466398567129894, w1=0.05188396727760116\n",
      "Gradient Descent(353/499): loss=0.33436069011331604, w0=-0.3146639863877729, w1=0.051813706233809174\n",
      "Gradient Descent(354/499): loss=0.33427895562129917, w0=-0.314663987068423, w1=0.05174412090505146\n",
      "Gradient Descent(355/499): loss=0.33419795028773613, w0=-0.31466398771504056, w1=0.05167520231741513\n",
      "Gradient Descent(356/499): loss=0.3341176658051498, w0=-0.3146639883293271, w1=0.051606941625364534\n",
      "Gradient Descent(357/499): loss=0.33403809397822, w0=-0.31466398891289926, w1=0.05153933010984367\n",
      "Gradient Descent(358/499): loss=0.33395922672206907, w0=-0.3146639894672927, w1=0.0514723591764075\n",
      "Gradient Descent(359/499): loss=0.3338810560605746, w0=-0.31466398999396633, w1=0.05140602035338171\n",
      "Gradient Descent(360/499): loss=0.3338035741247116, w0=-0.3146639904943062, w1=0.0513403052900504\n",
      "Gradient Descent(361/499): loss=0.33372677315092225, w0=-0.314663990969629, w1=0.05127520575487141\n",
      "Gradient Descent(362/499): loss=0.3336506454795139, w0=-0.3146639914211855, w1=0.05121071363371868\n",
      "Gradient Descent(363/499): loss=0.3335751835530835, w0=-0.31466399185016414, w1=0.051146820928151336\n",
      "Gradient Descent(364/499): loss=0.33350037991496917, w0=-0.3146639922576937, w1=0.05108351975370908\n",
      "Gradient Descent(365/499): loss=0.3334262272077272, w0=-0.31466399264484674, w1=0.051020802338233424\n",
      "Gradient Descent(366/499): loss=0.3333527181716343, w0=-0.314663993012642, w1=0.050958661020214346\n",
      "Gradient Descent(367/499): loss=0.33327984564321694, w0=-0.31466399336204737, w1=0.05089708824716208\n",
      "Gradient Descent(368/499): loss=0.33320760255380205, w0=-0.3146639936939824, w1=0.05083607657400355\n",
      "Gradient Descent(369/499): loss=0.33313598192809474, w0=-0.3146639940093206, w1=0.050775618661503055\n",
      "Gradient Descent(370/499): loss=0.3330649768827789, w0=-0.31466399430889175, w1=0.05071570727470694\n",
      "Gradient Descent(371/499): loss=0.33299458062514, w0=-0.31466399459348426, w1=0.05065633528141176\n",
      "Gradient Descent(372/499): loss=0.3329247864517123, w0=-0.31466399486384705, w1=0.05059749565065557\n",
      "Gradient Descent(373/499): loss=0.33285558774694707, w0=-0.3146639951206916, w1=0.05053918145123214\n",
      "Gradient Descent(374/499): loss=0.3327869779819049, w0=-0.3146639953646938, w1=0.05048138585022751\n",
      "Gradient Descent(375/499): loss=0.3327189507129665, w0=-0.3146639955964958, w1=0.05042410211157873\n",
      "Gradient Descent(376/499): loss=0.33265149958056817, w0=-0.3146639958167077, w1=0.050367323594654324\n",
      "Gradient Descent(377/499): loss=0.3325846183079553, w0=-0.31466399602590883, w1=0.05031104375285618\n",
      "Gradient Descent(378/499): loss=0.33251830069995814, w0=-0.3146639962246498, w1=0.050255256132242535\n",
      "Gradient Descent(379/499): loss=0.3324525406417871, w0=-0.31466399641345366, w1=0.050199954370171745\n",
      "Gradient Descent(380/499): loss=0.332387332097847, w0=-0.31466399659281724, w1=0.05014513219396646\n",
      "Gradient Descent(381/499): loss=0.3323226691105722, w0=-0.31466399676321255, w1=0.05009078341959797\n",
      "Gradient Descent(382/499): loss=0.3322585457992797, w0=-0.31466399692508795, w1=0.05003690195039029\n",
      "Gradient Descent(383/499): loss=0.3321949563590411, w0=-0.3146639970788695, w1=0.04998348177574385\n",
      "Gradient Descent(384/499): loss=0.33213189505957286, w0=-0.3146639972249619, w1=0.04993051696987829\n",
      "Gradient Descent(385/499): loss=0.3320693562441454, w0=-0.3146639973637496, w1=0.049878001690594226\n",
      "Gradient Descent(386/499): loss=0.3320073343285081, w0=-0.3146639974955978, w1=0.049825930178053625\n",
      "Gradient Descent(387/499): loss=0.33194582379983434, w0=-0.3146639976208535, w1=0.049774296753578445\n",
      "Gradient Descent(388/499): loss=0.33188481921568036, w0=-0.3146639977398463, w1=0.04972309581846741\n",
      "Gradient Descent(389/499): loss=0.3318243152029641, w0=-0.3146639978528894, w1=0.04967232185283046\n",
      "Gradient Descent(390/499): loss=0.3317643064569579, w0=-0.3146639979602803, w1=0.04962196941444076\n",
      "Gradient Descent(391/499): loss=0.3317047877402995, w0=-0.3146639980623015, w1=0.049572033137603874\n",
      "Gradient Descent(392/499): loss=0.3316457538820171, w0=-0.3146639981592216, w1=0.04952250773204392\n",
      "Gradient Descent(393/499): loss=0.33158719977657153, w0=-0.31466399825129554, w1=0.04947338798180643\n",
      "Gradient Descent(394/499): loss=0.3315291203829127, w0=-0.31466399833876574, w1=0.04942466874417758\n",
      "Gradient Descent(395/499): loss=0.33147151072355147, w0=-0.3146639984218623, w1=0.049376344948619685\n",
      "Gradient Descent(396/499): loss=0.331414365883647, w0=-0.314663998500804, w1=0.04932841159572258\n",
      "Gradient Descent(397/499): loss=0.3313576810101072, w0=-0.31466399857579846, w1=0.04928086375617066\n",
      "Gradient Descent(398/499): loss=0.33130145131070515, w0=-0.31466399864704314, w1=0.04923369656972544\n",
      "Gradient Descent(399/499): loss=0.33124567205320776, w0=-0.3146639987147255, w1=0.04918690524422323\n",
      "Gradient Descent(400/499): loss=0.33119033856452035, w0=-0.3146639987790236, w1=0.04914048505458785\n",
      "Gradient Descent(401/499): loss=0.3311354462298424, w0=-0.31466399884010676, w1=0.049094431341858084\n",
      "Gradient Descent(402/499): loss=0.33108099049183887, w0=-0.3146639988981357, w1=0.04904873951222961\n",
      "Gradient Descent(403/499): loss=0.331026966849823, w0=-0.314663998953263, w1=0.049003405036111225\n",
      "Gradient Descent(404/499): loss=0.33097337085895284, w0=-0.3146639990056339, w1=0.04895842344719522\n",
      "Gradient Descent(405/499): loss=0.33092019812943974, w0=-0.31466399905538617, w1=0.048913790341541495\n",
      "Gradient Descent(406/499): loss=0.33086744432577014, w0=-0.31466399910265075, w1=0.048869501376675416\n",
      "Gradient Descent(407/499): loss=0.3308151051659389, w0=-0.314663999147552, w1=0.048825552270699055\n",
      "Gradient Descent(408/499): loss=0.33076317642069525, w0=-0.3146639991902081, w1=0.048781938801415674\n",
      "Gradient Descent(409/499): loss=0.3307116539127987, w0=-0.3146639992307313, w1=0.04873865680546724\n",
      "Gradient Descent(410/499): loss=0.33066053351629004, w0=-0.3146639992692283, w1=0.048695702177484756\n",
      "Gradient Descent(411/499): loss=0.33060981115576976, w0=-0.3146639993058003, w1=0.04865307086925124\n",
      "Gradient Descent(412/499): loss=0.33055948280569114, w0=-0.3146639993405437, w1=0.04861075888887712\n",
      "Gradient Descent(413/499): loss=0.33050954448966136, w0=-0.3146639993735498, w1=0.0485687622999879\n",
      "Gradient Descent(414/499): loss=0.33045999227975553, w0=-0.31466399940490547, w1=0.04852707722092385\n",
      "Gradient Descent(415/499): loss=0.3304108222958407, w0=-0.3146639994346933, w1=0.04848569982395162\n",
      "Gradient Descent(416/499): loss=0.33036203070491066, w0=-0.3146639994629917, w1=0.04844462633448752\n",
      "Gradient Descent(417/499): loss=0.3303136137204292, w0=-0.314663999489875, w1=0.048403853030332296\n",
      "Gradient Descent(418/499): loss=0.3302655676016868, w0=-0.31466399951541413, w1=0.04836337624091729\n",
      "Gradient Descent(419/499): loss=0.3302178886531642, w0=-0.31466399953967616, w1=0.04832319234656175\n",
      "Gradient Descent(420/499): loss=0.3301705732239069, w0=-0.31466399956272506, w1=0.04828329777774112\n",
      "Gradient Descent(421/499): loss=0.3301236177069096, w0=-0.31466399958462143, w1=0.04824368901436616\n",
      "Gradient Descent(422/499): loss=0.3300770185385104, w0=-0.31466399960542285, w1=0.048204362585072776\n",
      "Gradient Descent(423/499): loss=0.3300307721977922, w0=-0.31466399962518415, w1=0.04816531506652229\n",
      "Gradient Descent(424/499): loss=0.3299848752059956, w0=-0.3146639996439573, w1=0.04812654308271208\n",
      "Gradient Descent(425/499): loss=0.32993932412593996, w0=-0.3146639996617917, w1=0.048088043304296425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(426/499): loss=0.3298941155614528, w0=-0.3146639996787343, w1=0.04804981244791733\n",
      "Gradient Descent(427/499): loss=0.3298492461568081, w0=-0.3146639996948297, w1=0.04801184727554529\n",
      "Gradient Descent(428/499): loss=0.3298047125961737, w0=-0.31466399971012027, w1=0.047974144593829704\n",
      "Gradient Descent(429/499): loss=0.3297605116030666, w0=-0.3146639997246462, w1=0.04793670125345895\n",
      "Gradient Descent(430/499): loss=0.3297166399398162, w0=-0.3146639997384458, w1=0.04789951414852983\n",
      "Gradient Descent(431/499): loss=0.3296730944070363, w0=-0.31466399975155523, w1=0.047862580215926355\n",
      "Gradient Descent(432/499): loss=0.32962987184310516, w0=-0.31466399976400916, w1=0.047825896434707624\n",
      "Gradient Descent(433/499): loss=0.32958696912365265, w0=-0.3146639997758403, w1=0.047789459825504775\n",
      "Gradient Descent(434/499): loss=0.32954438316105494, w0=-0.3146639997870798, w1=0.04775326744992677\n",
      "Gradient Descent(435/499): loss=0.329502110903939, w0=-0.3146639997977573, w1=0.04771731640997493\n",
      "Gradient Descent(436/499): loss=0.3294601493366917, w0=-0.3146639998079008, w1=0.04768160384746608\n",
      "Gradient Descent(437/499): loss=0.3294184954789772, w0=-0.314663999817537, w1=0.04764612694346416\n",
      "Gradient Descent(438/499): loss=0.32937714638526344, w0=-0.31466399982669135, w1=0.047610882917720206\n",
      "Gradient Descent(439/499): loss=0.3293360991443527, w0=-0.3146639998353879, w1=0.047575869028120554\n",
      "Gradient Descent(440/499): loss=0.32929535087892053, w0=-0.31466399984364957, w1=0.04754108257014311\n",
      "Gradient Descent(441/499): loss=0.3292548987450634, w0=-0.31466399985149807, w1=0.04750652087632163\n",
      "Gradient Descent(442/499): loss=0.32921473993184935, w0=-0.31466399985895405, w1=0.04747218131571783\n",
      "Gradient Descent(443/499): loss=0.32917487166087817, w0=-0.31466399986603716, w1=0.04743806129340126\n",
      "Gradient Descent(444/499): loss=0.3291352911858475, w0=-0.314663999872766, w1=0.04740415824993678\n",
      "Gradient Descent(445/499): loss=0.3290959957921246, w0=-0.31466399987915833, w1=0.04737046966087955\n",
      "Gradient Descent(446/499): loss=0.3290569827963252, w0=-0.31466399988523097, w1=0.0473369930362774\n",
      "Gradient Descent(447/499): loss=0.3290182495458983, w0=-0.3146639998909999, w1=0.04730372592018049\n",
      "Gradient Descent(448/499): loss=0.3289797934187172, w0=-0.3146639998964803, w1=0.04727066589015816\n",
      "Gradient Descent(449/499): loss=0.32894161182267667, w0=-0.31466399990168664, w1=0.04723781055682278\n",
      "Gradient Descent(450/499): loss=0.3289037021952959, w0=-0.3146639999066326, w1=0.04720515756336065\n",
      "Gradient Descent(451/499): loss=0.3288660620033266, w0=-0.31466399991133115, w1=0.047172704585069655\n",
      "Gradient Descent(452/499): loss=0.3288286887423684, w0=-0.3146639999157947, w1=0.047140449328903766\n",
      "Gradient Descent(453/499): loss=0.32879157993648905, w0=-0.31466399992003496, w1=0.0471083895330241\n",
      "Gradient Descent(454/499): loss=0.3287547331378491, w0=-0.31466399992406313, w1=0.04707652296635658\n",
      "Gradient Descent(455/499): loss=0.32871814592633475, w0=-0.31466399992788985, w1=0.04704484742815604\n",
      "Gradient Descent(456/499): loss=0.32868181590919365, w0=-0.31466399993152516, w1=0.04701336074757666\n",
      "Gradient Descent(457/499): loss=0.328645740720676, w0=-0.3146639999349786, w1=0.04698206078324868\n",
      "Gradient Descent(458/499): loss=0.328609918021683, w0=-0.31466399993825933, w1=0.04695094542286128\n",
      "Gradient Descent(459/499): loss=0.328574345499419, w0=-0.31466399994137595, w1=0.046920012582751504\n",
      "Gradient Descent(460/499): loss=0.3285390208670478, w0=-0.31466399994433664, w1=0.04688926020749919\n",
      "Gradient Descent(461/499): loss=0.3285039418633557, w0=-0.3146639999471492, w1=0.046858686269527765\n",
      "Gradient Descent(462/499): loss=0.3284691062524189, w0=-0.3146639999498211, w1=0.046828288768710896\n",
      "Gradient Descent(463/499): loss=0.3284345118232753, w0=-0.3146639999523593, w1=0.0467980657319848\n",
      "Gradient Descent(464/499): loss=0.3284001563896008, w0=-0.3146639999547705, w1=0.04676801521296618\n",
      "Gradient Descent(465/499): loss=0.32836603778939105, w0=-0.31466399995706107, w1=0.046738135291575784\n",
      "Gradient Descent(466/499): loss=0.32833215388464776, w0=-0.31466399995923705, w1=0.04670842407366732\n",
      "Gradient Descent(467/499): loss=0.32829850256106874, w0=-0.3146639999613042, w1=0.046678879690661854\n",
      "Gradient Descent(468/499): loss=0.328265081727743, w0=-0.3146639999632678, w1=0.046649500299187414\n",
      "Gradient Descent(469/499): loss=0.32823188931685043, w0=-0.3146639999651332, w1=0.0466202840807239\n",
      "Gradient Descent(470/499): loss=0.328198923283365, w0=-0.3146639999669053, w1=0.046591229241253085\n",
      "Gradient Descent(471/499): loss=0.3281661816047627, w0=-0.3146639999685887, w1=0.046562334010913704\n",
      "Gradient Descent(472/499): loss=0.328133662280734, w0=-0.31466399997018785, w1=0.04653359664366154\n",
      "Gradient Descent(473/499): loss=0.32810136333289974, w0=-0.31466399997170696, w1=0.04650501541693438\n",
      "Gradient Descent(474/499): loss=0.3280692828045315, w0=-0.31466399997315003, w1=0.04647658863132191\n",
      "Gradient Descent(475/499): loss=0.3280374187602753, w0=-0.3146639999745209, w1=0.04644831461024028\n",
      "Gradient Descent(476/499): loss=0.3280057692858811, w0=-0.3146639999758231, w1=0.04642019169961143\n",
      "Gradient Descent(477/499): loss=0.32797433248793284, w0=-0.3146639999770602, w1=0.04639221826754703\n",
      "Gradient Descent(478/499): loss=0.3279431064935861, w0=-0.3146639999782353, w1=0.04636439270403699\n",
      "Gradient Descent(479/499): loss=0.32791208945030637, w0=-0.3146639999793516, w1=0.04633671342064246\n",
      "Gradient Descent(480/499): loss=0.3278812795256127, w0=-0.31466399998041206, w1=0.046309178850193235\n",
      "Gradient Descent(481/499): loss=0.3278506749068252, w0=-0.31466399998141936, w1=0.04628178744648957\n",
      "Gradient Descent(482/499): loss=0.327820273800814, w0=-0.31466399998237626, w1=0.046254537684008265\n",
      "Gradient Descent(483/499): loss=0.3277900744337546, w0=-0.3146639999832852, w1=0.04622742805761295\n",
      "Gradient Descent(484/499): loss=0.3277600750508847, w0=-0.3146639999841486, w1=0.04620045708226862\n",
      "Gradient Descent(485/499): loss=0.3277302739162651, w0=-0.3146639999849688, w1=0.04617362329276021\n",
      "Gradient Descent(486/499): loss=0.3277006693125435, w0=-0.3146639999857479, w1=0.04614692524341523\n",
      "Gradient Descent(487/499): loss=0.3276712595407225, w0=-0.314663999986488, w1=0.04612036150783043\n",
      "Gradient Descent(488/499): loss=0.32764204291993015, w0=-0.31466399998719097, w1=0.04609393067860233\n",
      "Gradient Descent(489/499): loss=0.32761301778719376, w0=-0.31466399998785877, w1=0.04606763136706169\n",
      "Gradient Descent(490/499): loss=0.32758418249721727, w0=-0.3146639999884931, w1=0.046041462203011745\n",
      "Gradient Descent(491/499): loss=0.3275555354221611, w0=-0.31466399998909567, w1=0.04601542183447018\n",
      "Gradient Descent(492/499): loss=0.32752707495142575, w0=-0.31466399998966804, w1=0.04598950892741489\n",
      "Gradient Descent(493/499): loss=0.32749879949143773, w0=-0.3146639999902117, w1=0.04596372216553331\n",
      "Gradient Descent(494/499): loss=0.3274707074654393, w0=-0.31466399999072814, w1=0.045938060249975335\n",
      "Gradient Descent(495/499): loss=0.32744279731328, w0=-0.31466399999121863, w1=0.04591252189910987\n",
      "Gradient Descent(496/499): loss=0.32741506749121246, w0=-0.31466399999168454, w1=0.045887105848284745\n",
      "Gradient Descent(497/499): loss=0.32738751647168907, w0=-0.3146639999921271, w1=0.045861810849590165\n",
      "Gradient Descent(498/499): loss=0.32736014274316444, w0=-0.3146639999925474, w1=0.0458366356716255\n",
      "Gradient Descent(499/499): loss=0.3273329448098971, w0=-0.3146639999929467, w1=0.045811579099269424\n"
     ]
    }
   ],
   "source": [
    "w = np.array(np.random.rand((tX.shape[1])))\n",
    "max_iters = 500 #choosing number of iterations\n",
    "gamma = 5 *10**(-2)\n",
    "(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)\n",
    "#gamma = 5/max_iters #choosing gamma \n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
