{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from extend import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree = 4\n",
    "#tX = build_poly(tX, degree)\n",
    "#tX = tX[:,1:]\n",
    "tX = extend(tX,[np.cos,np.sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 90)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = (tX - np.mean(tX,axis = 0))/(np.std(tX,axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[(np.ones(tX.shape[0]) , tX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 91)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=23.20516395433644, w0=0.4407671623145117, w1=0.6142744317686821\n",
      "Gradient Descent(1/499): loss=11.178485032819548, w0=0.40299560419856695, w1=0.5924515070081597\n",
      "Gradient Descent(2/499): loss=9.478465961197276, w0=0.36711262398843497, w1=0.5745191061451873\n",
      "Gradient Descent(3/499): loss=8.165390393046923, w0=0.3330237927888273, w1=0.5581435587725778\n",
      "Gradient Descent(4/499): loss=7.121884754728213, w0=0.3006394031492154, w1=0.5430187505956631\n",
      "Gradient Descent(5/499): loss=6.2791146413506915, w0=0.2698742329915973, w1=0.5289680276694754\n",
      "Gradient Descent(6/499): loss=5.588069924089063, w0=0.24064732134187133, w1=0.515850595481097\n",
      "Gradient Descent(7/499): loss=5.013432110114143, w0=0.21288175527464154, w1=0.503550837762418\n",
      "Gradient Descent(8/499): loss=4.52944452313334, w0=0.18650446751078154, w1=0.49197284512519024\n",
      "Gradient Descent(9/499): loss=4.117091505124261, w0=0.1614460441351218, w1=0.48103632041468414\n",
      "Gradient Descent(10/499): loss=3.762156232881459, w0=0.13764054192825134, w1=0.47067342262112427\n",
      "Gradient Descent(11/499): loss=3.4538724687841604, w0=0.11502531483172992, w1=0.46082632501109994\n",
      "Gradient Descent(12/499): loss=3.183980735281908, w0=0.09354084909003929, w1=0.4514453202194193\n",
      "Gradient Descent(13/499): loss=2.9460619304335363, w0=0.0731306066354373, w1=0.4424873442374621\n",
      "Gradient Descent(14/499): loss=2.735062721941997, w0=0.05374087630356898, w1=0.4339148212185997\n",
      "Gradient Descent(15/499): loss=2.54695452912942, w0=0.03532063248829718, w1=0.42569475408226587\n",
      "Gradient Descent(16/499): loss=2.3784862833075757, w0=0.01782140086379162, w1=0.4177980036143765\n",
      "Gradient Descent(17/499): loss=2.227003536078002, w0=0.0011971308205137102, w1=0.41019871235023597\n",
      "Gradient Descent(18/499): loss=2.0903148780602367, w0=-0.014595925720598289, w1=0.4028738399309748\n",
      "Gradient Descent(19/499): loss=1.9665923597810524, w0=-0.029599329434652877, w1=0.3958027845777511\n",
      "Gradient Descent(20/499): loss=1.85429654428423, w0=-0.0438525629630031, w1=0.3889670713960036\n",
      "Gradient Descent(21/499): loss=1.752119546230412, w0=-0.05739313481493444, w1=0.3823500928430901\n",
      "Gradient Descent(22/499): loss=1.6589413112094706, w0=-0.07025667807426801, w1=0.3759368902049849\n",
      "Gradient Descent(23/499): loss=1.5737957211986264, w0=-0.08247704417063387, w1=0.36971396759221525\n",
      "Gradient Descent(24/499): loss=1.4958440529926404, w0=-0.09408639196218044, w1=0.3636691319827122\n",
      "Gradient Descent(25/499): loss=1.4243539853536133, w0=-0.10511527236414885, w1=0.3577913543640862\n",
      "Gradient Descent(26/499): loss=1.3586828292961077, w0=-0.11559270874601808, w1=0.3520706481784135\n",
      "Gradient Descent(27/499): loss=1.2982640006540709, w0=-0.12554627330879323, w1=0.3464979621395903\n",
      "Gradient Descent(28/499): loss=1.242596003925389, w0=-0.13500215964342901, w1=0.34106508514593176\n",
      "Gradient Descent(29/499): loss=1.1912333786142226, w0=-0.14398525166133253, w1=0.3357645615017491\n",
      "Gradient Descent(30/499): loss=1.143779193028497, w0=-0.1525191890783404, w1=0.33058961503126194\n",
      "Gradient Descent(31/499): loss=1.0998787692582874, w0=-0.16062642962449747, w1=0.32553408094682373\n",
      "Gradient Descent(32/499): loss=1.059214396464521, w0=-0.16832830814334634, w1=0.3205923445440566\n",
      "Gradient Descent(33/499): loss=1.0215008445084852, w0=-0.17564509273625242, w1=0.31575928595644276\n",
      "Gradient Descent(34/499): loss=0.9864815312808707, w0=-0.1825960380995129, w1=0.31103023032421906\n",
      "Gradient Descent(35/499): loss=0.9539252284051896, w0=-0.18919943619461005, w1=0.3064009028268075\n",
      "Gradient Descent(36/499): loss=0.923623213879599, w0=-0.19547266438495206, w1=0.3018673881017393\n",
      "Gradient Descent(37/499): loss=0.8953867985701233, w0=-0.20143223116577674, w1=0.2974260936314815\n",
      "Gradient Descent(38/499): loss=0.8690451676608835, w0=-0.20709381960755996, w1=0.29307371672672033\n",
      "Gradient Descent(39/499): loss=0.8444434892233195, w0=-0.2124723286272538, w1=0.2888072147734205\n",
      "Gradient Descent(40/499): loss=0.8214412507424107, w0=-0.21758191219596273, w1=0.28462377844349124\n",
      "Gradient Descent(41/499): loss=0.7999107912962296, w0=-0.22243601658623605, w1=0.28052080759670683\n",
      "Gradient Descent(42/499): loss=0.7797360025470595, w0=-0.2270474157569955, w1=0.2764958896257635\n",
      "Gradient Descent(43/499): loss=0.7608111760844984, w0=-0.23142824496921682, w1=0.27254678001782395\n",
      "Gradient Descent(44/499): loss=0.7430399782030099, w0=-0.23559003272082688, w1=0.26867138492516923\n",
      "Gradient Descent(45/499): loss=0.7263345360803816, w0=-0.23954373108485624, w1=0.26486774555507075\n",
      "Gradient Descent(46/499): loss=0.7106146216886459, w0=-0.24329974453068398, w1=0.26113402420499726\n",
      "Gradient Descent(47/499): loss=0.6958069217221714, w0=-0.24686795730422018, w1=0.25746849178399883\n",
      "Gradient Descent(48/499): loss=0.6818443834515457, w0=-0.2502577594390794, w1=0.2538695166747121\n",
      "Gradient Descent(49/499): loss=0.668665627770771, w0=-0.2534780714671955, w1=0.25033555480302194\n",
      "Gradient Descent(50/499): loss=0.6562144218495006, w0=-0.2565373678939056, w1=0.24686514079407895\n",
      "Gradient Descent(51/499): loss=0.6444392047710702, w0=-0.25944369949928003, w1=0.2434568801041755\n",
      "Gradient Descent(52/499): loss=0.6332926603622863, w0=-0.2622047145243856, w1=0.24010944202798373\n",
      "Gradient Descent(53/499): loss=0.622731332127239, w0=-0.2648276787982357, w1=0.23682155348990122\n",
      "Gradient Descent(54/499): loss=0.6127152758048429, w0=-0.2673194948583931, w1=0.2335919935367794\n",
      "Gradient Descent(55/499): loss=0.6032077455945077, w0=-0.2696867201155425, w1=0.2304195884571664\n",
      "Gradient Descent(56/499): loss=0.5941749105494049, w0=-0.2719355841098342, w1=0.22730320745941723\n",
      "Gradient Descent(57/499): loss=0.5855855980329425, w0=-0.27407200490441114, w1=0.22424175884764844\n",
      "Gradient Descent(58/499): loss=0.5774110614800899, w0=-0.27610160465925904, w1=0.22123418664057845\n",
      "Gradient Descent(59/499): loss=0.569624770008379, w0=-0.27802972442636437, w1=0.21827946758383374\n",
      "Gradient Descent(60/499): loss=0.5622022176898072, w0=-0.2798614382051142, w1=0.21537660851134952\n",
      "Gradient Descent(61/499): loss=0.5551207505295672, w0=-0.2816015662949264, w1=0.21252464401608817\n",
      "Gradient Descent(62/499): loss=0.5483594094047363, w0=-0.2832546879802478, w1=0.20972263439446776\n",
      "Gradient Descent(63/499): loss=0.5418987873994524, w0=-0.2848251535813029, w1=0.20696966383267332\n",
      "Gradient Descent(64/499): loss=0.5357209001356656, w0=-0.28631709590230503, w1=0.2042648388064421\n",
      "Gradient Descent(65/499): loss=0.5298090678429614, w0=-0.2877344411072569, w1=0.20160728666900077\n",
      "Gradient Descent(66/499): loss=0.524147808039399, w0=-0.2890809190519609, w1=0.19899615440461474\n",
      "Gradient Descent(67/499): loss=0.5187227378097508, w0=-0.29036007309942957, w1=0.1964306075277155\n",
      "Gradient Descent(68/499): loss=0.5135204847696182, w0=-0.29157526944452455, w1=0.1939098291098197\n",
      "Gradient Descent(69/499): loss=0.5085286058950796, w0=-0.2927297059723646, w1=0.1914330189184745\n",
      "Gradient Descent(70/499): loss=0.5037355134790674, w0=-0.29382642067381237, w1=0.18899939265427035\n",
      "Gradient Descent(71/499): loss=0.49913040754865423, w0=-0.29486829964018757, w1=0.18660818127358073\n",
      "Gradient Descent(72/499): loss=0.49470321414281726, w0=-0.2958580846582438, w1=0.18425863038613335\n",
      "Gradient Descent(73/499): loss=0.49044452890889645, w0=-0.296798380425397, w1=0.18194999971780595\n",
      "Gradient Descent(74/499): loss=0.48634556552858793, w0=-0.29769166140419234, w1=0.17968156263018784\n",
      "Gradient Descent(75/499): loss=0.4823981085315931, w0=-0.29854027833404767, w1=0.17745260568947094\n",
      "Gradient Descent(76/499): loss=0.4785944700975256, w0=-0.29934646441741, w1=0.1752624282781409\n",
      "Gradient Descent(77/499): loss=0.4749274504849058, w0=-0.30011234119660396, w1=0.17311034224374494\n",
      "Gradient Descent(78/499): loss=0.471390301760465, w0=-0.300839924136838, w1=0.17099567157972695\n",
      "Gradient Descent(79/499): loss=0.46797669453295165, w0=-0.30153112793006015, w1=0.16891775213395258\n",
      "Gradient Descent(80/499): loss=0.46468068742355617, w0=-0.3021877715336209, w1=0.1668759313411057\n",
      "Gradient Descent(81/499): loss=0.46149669903022317, w0=-0.30281158295700344, w1=0.16486956797563104\n",
      "Gradient Descent(82/499): loss=0.45841948216582434, w0=-0.30340420380921657, w1=0.16289803192233282\n",
      "Gradient Descent(83/499): loss=0.4554441001706469, w0=-0.3039671936188188, w1=0.16096070396212264\n",
      "Gradient Descent(84/499): loss=0.45256590511815104, w0=-0.3045020339379407, w1=0.15905697557074652\n",
      "Gradient Descent(85/499): loss=0.4497805177496556, w0=-0.30501013224110624, w1=0.15718624872861717\n",
      "Gradient Descent(86/499): loss=0.4470838089887093, w0=-0.3054928256291133, w1=0.1553479357401371\n",
      "Gradient Descent(87/499): loss=0.44447188289956274, w0=-0.30595138434771973, w1=0.1535414590611258\n",
      "Gradient Descent(88/499): loss=0.441941060966493, w0=-0.3063870151303956, w1=0.15176625113316222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(89/499): loss=0.4394878675819216, w0=-0.3068008643739375, w1=0.15002175422382868\n",
      "Gradient Descent(90/499): loss=0.4371090166413679, w0=-0.30719402115530203, w1=0.14830742027199184\n",
      "Gradient Descent(91/499): loss=0.43480139915245014, w0=-0.3075675200975981, w1=0.14662271073738967\n",
      "Gradient Descent(92/499): loss=0.4325620717734425, w0=-0.3079223440927791, w1=0.14496709645390637\n",
      "Gradient Descent(93/499): loss=0.43038824620441835, w0=-0.3082594268882008, w1=0.14334005748601675\n",
      "Gradient Descent(94/499): loss=0.4282772793608335, w0=-0.3085796555438512, w1=0.14174108298796695\n",
      "Gradient Descent(95/499): loss=0.4262266642655878, w0=-0.3088838727667188, w1=0.14016967106533249\n",
      "Gradient Descent(96/499): loss=0.4242340216012177, w0=-0.3091728791284428, w1=0.13862532863865756\n",
      "Gradient Descent(97/499): loss=0.4222970918689713, w0=-0.30944743517208034, w1=0.1371075713089345\n",
      "Gradient Descent(98/499): loss=0.42041372810614314, w0=-0.30970826341353574, w1=0.13561592322472862\n",
      "Gradient Descent(99/499): loss=0.41858188911725047, w0=-0.30995605024291817, w1=0.13414991695079315\n",
      "Gradient Descent(100/499): loss=0.4167996331784597, w0=-0.31019144773083124, w1=0.13270909333805292\n",
      "Gradient Descent(101/499): loss=0.41506511217813424, w0=-0.31041507534434837, w1=0.1312930013948637\n",
      "Gradient Descent(102/499): loss=0.41337656615955226, w0=-0.3106275215771894, w1=0.12990119815947776\n",
      "Gradient Descent(103/499): loss=0.4117323182347006, w0=-0.3108293454983882, w1=0.1285332485736665\n",
      "Gradient Descent(104/499): loss=0.4101307698406831, w0=-0.31102107822352676, w1=0.12718872535746717\n",
      "Gradient Descent(105/499): loss=0.4085703963126568, w0=-0.31120322431240816, w1=0.12586720888503447\n",
      "Gradient Descent(106/499): loss=0.4070497427493841, w0=-0.31137626309684524, w1=0.1245682870615887\n",
      "Gradient Descent(107/499): loss=0.40556742014947034, w0=-0.31154064994206027, w1=0.1232915552014611\n",
      "Gradient Descent(108/499): loss=0.4041221017981531, w0=-0.31169681744501426, w1=0.12203661590724384\n",
      "Gradient Descent(109/499): loss=0.4027125198861707, w0=-0.31184517657282035, w1=0.12080307895005787\n",
      "Gradient Descent(110/499): loss=0.40133746234372036, w0=-0.31198611774423585, w1=0.11959056115095537\n",
      "Gradient Descent(111/499): loss=0.3999957698739058, w0=-0.3121200118570804, w1=0.11839868626347691\n",
      "Gradient Descent(112/499): loss=0.39868633317131474, w0=-0.31224721126428245, w1=0.1172270848573852\n",
      "Gradient Descent(113/499): loss=0.3974080903125174, w0=-0.31236805070112417, w1=0.11607539420359844\n",
      "Gradient Descent(114/499): loss=0.39616002430632374, w0=-0.31248284816612354, w1=0.11494325816034681\n",
      "Gradient Descent(115/499): loss=0.3949411607925883, w0=-0.3125919057578727, w1=0.1138303270605755\n",
      "Gradient Descent(116/499): loss=0.39375056587923724, w0=-0.3126955104700342, w1=0.11273625760061738\n",
      "Gradient Descent(117/499): loss=0.3925873441079828, w0=-0.31279393494658736, w1=0.11166071273015715\n",
      "Gradient Descent(118/499): loss=0.3914506365399363, w0=-0.31288743819931264, w1=0.11060336154350806\n",
      "Gradient Descent(119/499): loss=0.3903396189529916, w0=-0.31297626628940145, w1=0.10956387917222057\n",
      "Gradient Descent(120/499): loss=0.3892535001434798, w0=-0.3130606529749856, w1=0.10854194667904106\n",
      "Gradient Descent(121/499): loss=0.38819152032515225, w0=-0.31314082032629026, w1=0.1075372509532367\n",
      "Gradient Descent(122/499): loss=0.38715294961907337, w0=-0.31321697931002945, w1=0.10654948460730113\n",
      "Gradient Descent(123/499): loss=0.3861370866284813, w0=-0.3132893303445815, w1=0.10557834587505359\n",
      "Gradient Descent(124/499): loss=0.38514325709310954, w0=-0.3133580638274057, w1=0.10462353851114249\n",
      "Gradient Descent(125/499): loss=0.38417081261786984, w0=-0.3134233606360885, w1=0.10368477169196225\n",
      "Gradient Descent(126/499): loss=0.3832191294711621, w0=-0.3134853926043369, w1=0.10276175991799086\n",
      "Gradient Descent(127/499): loss=0.38228760744842066, w0=-0.3135443229741726, w1=0.10185422291755358\n",
      "Gradient Descent(128/499): loss=0.3813756687968208, w0=-0.3136003068255164, w1=0.10096188555201638\n",
      "Gradient Descent(129/499): loss=0.3804827571973538, w0=-0.3136534914842927, w1=0.10008447772241126\n",
      "Gradient Descent(130/499): loss=0.3796083368007509, w0=-0.31370401691013, w1=0.09922173427749396\n",
      "Gradient Descent(131/499): loss=0.3787518913139778, w0=-0.31375201606467523, w1=0.09837339492323285\n",
      "Gradient Descent(132/499): loss=0.3779129231342485, w0=-0.31379761526149297, w1=0.0975392041337264\n",
      "Gradient Descent(133/499): loss=0.3770909525277184, w0=-0.3138409344984696, w1=0.09671891106354544\n",
      "Gradient Descent(134/499): loss=0.37628551685020906, w0=-0.3138820877735972, w1=0.09591226946149477\n",
      "Gradient Descent(135/499): loss=0.3754961698074917, w0=-0.3139211833849682, w1=0.09511903758578769\n",
      "Gradient Descent(136/499): loss=0.3747224807528291, w0=-0.3139583242157704, w1=0.09433897812062571\n",
      "Gradient Descent(137/499): loss=0.37396403401962114, w0=-0.3139936080050323, w1=0.09357185809417488\n",
      "Gradient Descent(138/499): loss=0.37322042828714536, w0=-0.31402712760483087, w1=0.0928174487979287\n",
      "Gradient Descent(139/499): loss=0.37249127597751086, w0=-0.3140589712246393, w1=0.0920755257074472\n",
      "Gradient Descent(140/499): loss=0.37177620268207134, w0=-0.3140892226634571, w1=0.09134586840446053\n",
      "Gradient Descent(141/499): loss=0.37107484661564855, w0=-0.31411796153033383, w1=0.09062826050032471\n",
      "Gradient Descent(142/499): loss=0.37038685809702726, w0=-0.3141452634538665, w1=0.08992248956081667\n",
      "Gradient Descent(143/499): loss=0.3697118990542773, w0=-0.31417120028122236, w1=0.08922834703225457\n",
      "Gradient Descent(144/499): loss=0.36904964255355094, w0=-0.3141958402672102, w1=0.08854562816892958\n",
      "Gradient Descent(145/499): loss=0.368399772350083, w0=-0.31421924825389846, w1=0.08787413196183377\n",
      "Gradient Descent(146/499): loss=0.36776198246020486, w0=-0.3142414858412521, w1=0.08721366106866929\n",
      "Gradient Descent(147/499): loss=0.36713597675325266, w0=-0.31426261154923785, w1=0.08656402174512282\n",
      "Gradient Descent(148/499): loss=0.3665214685623163, w0=-0.3142826809718241, w1=0.08592502377738931\n",
      "Gradient Descent(149/499): loss=0.36591818031284434, w0=-0.31430174692328083, w1=0.08529648041592862\n",
      "Gradient Descent(150/499): loss=0.36532584316817224, w0=-0.3143198595771645, w1=0.0846782083104383\n",
      "Gradient Descent(151/499): loss=0.3647441966910998, w0=-0.31433706659835386, w1=0.08407002744602562\n",
      "Gradient Descent(152/499): loss=0.36417298852069585, w0=-0.31435341326848354, w1=0.08347176108056169\n",
      "Gradient Descent(153/499): loss=0.3636119740635514, w0=-0.3143689426051065, w1=0.08288323568320041\n",
      "Gradient Descent(154/499): loss=0.36306091619875125, w0=-0.31438369547489814, w1=0.0823042808740449\n",
      "Gradient Descent(155/499): loss=0.36251958499587655, w0=-0.3143977107012, w1=0.08173472936494368\n",
      "Gradient Descent(156/499): loss=0.3619877574453842, w0=-0.3144110251661866, w1=0.08117441690139926\n",
      "Gradient Descent(157/499): loss=0.36146521720075153, w0=-0.31442367390792364, w1=0.08062318220557145\n",
      "Gradient Descent(158/499): loss=0.36095175433180643, w0=-0.31443569021257367, w1=0.08008086692035771\n",
      "Gradient Descent(159/499): loss=0.36044716508869507, w0=-0.31444710570199097, w1=0.07954731555453304\n",
      "Gradient Descent(160/499): loss=0.35995125167596864, w0=-0.31445795041693725, w1=0.0790223754289318\n",
      "Gradient Descent(161/499): loss=0.35946382203630356, w0=-0.314468252896136, w1=0.0785058966236539\n",
      "Gradient Descent(162/499): loss=0.35898468964338703, w0=-0.31447804025137466, w1=0.07799773192627803\n",
      "Gradient Descent(163/499): loss=0.35851367330353506, w0=-0.31448733823885117, w1=0.07749773678106457\n",
      "Gradient Descent(164/499): loss=0.35805059696562586, w0=-0.3144961713269537, w1=0.07700576923913088\n",
      "Gradient Descent(165/499): loss=0.3575952895389565, w0=-0.3145045627606509, w1=0.07652168990958197\n",
      "Gradient Descent(166/499): loss=0.3571475847186517, w0=-0.3145125346226631, w1=0.07604536191157944\n",
      "Gradient Descent(167/499): loss=0.35670732081827244, w0=-0.3145201078915745, w1=0.07557665082733221\n",
      "Gradient Descent(168/499): loss=0.35627434060928875, w0=-0.31452730249704014, w1=0.07511542465599191\n",
      "Gradient Descent(169/499): loss=0.355848491167103, w0=-0.3145341373722323, w1=0.07466155376843685\n",
      "Gradient Descent(170/499): loss=0.35542962372331915, w0=-0.3145406305036647, w1=0.07421491086292806\n",
      "Gradient Descent(171/499): loss=0.35501759352397727, w0=-0.31454679897852533, w1=0.07377537092162142\n",
      "Gradient Descent(172/499): loss=0.3546122596934788, w0=-0.31455265902964274, w1=0.07334281116791985\n",
      "Gradient Descent(173/499): loss=0.35421348510394707, w0=-0.3145582260782041, w1=0.07291711102464982\n",
      "Gradient Descent(174/499): loss=0.3538211362497792, w0=-0.3145635147743372, w1=0.07249815207304681\n",
      "Gradient Descent(175/499): loss=0.3534350831271568, w0=-0.31456853903566345, w1=0.0720858180125344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(176/499): loss=0.35305519911829447, w0=-0.31457331208392325, w1=0.07167999462128163\n",
      "Gradient Descent(177/499): loss=0.35268136088021657, w0=-0.3145778464797699, w1=0.07128056971752421\n",
      "Gradient Descent(178/499): loss=0.35231344823786437, w0=-0.314582154155824, w1=0.07088743312163448\n",
      "Gradient Descent(179/499): loss=0.3519513440813402, w0=-0.3145862464480753, w1=0.07050047661892606\n",
      "Gradient Descent(180/499): loss=0.35159493426711297, w0=-0.31459013412571385, w1=0.07011959392317871\n",
      "Gradient Descent(181/499): loss=0.35124410752300705, w0=-0.3145938274194703, w1=0.06974468064086962\n",
      "Gradient Descent(182/499): loss=0.3508987553568174, w0=-0.31459733604853873, w1=0.0693756342360972\n",
      "Gradient Descent(183/499): loss=0.3505587719683899, w0=-0.3146006692461536, w1=0.069012353996184\n",
      "Gradient Descent(184/499): loss=0.3502240541650208, w0=-0.31460383578388756, w1=0.06865474099794536\n",
      "Gradient Descent(185/499): loss=0.3498945012800332, w0=-0.31460684399473465, w1=0.06830269807461078\n",
      "Gradient Descent(186/499): loss=0.3495700150943968, w0=-0.3146097017950392, w1=0.0679561297833851\n",
      "Gradient Descent(187/499): loss=0.3492504997612597, w0=-0.3146124167053284, w1=0.06761494237363687\n",
      "Gradient Descent(188/499): loss=0.348935861733273, w0=-0.314614995870103, w1=0.0672790437557016\n",
      "Gradient Descent(189/499): loss=0.34862600969258856, w0=-0.3146174460766387, w1=0.06694834347028757\n",
      "Gradient Descent(190/499): loss=0.34832085448342015, w0=-0.3146197737728475, w1=0.06662275265847238\n",
      "Gradient Descent(191/499): loss=0.3480203090470605, w0=-0.31462198508424566, w1=0.06630218403227842\n",
      "Gradient Descent(192/499): loss=0.3477242883592533, w0=-0.3146240858300738, w1=0.06598655184581571\n",
      "Gradient Descent(193/499): loss=0.3474327093698234, w0=-0.3146260815386103, w1=0.06567577186698093\n",
      "Gradient Descent(194/499): loss=0.34714549094447134, w0=-0.3146279774617199, w1=0.06536976134970138\n",
      "Gradient Descent(195/499): loss=0.3468625538086454, w0=-0.3146297785886738, w1=0.06506843900671314\n",
      "Gradient Descent(196/499): loss=0.3465838204934064, w0=-0.3146314896592799, w1=0.06477172498286267\n",
      "Gradient Descent(197/499): loss=0.3463092152832033, w0=-0.3146331151763555, w1=0.06447954082892135\n",
      "Gradient Descent(198/499): loss=0.3460386641654845, w0=-0.31463465941757723, w1=0.06419180947590282\n",
      "Gradient Descent(199/499): loss=0.34577209478206955, w0=-0.3146361264467377, w1=0.06390845520987289\n",
      "Gradient Descent(200/499): loss=0.34550943638221177, w0=-0.31463752012444, w1=0.06362940364724227\n",
      "Gradient Descent(201/499): loss=0.3452506197772845, w0=-0.31463884411825704, w1=0.06335458171053245\n",
      "Gradient Descent(202/499): loss=0.34499557729702574, w0=-0.31464010191238306, w1=0.06308391760460505\n",
      "Gradient Descent(203/499): loss=0.3447442427472807, w0=-0.31464129681680264, w1=0.06281734079334571\n",
      "Gradient Descent(204/499): loss=0.34449655136918284, w0=-0.3146424319760011, w1=0.06255478197679298\n",
      "Gradient Descent(205/499): loss=0.3442524397997167, w0=-0.31464351037723953, w1=0.062296173068703685\n",
      "Gradient Descent(206/499): loss=0.3440118460336103, w0=-0.31464453485841587, w1=0.06204144717454567\n",
      "Gradient Descent(207/499): loss=0.3437747093865024, w0=-0.31464550811553327, w1=0.061790538569909625\n",
      "Gradient Descent(208/499): loss=0.3435409704593384, w0=-0.31464643270979464, w1=0.061543382679331425\n",
      "Gradient Descent(209/499): loss=0.34331057110394664, w0=-0.3146473110743428, w1=0.06129991605551683\n",
      "Gradient Descent(210/499): loss=0.34308345438974824, w0=-0.3146481455206634, w1=0.061060076358960484\n",
      "Gradient Descent(211/499): loss=0.34285956457155975, w0=-0.31464893824466783, w1=0.06082380233795128\n",
      "Gradient Descent(212/499): loss=0.34263884705844416, w0=-0.3146496913324719, w1=0.060591033808956404\n",
      "Gradient Descent(213/499): loss=0.3424212483835742, w0=-0.3146504067658856, w1=0.060361711637376456\n",
      "Gradient Descent(214/499): loss=0.34220671617506576, w0=-0.31465108642762857, w1=0.060135777718664234\n",
      "Gradient Descent(215/499): loss=0.3419951991277481, w0=-0.3146517321062842, w1=0.05991317495979994\n",
      "Gradient Descent(216/499): loss=0.341786646975834, w0=-0.31465234550100696, w1=0.059693847261115644\n",
      "Gradient Descent(217/499): loss=0.34158101046645717, w0=-0.3146529282259934, w1=0.05947773949846209\n",
      "Gradient Descent(218/499): loss=0.34137824133404426, w0=-0.31465348181473035, w1=0.05926479750571101\n",
      "Gradient Descent(219/499): loss=0.34117829227549146, w0=-0.31465400772403035, w1=0.05905496805758618\n",
      "Gradient Descent(220/499): loss=0.3409811169261151, w0=-0.3146545073378652, w1=0.05884819885281685\n",
      "Gradient Descent(221/499): loss=0.3407866698363482, w0=-0.3146549819710082, w1=0.05864443849760697\n",
      "Gradient Descent(222/499): loss=0.34059490644915597, w0=-0.3146554328724939, w1=0.058443636489413996\n",
      "Gradient Descent(223/499): loss=0.34040578307814473, w0=-0.3146558612289052, w1=0.05824574320103118\n",
      "Gradient Descent(224/499): loss=0.3402192568863375, w0=-0.3146562681674958, w1=0.05805070986496726\n",
      "Gradient Descent(225/499): loss=0.34003528586559406, w0=-0.3146566547591568, w1=0.05785848855811771\n",
      "Gradient Descent(226/499): loss=0.3398538288166511, w0=-0.3146570220212345, w1=0.0576690321867217\n",
      "Gradient Descent(227/499): loss=0.3396748453297617, w0=-0.3146573709202083, w1=0.057482294471599256\n",
      "Gradient Descent(228/499): loss=0.339498295765911, w0=-0.31465770237423324, w1=0.05729822993366294\n",
      "Gradient Descent(229/499): loss=0.3393241412385886, w0=-0.3146580172555568, w1=0.05711679387969876\n",
      "Gradient Descent(230/499): loss=0.33915234359610014, w0=-0.31465831639281405, w1=0.056937942388410906\n",
      "Gradient Descent(231/499): loss=0.3389828654043938, w0=-0.3146586005732083, w1=0.056761632296725206\n",
      "Gradient Descent(232/499): loss=0.33881566993039086, w0=-0.31465887054458275, w1=0.05658782118634613\n",
      "Gradient Descent(233/499): loss=0.33865072112579614, w0=-0.31465912701738835, w1=0.05641646737056245\n",
      "Gradient Descent(234/499): loss=0.3384879836113765, w0=-0.31465937066655353, w1=0.056247529881296644\n",
      "Gradient Descent(235/499): loss=0.3383274226616878, w0=-0.31465960213326033, w1=0.05608096845639328\n",
      "Gradient Descent(236/499): loss=0.3381690041902391, w0=-0.3146598220266317, w1=0.05591674352714162\n",
      "Gradient Descent(237/499): loss=0.33801269473507384, w0=-0.31466003092533434, w1=0.055754816206028075\n",
      "Gradient Descent(238/499): loss=0.33785846144475873, w0=-0.31466022937910176, w1=0.0555951482747138\n",
      "Gradient Descent(239/499): loss=0.3377062720647641, w0=-0.3146604179101807, w1=0.055437702172233164\n",
      "Gradient Descent(240/499): loss=0.3375560949242217, w0=-0.3146605970147055, w1=0.055282440983408815\n",
      "Gradient Descent(241/499): loss=0.3374078989230512, w0=-0.314660767164004, w1=0.05512932842747906\n",
      "Gradient Descent(242/499): loss=0.33726165351943793, w0=-0.31466092880583746, w1=0.054978328846933454\n",
      "Gradient Descent(243/499): loss=0.3371173287176542, w0=-0.3146610823655791, w1=0.05482940719655264\n",
      "Gradient Descent(244/499): loss=0.336974895056211, w0=-0.31466122824733356, w1=0.054682529032648364\n",
      "Gradient Descent(245/499): loss=0.33683432359633, w0=-0.3146613668350002, w1=0.05453766050249991\n",
      "Gradient Descent(246/499): loss=0.3366955859107249, w0=-0.31466149849328334, w1=0.054394768333983094\n",
      "Gradient Descent(247/499): loss=0.33655865407268215, w0=-0.31466162356865224, w1=0.05425381982538811\n",
      "Gradient Descent(248/499): loss=0.33642350064543114, w0=-0.3146617423902526, w1=0.05411478283542263\n",
      "Gradient Descent(249/499): loss=0.33629009867179616, w0=-0.3146618552707728, w1=0.05397762577339658\n",
      "Gradient Descent(250/499): loss=0.3361584216641178, w0=-0.31466196250726686, w1=0.05384231758958508\n",
      "Gradient Descent(251/499): loss=0.33602844359443884, w0=-0.3146620643819361, w1=0.053708827765766166\n",
      "Gradient Descent(252/499): loss=0.3359001388849438, w0=-0.3146621611628718, w1=0.0535771263059299\n",
      "Gradient Descent(253/499): loss=0.335773482398645, w0=-0.3146622531047606, w1=0.053447183727155685\n",
      "Gradient Descent(254/499): loss=0.33564844943030764, w0=-0.3146623404495548, w1=0.05331897105065445\n",
      "Gradient Descent(255/499): loss=0.3355250156976053, w0=-0.31466242342710926, w1=0.05319245979297268\n",
      "Gradient Descent(256/499): loss=0.33540315733250065, w0=-0.31466250225578585, w1=0.05306762195735508\n",
      "Gradient Descent(257/499): loss=0.33528285087284226, w0=-0.3146625771430285, w1=0.052944430025263015\n",
      "Gradient Descent(258/499): loss=0.3351640732541721, w0=-0.3146626482859089, w1=0.05282285694804562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(259/499): loss=0.3350468018017372, w0=-0.3146627158716452, w1=0.05270287613876078\n",
      "Gradient Descent(260/499): loss=0.33493101422269944, w0=-0.3146627800780945, w1=0.0525844614641431\n",
      "Gradient Descent(261/499): loss=0.33481668859853575, w0=-0.31466284107422127, w1=0.05246758723671605\n",
      "Gradient Descent(262/499): loss=0.33470380337762606, w0=-0.3146628990205416, w1=0.05235222820704572\n",
      "Gradient Descent(263/499): loss=0.3345923373680204, w0=-0.3146629540695458, w1=0.05223835955613326\n",
      "Gradient Descent(264/499): loss=0.3344822697303815, w0=-0.3146630063660997, w1=0.05212595688794364\n",
      "Gradient Descent(265/499): loss=0.33437357997109673, w0=-0.31466305604782574, w1=0.052014996222068004\n",
      "Gradient Descent(266/499): loss=0.33426624793555637, w0=-0.3146631032454654, w1=0.05190545398651716\n",
      "Gradient Descent(267/499): loss=0.3341602538015897, w0=-0.31466314808322304, w1=0.05179730701064379\n",
      "Gradient Descent(268/499): loss=0.33405557807305797, w0=-0.31466319067909265, w1=0.05169053251819091\n",
      "Gradient Descent(269/499): loss=0.3339522015735975, w0=-0.3146632311451687, w1=0.05158510812046419\n",
      "Gradient Descent(270/499): loss=0.3338501054405095, w0=-0.31466326958794083, w1=0.05148101180962593\n",
      "Gradient Descent(271/499): loss=0.3337492711187922, w0=-0.31466330610857424, w1=0.05137822195210828\n",
      "Gradient Descent(272/499): loss=0.33364968035531056, w0=-0.3146633408031759, w1=0.051276717282143676\n",
      "Gradient Descent(273/499): loss=0.3335513151931006, w0=-0.3146633737630473, w1=0.051176476895410036\n",
      "Gradient Descent(274/499): loss=0.3334541579658044, w0=-0.3146634050749251, w1=0.05107748024278885\n",
      "Gradient Descent(275/499): loss=0.33335819129223165, w0=-0.31466343482120884, w1=0.05097970712423394\n",
      "Gradient Descent(276/499): loss=0.3332633980710443, w0=-0.31466346308017834, w1=0.0508831376827488\n",
      "Gradient Descent(277/499): loss=0.33316976147556066, w0=-0.31466348992619925, w1=0.05078775239847058\n",
      "Gradient Descent(278/499): loss=0.3330772649486772, w0=-0.314663515429919, w1=0.05069353208285879\n",
      "Gradient Descent(279/499): loss=0.3329858921979021, w0=-0.3146635396584527, w1=0.05060045787298659\n",
      "Gradient Descent(280/499): loss=0.3328956271905002, w0=-0.31466356267555956, w1=0.050508511225933\n",
      "Gradient Descent(281/499): loss=0.33280645414874394, w0=-0.31466358454181104, w1=0.05041767391327405\n",
      "Gradient Descent(282/499): loss=0.3327183575452698, w0=-0.3146636053147498, w1=0.05032792801567103\n",
      "Gradient Descent(283/499): loss=0.33263132209853535, w0=-0.31466362504904155, w1=0.05023925591755415\n",
      "Gradient Descent(284/499): loss=0.33254533276837533, w0=-0.3146636437966186, w1=0.050151640301899766\n",
      "Gradient Descent(285/499): loss=0.3324603747516531, w0=-0.3146636616068167, w1=0.05006506414509953\n",
      "Gradient Descent(286/499): loss=0.3323764334780076, w0=-0.3146636785265048, w1=0.04997951071191967\n",
      "Gradient Descent(287/499): loss=0.33229349460568885, w0=-0.3146636946002084, w1=0.04989496355054892\n",
      "Gradient Descent(288/499): loss=0.33221154401748404, w0=-0.3146637098702267, w1=0.04981140648773332\n",
      "Gradient Descent(289/499): loss=0.332130567816729, w0=-0.314663724376744, w1=0.04972882362399638\n",
      "Gradient Descent(290/499): loss=0.3320505523234042, w0=-0.31466373815793536, w1=0.04964719932894303\n",
      "Gradient Descent(291/499): loss=0.3319714840703134, w0=-0.31466375125006707, w1=0.04956651823664585\n",
      "Gradient Descent(292/499): loss=0.3318933497993399, w0=-0.3146637636875921, w1=0.04948676524111207\n",
      "Gradient Descent(293/499): loss=0.3318161364577835, w0=-0.31466377550324076, w1=0.04940792549182983\n",
      "Gradient Descent(294/499): loss=0.33173983119477146, w0=-0.3146637867281069, w1=0.049329984389392366\n",
      "Gradient Descent(295/499): loss=0.33166442135774366, w0=-0.3146637973917296, w1=0.04925292758119859\n",
      "Gradient Descent(296/499): loss=0.33158989448901066, w0=-0.3146638075221711, w1=0.04917674095722877\n",
      "Gradient Descent(297/499): loss=0.3315162383223808, w0=-0.31466381714609043, w1=0.049101410645893885\n",
      "Gradient Descent(298/499): loss=0.33144344077985666, w0=-0.3146638262888137, w1=0.04902692300995739\n",
      "Gradient Descent(299/499): loss=0.3313714899683986, w0=-0.3146638349744007, w1=0.048953264642528006\n",
      "Gradient Descent(300/499): loss=0.33130037417675234, w0=-0.3146638432257083, w1=0.04888042236312235\n",
      "Gradient Descent(301/499): loss=0.3312300818723412, w0=-0.3146638510644504, w1=0.04880838321379604\n",
      "Gradient Descent(302/499): loss=0.3311606016982204, w0=-0.3146638585112553, w1=0.048737134455342127\n",
      "Gradient Descent(303/499): loss=0.33109192247009056, w0=-0.31466386558571985, w1=0.04866666356355559\n",
      "Gradient Descent(304/499): loss=0.3310240331733725, w0=-0.3146638723064611, w1=0.04859695822556275\n",
      "Gradient Descent(305/499): loss=0.33095692296033746, w0=-0.3146638786911652, w1=0.048528006336214416\n",
      "Gradient Descent(306/499): loss=0.33089058114729547, w0=-0.314663884756634, w1=0.04845979599454158\n",
      "Gradient Descent(307/499): loss=0.33082499721183684, w0=-0.31466389051882926, w1=0.04839231550027265\n",
      "Gradient Descent(308/499): loss=0.3307601607901297, w0=-0.31466389599291467, w1=0.048325553350410985\n",
      "Gradient Descent(309/499): loss=0.3306960616742673, w0=-0.31466390119329574, w1=0.04825949823587177\n",
      "Gradient Descent(310/499): loss=0.33063268980966853, w0=-0.31466390613365763, w1=0.04819413903817709\n",
      "Gradient Descent(311/499): loss=0.3305700352925275, w0=-0.31466391082700135, w1=0.04812946482620821\n",
      "Gradient Descent(312/499): loss=0.3305080883673122, w0=-0.31466391528567783, w1=0.04806546485301404\n",
      "Gradient Descent(313/499): loss=0.3304468394243107, w0=-0.31466391952142037, w1=0.04800212855267473\n",
      "Gradient Descent(314/499): loss=0.3303862789972244, w0=-0.3146639235453757, w1=0.047939445537219516\n",
      "Gradient Descent(315/499): loss=0.3303263977608061, w0=-0.3146639273681332, w1=0.04787740559359773\n",
      "Gradient Descent(316/499): loss=0.3302671865285435, w0=-0.3146639309997527, w1=0.04781599868070214\n",
      "Gradient Descent(317/499): loss=0.33020863625038555, w0=-0.3146639344497911, w1=0.047755214926443613\n",
      "Gradient Descent(318/499): loss=0.33015073801051203, w0=-0.3146639377273276, w1=0.04769504462487626\n",
      "Gradient Descent(319/499): loss=0.3300934830251433, w0=-0.31466394084098714, w1=0.04763547823337206\n",
      "Gradient Descent(320/499): loss=0.330036862640393, w0=-0.3146639437989636, w1=0.047576506369844236\n",
      "Gradient Descent(321/499): loss=0.3299808683301583, w0=-0.3146639466090412, w1=0.0475181198100184\n",
      "Gradient Descent(322/499): loss=0.3299254916940506, w0=-0.3146639492786148, w1=0.04746030948475065\n",
      "Gradient Descent(323/499): loss=0.329870724455363, w0=-0.31466395181470963, w1=0.04740306647739185\n",
      "Gradient Descent(324/499): loss=0.3298165584590769, w0=-0.3146639542239996, w1=0.04734638202119719\n",
      "Gradient Descent(325/499): loss=0.3297629856699027, w0=-0.314663956512825, w1=0.0472902474967803\n",
      "Gradient Descent(326/499): loss=0.3297099981703574, w0=-0.3146639586872091, w1=0.047234654429611084\n",
      "Gradient Descent(327/499): loss=0.32965758815887775, w0=-0.31466396075287384, w1=0.04717959448755654\n",
      "Gradient Descent(328/499): loss=0.3296057479479656, w0=-0.31466396271525526, w1=0.04712505947846377\n",
      "Gradient Descent(329/499): loss=0.3295544699623685, w0=-0.31466396457951756, w1=0.0470710413477845\n",
      "Gradient Descent(330/499): loss=0.32950374673729166, w0=-0.31466396635056665, w1=0.04701753217624029\n",
      "Gradient Descent(331/499): loss=0.32945357091664323, w0=-0.31466396803306323, w1=0.046964524177527826\n",
      "Gradient Descent(332/499): loss=0.3294039352513092, w0=-0.3146639696314349, w1=0.04691200969606352\n",
      "Gradient Descent(333/499): loss=0.32935483259746046, w0=-0.31466397114988787, w1=0.046859981204766726\n",
      "Gradient Descent(334/499): loss=0.3293062559148893, w0=-0.31466397259241813, w1=0.046808431302880954\n",
      "Gradient Descent(335/499): loss=0.32925819826537545, w0=-0.3146639739628218, w1=0.04675735271383234\n",
      "Gradient Descent(336/499): loss=0.3292106528110803, w0=-0.3146639752647052, w1=0.046706738283124806\n",
      "Gradient Descent(337/499): loss=0.3291636128129706, w0=-0.3146639765014943, w1=0.04665658097627117\n",
      "Gradient Descent(338/499): loss=0.3291170716292681, w0=-0.3146639776764439, w1=0.04660687387675971\n",
      "Gradient Descent(339/499): loss=0.3290710227139286, w0=-0.3146639787926459, w1=0.046557610184055374\n",
      "Gradient Descent(340/499): loss=0.32902545961514446, w0=-0.3146639798530378, w1=0.04650878321163526\n",
      "Gradient Descent(341/499): loss=0.3289803759738763, w0=-0.31466398086041, w1=0.04646038638505758\n",
      "Gradient Descent(342/499): loss=0.32893576552240733, w0=-0.31466398181741345, w1=0.0464124132400636\n",
      "Gradient Descent(343/499): loss=0.3288916220829249, w0=-0.3146639827265667, w1=0.046364857420712016\n",
      "Gradient Descent(344/499): loss=0.3288479395661248, w0=-0.3146639835902622, w1=0.046317712677545114\n",
      "Gradient Descent(345/499): loss=0.32880471196984096, w0=-0.31466398441077287, w1=0.046270972865786276\n",
      "Gradient Descent(346/499): loss=0.32876193337769727, w0=-0.3146639851902579, w1=0.046224631943568156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(347/499): loss=0.3287195979577834, w0=-0.3146639859307686, w1=0.04617868397019109\n",
      "Gradient Descent(348/499): loss=0.3286776999613528, w0=-0.3146639866342537, w1=0.04613312310441122\n",
      "Gradient Descent(349/499): loss=0.32863623372154305, w0=-0.31466398730256445, w1=0.04608794360275772\n",
      "Gradient Descent(350/499): loss=0.3285951936521174, w0=-0.3146639879374596, w1=0.046043139817878706\n",
      "Gradient Descent(351/499): loss=0.328554574246229, w0=-0.3146639885406099, w1=0.045998706196915345\n",
      "Gradient Descent(352/499): loss=0.32851437007520323, w0=-0.3146639891136026, w1=0.045954637279903586\n",
      "Gradient Descent(353/499): loss=0.3284745757873438, w0=-0.3146639896579456, w1=0.04591092769820313\n",
      "Gradient Descent(354/499): loss=0.32843518610675654, w0=-0.3146639901750714, w1=0.04586757217295309\n",
      "Gradient Descent(355/499): loss=0.32839619583219415, w0=-0.3146639906663408, w1=0.04582456551355394\n",
      "Gradient Descent(356/499): loss=0.3283575998359194, w0=-0.31466399113304666, w1=0.04578190261617528\n",
      "Gradient Descent(357/499): loss=0.32831939306258784, w0=-0.31466399157641717, w1=0.04573957846228893\n",
      "Gradient Descent(358/499): loss=0.3282815705281489, w0=-0.3146639919976191, w1=0.045697588117227024\n",
      "Gradient Descent(359/499): loss=0.3282441273187655, w0=-0.31466399239776077, w1=0.045655926728764486\n",
      "Gradient Descent(360/499): loss=0.32820705858975097, w0=-0.3146639927778953, w1=0.04561458952572572\n",
      "Gradient Descent(361/499): loss=0.3281703595645241, w0=-0.31466399313902305, w1=0.045573571816614825\n",
      "Gradient Descent(362/499): loss=0.32813402553358123, w0=-0.31466399348209434, w1=0.045532868988269186\n",
      "Gradient Descent(363/499): loss=0.3280980518534847, w0=-0.31466399380801197, w1=0.04549247650453581\n",
      "Gradient Descent(364/499): loss=0.3280624339458693, w0=-0.3146639941176336, w1=0.04545238990497021\n",
      "Gradient Descent(365/499): loss=0.3280271672964635, w0=-0.3146639944117742, w1=0.045412604803557306\n",
      "Gradient Descent(366/499): loss=0.32799224745412714, w0=-0.3146639946912076, w1=0.04537311688745402\n",
      "Gradient Descent(367/499): loss=0.3279576700299056, w0=-0.31466399495666925, w1=0.04533392191575324\n",
      "Gradient Descent(368/499): loss=0.3279234306960983, w0=-0.31466399520885774, w1=0.0452950157182687\n",
      "Gradient Descent(369/499): loss=0.32788952518534253, w0=-0.31466399544843676, w1=0.045256394194340445\n",
      "Gradient Descent(370/499): loss=0.32785594928971307, w0=-0.31466399567603676, w1=0.04521805331166058\n",
      "Gradient Descent(371/499): loss=0.32782269885983495, w0=-0.3146639958922567, w1=0.04517998910511887\n",
      "Gradient Descent(372/499): loss=0.32778976980401203, w0=-0.31466399609766554, w1=0.045142197675667926\n",
      "Gradient Descent(373/499): loss=0.3277571580873678, w0=-0.3146639962928039, w1=0.0451046751892076\n",
      "Gradient Descent(374/499): loss=0.3277248597310021, w0=-0.3146639964781852, w1=0.04506741787548825\n",
      "Gradient Descent(375/499): loss=0.3276928708111598, w0=-0.3146639966542974, w1=0.045030422027032606\n",
      "Gradient Descent(376/499): loss=0.3276611874584133, w0=-0.3146639968216039, w1=0.04499368399807586\n",
      "Gradient Descent(377/499): loss=0.32762980585685897, w0=-0.31466399698054504, w1=0.04495720020352368\n",
      "Gradient Descent(378/499): loss=0.32759872224332387, w0=-0.31466399713153903, w1=0.0449209671179279\n",
      "Gradient Descent(379/499): loss=0.327567932906589, w0=-0.31466399727498323, w1=0.0448849812744795\n",
      "Gradient Descent(380/499): loss=0.327537434186621, w0=-0.31466399741125517, w1=0.044849239264018605\n",
      "Gradient Descent(381/499): loss=0.3275072224738189, w0=-0.31466399754071345, w1=0.04481373773406128\n",
      "Gradient Descent(382/499): loss=0.32747729420827104, w0=-0.31466399766369874, w1=0.04477847338784274\n",
      "Gradient Descent(383/499): loss=0.3274476458790251, w0=-0.31466399778053467, w1=0.044743442983376715\n",
      "Gradient Descent(384/499): loss=0.32741827402336837, w0=-0.31466399789152877, w1=0.04470864333253076\n",
      "Gradient Descent(385/499): loss=0.3273891752261206, w0=-0.3146639979969731, w1=0.04467407130011718\n",
      "Gradient Descent(386/499): loss=0.327360346118937, w0=-0.3146639980971451, w1=0.044639723802999284\n",
      "Gradient Descent(387/499): loss=0.32733178337962243, w0=-0.31466399819230845, w1=0.044605597809212784\n",
      "Gradient Descent(388/499): loss=0.32730348373145757, w0=-0.3146639982827136, w1=0.044571690337102\n",
      "Gradient Descent(389/499): loss=0.3272754439425331, w0=-0.3146639983685984, w1=0.04453799845447067\n",
      "Gradient Descent(390/499): loss=0.3272476608250971, w0=-0.31466399845018883, w1=0.04450451927774709\n",
      "Gradient Descent(391/499): loss=0.32722013123491056, w0=-0.3146639985276997, w1=0.044471249971163355\n",
      "Gradient Descent(392/499): loss=0.32719285207061427, w0=-0.314663998601335, w1=0.04443818774594849\n",
      "Gradient Descent(393/499): loss=0.32716582027310487, w0=-0.3146639986712884, w1=0.04440532985953512\n",
      "Gradient Descent(394/499): loss=0.3271390328249206, w0=-0.3146639987377441, w1=0.04437267361477961\n",
      "Gradient Descent(395/499): loss=0.32711248674963694, w0=-0.3146639988008769, w1=0.04434021635919535\n",
      "Gradient Descent(396/499): loss=0.32708617911127164, w0=-0.31466399886085306, w1=0.04430795548419892\n",
      "Gradient Descent(397/499): loss=0.3270601070136984, w0=-0.3146639989178303, w1=0.04427588842436905\n",
      "Gradient Descent(398/499): loss=0.32703426760007026, w0=-0.3146639989719586, w1=0.04424401265671805\n",
      "Gradient Descent(399/499): loss=0.3270086580522519, w0=-0.31466399902338044, w1=0.04421232569997548\n",
      "Gradient Descent(400/499): loss=0.3269832755902597, w0=-0.3146639990722311, w1=0.04418082511388401\n",
      "Gradient Descent(401/499): loss=0.3269581174717117, w0=-0.3146639991186392, w1=0.04414950849850707\n",
      "Gradient Descent(402/499): loss=0.32693318099128554, w0=-0.3146639991627268, w1=0.0441183734935482\n",
      "Gradient Descent(403/499): loss=0.32690846348018415, w0=-0.31466399920460997, w1=0.044087417777681846\n",
      "Gradient Descent(404/499): loss=0.3268839623056107, w0=-0.3146639992443989, w1=0.04405663906789553\n",
      "Gradient Descent(405/499): loss=0.32685967487025086, w0=-0.31466399928219835, w1=0.044026035118842996\n",
      "Gradient Descent(406/499): loss=0.326835598611763, w0=-0.31466399931810773, w1=0.043995603722208336\n",
      "Gradient Descent(407/499): loss=0.32681173100227595, w0=-0.31466399935222156, w1=0.043965342706080796\n",
      "Gradient Descent(408/499): loss=0.32678806954789613, w0=-0.31466399938462963, w1=0.04393524993434014\n",
      "Gradient Descent(409/499): loss=0.3267646117882193, w0=-0.31466399941541723, w1=0.043905323306052374\n",
      "Gradient Descent(410/499): loss=0.32674135529585235, w0=-0.3146639994446654, w1=0.0438755607548756\n",
      "Gradient Descent(411/499): loss=0.3267182976759408, w0=-0.31466399947245105, w1=0.04384596024847598\n",
      "Gradient Descent(412/499): loss=0.32669543656570416, w0=-0.3146639994988474, w1=0.04381651978795344\n",
      "Gradient Descent(413/499): loss=0.3266727696339782, w0=-0.3146639995239238, w1=0.0437872374072771\n",
      "Gradient Descent(414/499): loss=0.32665029458076394, w0=-0.3146639995477464, w1=0.04375811117273022\n",
      "Gradient Descent(415/499): loss=0.3266280091367837, w0=-0.3146639995703777, w1=0.04372913918236448\n",
      "Gradient Descent(416/499): loss=0.326605911063044, w0=-0.31466399959187746, w1=0.04370031956546343\n",
      "Gradient Descent(417/499): loss=0.3265839981504043, w0=-0.3146639996123021, w1=0.04367165048201503\n",
      "Gradient Descent(418/499): loss=0.3265622682191534, w0=-0.3146639996317055, w1=0.043643130122193036\n",
      "Gradient Descent(419/499): loss=0.32654071911859117, w0=-0.31466399965013864, w1=0.04361475670584711\n",
      "Gradient Descent(420/499): loss=0.3265193487266169, w0=-0.3146639996676501, w1=0.043586528482001556\n",
      "Gradient Descent(421/499): loss=0.32649815494932477, w0=-0.3146639996842859, w1=0.04355844372836246\n",
      "Gradient Descent(422/499): loss=0.3264771357206033, w0=-0.3146639997000898, w1=0.04353050075083316\n",
      "Gradient Descent(423/499): loss=0.32645628900174284, w0=-0.31466399971510345, w1=0.04350269788303783\n",
      "Gradient Descent(424/499): loss=0.32643561278104827, w0=-0.3146639997293664, w1=0.04347503348585314\n",
      "Gradient Descent(425/499): loss=0.3264151050734566, w0=-0.3146639997429161, w1=0.043447505946947754\n",
      "Gradient Descent(426/499): loss=0.32639476392016153, w0=-0.31466399975578824, w1=0.04342011368032962\n",
      "Gradient Descent(427/499): loss=0.3263745873882433, w0=-0.31466399976801673, w1=0.04339285512590085\n",
      "Gradient Descent(428/499): loss=0.3263545735703038, w0=-0.3146639997796337, w1=0.043365728749020095\n",
      "Gradient Descent(429/499): loss=0.32633472058410656, w0=-0.3146639997906698, w1=0.04333873304007231\n",
      "Gradient Descent(430/499): loss=0.3263150265722235, w0=-0.314663999801154, w1=0.04331186651404575\n",
      "Gradient Descent(431/499): loss=0.3262954897016859, w0=-0.3146639998111139, w1=0.04328512771011606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(432/499): loss=0.32627610816364083, w0=-0.3146639998205758, w1=0.04325851519123743\n",
      "Gradient Descent(433/499): loss=0.3262568801730123, w0=-0.31466399982956456, w1=0.04323202754374056\n",
      "Gradient Descent(434/499): loss=0.32623780396816837, w0=-0.3146639998381038, w1=0.043205663376937424\n",
      "Gradient Descent(435/499): loss=0.32621887781059195, w0=-0.31466399984621596, w1=0.0431794213227327\n",
      "Gradient Descent(436/499): loss=0.326200099984558, w0=-0.3146639998539225, w1=0.04315330003524172\n",
      "Gradient Descent(437/499): loss=0.32618146879681365, w0=-0.31466399986124366, w1=0.04312729819041485\n",
      "Gradient Descent(438/499): loss=0.32616298257626436, w0=-0.3146639998681987, w1=0.04310141448566821\n",
      "Gradient Descent(439/499): loss=0.3261446396736646, w0=-0.3146639998748059, w1=0.043075647639520584\n",
      "Gradient Descent(440/499): loss=0.32612643846131256, w0=-0.3146639998810827, w1=0.04304999639123646\n",
      "Gradient Descent(441/499): loss=0.32610837733274967, w0=-0.3146639998870456, w1=0.04302445950047508\n",
      "Gradient Descent(442/499): loss=0.32609045470246445, w0=-0.31466399989271027, w1=0.04299903574694537\n",
      "Gradient Descent(443/499): loss=0.3260726690056007, w0=-0.31466399989809163, w1=0.04297372393006672\n",
      "Gradient Descent(444/499): loss=0.3260550186976704, w0=-0.3146639999032039, w1=0.04294852286863544\n",
      "Gradient Descent(445/499): loss=0.3260375022542699, w0=-0.31466399990806043, w1=0.042923431400496814\n",
      "Gradient Descent(446/499): loss=0.326020118170801, w0=-0.31466399991267413, w1=0.04289844838222275\n",
      "Gradient Descent(447/499): loss=0.3260028649621962, w0=-0.31466399991705707, w1=0.04287357268879477\n",
      "Gradient Descent(448/499): loss=0.32598574116264706, w0=-0.3146639999212208, w1=0.04284880321329239\n",
      "Gradient Descent(449/499): loss=0.32596874532533765, w0=-0.3146639999251763, w1=0.0428241388665867\n",
      "Gradient Descent(450/499): loss=0.32595187602218145, w0=-0.31466399992893396, w1=0.04279957857703917\n",
      "Gradient Descent(451/499): loss=0.3259351318435615, w0=-0.31466399993250366, w1=0.04277512129020544\n",
      "Gradient Descent(452/499): loss=0.32591851139807515, w0=-0.31466399993589483, w1=0.04275076596854418\n",
      "Gradient Descent(453/499): loss=0.3259020133122825, w0=-0.31466399993911637, w1=0.042726511591130764\n",
      "Gradient Descent(454/499): loss=0.3258856362304578, w0=-0.31466399994217675, w1=0.04270235715337588\n",
      "Gradient Descent(455/499): loss=0.32586937881434513, w0=-0.3146639999450841, w1=0.04267830166674873\n",
      "Gradient Descent(456/499): loss=0.32585323974291724, w0=-0.314663999947846, w1=0.04265434415850503\n",
      "Gradient Descent(457/499): loss=0.3258372177121377, w0=-0.31466399995046973, w1=0.042630483671419514\n",
      "Gradient Descent(458/499): loss=0.3258213114347282, w0=-0.31466399995296224, w1=0.04260671926352293\n",
      "Gradient Descent(459/499): loss=0.325805519639936, w0=-0.31466399995533006, w1=0.0425830500078435\n",
      "Gradient Descent(460/499): loss=0.32578984107330755, w0=-0.31466399995757943, w1=0.04255947499215277\n",
      "Gradient Descent(461/499): loss=0.325774274496465, w0=-0.3146639999597163, w1=0.04253599331871562\n",
      "Gradient Descent(462/499): loss=0.3257588186868841, w0=-0.3146639999617462, w1=0.0425126041040446\n",
      "Gradient Descent(463/499): loss=0.3257434724376773, w0=-0.31466399996367456, w1=0.04248930647865837\n",
      "Gradient Descent(464/499): loss=0.3257282345573793, w0=-0.3146639999655064, w1=0.04246609958684415\n",
      "Gradient Descent(465/499): loss=0.3257131038697353, w0=-0.31466399996724664, w1=0.04244298258642429\n",
      "Gradient Descent(466/499): loss=0.32569807921349303, w0=-0.3146639999688998, w1=0.042419954648526624\n",
      "Gradient Descent(467/499): loss=0.325683159442197, w0=-0.3146639999704703, w1=0.04239701495735883\n",
      "Gradient Descent(468/499): loss=0.32566834342398615, w0=-0.31466399997196215, w1=0.0423741627099865\n",
      "Gradient Descent(469/499): loss=0.3256536300413947, w0=-0.31466399997337935, w1=0.042351397116114985\n",
      "Gradient Descent(470/499): loss=0.32563901819115537, w0=-0.31466399997472566, w1=0.04232871739787492\n",
      "Gradient Descent(471/499): loss=0.325624506784005, w0=-0.3146639999760046, w1=0.04230612278961134\n",
      "Gradient Descent(472/499): loss=0.32561009474449426, w0=-0.3146639999772195, w1=0.04228361253767637\n",
      "Gradient Descent(473/499): loss=0.3255957810107992, w0=-0.31466399997837363, w1=0.04226118590022537\n",
      "Gradient Descent(474/499): loss=0.32558156453453513, w0=-0.31466399997947, w1=0.04223884214701662\n",
      "Gradient Descent(475/499): loss=0.3255674442805743, w0=-0.3146639999805114, w1=0.04221658055921422\n",
      "Gradient Descent(476/499): loss=0.32555341922686526, w0=-0.31466399998150074, w1=0.042194400429194465\n",
      "Gradient Descent(477/499): loss=0.3255394883642549, w0=-0.31466399998244055, w1=0.04217230106035537\n",
      "Gradient Descent(478/499): loss=0.32552565069631356, w0=-0.31466399998333333, w1=0.0421502817669295\n",
      "Gradient Descent(479/499): loss=0.32551190523916235, w0=-0.31466399998418143, w1=0.042128341873799866\n",
      "Gradient Descent(480/499): loss=0.3254982510213027, w0=-0.31466399998498706, w1=0.042106480716319006\n",
      "Gradient Descent(481/499): loss=0.3254846870834482, w0=-0.31466399998575234, w1=0.042084697640131064\n",
      "Gradient Descent(482/499): loss=0.3254712124783599, w0=-0.3146639999864793, w1=0.042062992000996884\n",
      "Gradient Descent(483/499): loss=0.32545782627068254, w0=-0.3146639999871699, w1=0.042041363164622064\n",
      "Gradient Descent(484/499): loss=0.32544452753678393, w0=-0.31466399998782585, w1=0.04201981050648789\n",
      "Gradient Descent(485/499): loss=0.32543131536459685, w0=-0.31466399998844896, w1=0.04199833341168514\n",
      "Gradient Descent(486/499): loss=0.32541818885346224, w0=-0.3146639999890409, w1=0.04197693127475072\n",
      "Gradient Descent(487/499): loss=0.32540514711397556, w0=-0.3146639999896031, w1=0.041955603499506944\n",
      "Gradient Descent(488/499): loss=0.3253921892678346, w0=-0.31466399999013717, w1=0.04193434949890369\n",
      "Gradient Descent(489/499): loss=0.32537931444769, w0=-0.3146639999906445, w1=0.0419131686948631\n",
      "Gradient Descent(490/499): loss=0.325366521796998, w0=-0.3146639999911264, w1=0.04189206051812691\n",
      "Gradient Descent(491/499): loss=0.3253538104698739, w0=-0.3146639999915841, w1=0.041871024408106444\n",
      "Gradient Descent(492/499): loss=0.3253411796309496, w0=-0.31466399999201894, w1=0.04185005981273503\n",
      "Gradient Descent(493/499): loss=0.3253286284552323, w0=-0.31466399999243194, w1=0.041829166188322976\n",
      "Gradient Descent(494/499): loss=0.32531615612796394, w0=-0.31466399999282424, w1=0.041808342999414994\n",
      "Gradient Descent(495/499): loss=0.325303761844485, w0=-0.31466399999319683, w1=0.04178758971864999\n",
      "Gradient Descent(496/499): loss=0.3252914448100982, w0=-0.31466399999355077, w1=0.04176690582662325\n",
      "Gradient Descent(497/499): loss=0.32527920423993495, w0=-0.31466399999388694, w1=0.041746290811750984\n",
      "Gradient Descent(498/499): loss=0.32526703935882434, w0=-0.31466399999420624, w1=0.04172574417013711\n",
      "Gradient Descent(499/499): loss=0.32525494940116234, w0=-0.31466399999450956, w1=0.04170526540544228\n"
     ]
    }
   ],
   "source": [
    "w = np.array(np.random.rand((tX.shape[1])))\n",
    "max_iters = 500 #choosing number of iterations\n",
    "gamma = 5 *10**(-2)\n",
    "(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)\n",
    "#gamma = 5/max_iters #choosing gamma \n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1a5e51a4d3eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
