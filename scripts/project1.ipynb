{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from extend import *\n",
    "from gradient_descent import *\n",
    "from stochastic_gradient_descent import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.copy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_col = []\n",
    "for i in range(tX.shape[1]):\n",
    "    col = tX[:,i]\n",
    "    places_999 = (col == -999.0)\n",
    "    mean_col = col[col!=-999.0].mean()\n",
    "    col[places_999] = mean_col\n",
    "    tX[:,i] = col\n",
    "    mean_by_col.append(mean_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(tX > 0,axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_columns = tX[:,np.all(tX > 0,axis=0)]\n",
    "positive_columns_log = np.log(positive_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 211)\n",
      "(250000, 210)\n",
      "(250000, 270)\n"
     ]
    }
   ],
   "source": [
    "degree = 7\n",
    "tX = build_poly(tX, degree)\n",
    "print(tX.shape)\n",
    "tX = tX[:,1:]\n",
    "print(tX.shape)\n",
    "tX = extend(tX,[np.cos,np.sin])\n",
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[(tX, positive_columns_log)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 282)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(tX,axis = 0)\n",
    "std = np.std(tX,axis = 0)\n",
    "tX = (tX-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[(np.ones(tX.shape[0]) , tX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 283)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/4999): loss=328.9924474304812, w0=0.8373327131806233, gamma=0.03814682999606126\n",
      "Gradient Descent(1/4999): loss=67.36500416052242, w0=0.7933876904073299, gamma=0.03662440372783959\n",
      "Gradient Descent(2/4999): loss=24.629261037201427, w0=0.7528059579472014, gamma=0.07101784066818294\n",
      "Gradient Descent(3/4999): loss=18.48181280571487, w0=0.6769965465567306, gamma=0.1301131349468076\n",
      "Gradient Descent(4/4999): loss=12.210778674086157, w0=0.5479684840428307, gamma=0.22590374340637967\n",
      "Gradient Descent(5/4999): loss=7.07098229571129, w0=0.3530965767157508, gamma=0.15640856689574764\n",
      "Gradient Descent(6/4999): loss=3.92252112919229, w0=0.2486531018831586, gamma=0.03703163230455106\n",
      "Gradient Descent(7/4999): loss=4.276101055476611, w0=0.22779255009546143, gamma=0.03520638042814465\n",
      "Gradient Descent(8/4999): loss=2.7473795301137147, w0=0.20869461842722797, gamma=0.05493028736523836\n",
      "Gradient Descent(9/4999): loss=2.6063591366778582, w0=0.17994637912219144, gamma=0.37377205199522034\n",
      "Gradient Descent(10/4999): loss=2.4153533826718037, w0=-0.004925157218855114, gamma=0.7620767343484819\n",
      "Gradient Descent(11/4999): loss=1.4638883443863941, w0=-0.24096992302420284, gamma=0.30339421503215985\n",
      "Gradient Descent(12/4999): loss=0.8597590886564769, w0=-0.2633282796603487, gamma=0.06808839357750918\n",
      "Gradient Descent(13/4999): loss=1.2918063358104728, w0=-0.26682364639138595, gamma=0.0493993081982462\n",
      "Gradient Descent(14/4999): loss=0.7963125181577859, w0=-0.26918692676354705, gamma=0.035638975240546655\n",
      "Gradient Descent(15/4999): loss=0.7153768585295305, w0=-0.27080768305061054, gamma=0.035541070850854485\n",
      "Gradient Descent(16/4999): loss=0.6953953467903808, w0=-0.2723663835185377, gamma=0.11069607533555777\n",
      "Gradient Descent(17/4999): loss=0.6887424956152719, w0=-0.2770485636589987, gamma=0.20157686178074843\n",
      "Gradient Descent(18/4999): loss=0.6695641350062052, w0=-0.2846309652709953, gamma=0.5963152309494592\n",
      "Gradient Descent(19/4999): loss=0.6381023842466567, w0=-0.3025401213112424, gamma=0.26581657757662824\n",
      "Gradient Descent(20/4999): loss=0.5650411757907882, w0=-0.30576284925122804, gamma=0.07061106611393635\n",
      "Gradient Descent(21/4999): loss=0.5547919170640677, w0=-0.30639136899525593, gamma=0.05651459588153079\n",
      "Gradient Descent(22/4999): loss=0.5351911335324648, w0=-0.3068588933933719, gamma=0.03725156877567726\n",
      "Gradient Descent(23/4999): loss=0.5299357230789141, w0=-0.3071496458589381, gamma=0.03626873930511013\n",
      "Gradient Descent(24/4999): loss=0.5268313449682854, w0=-0.30742218201033356, gamma=0.2564995216059044\n",
      "Gradient Descent(25/4999): loss=0.5242142551348523, w0=-0.30927970486029355, gamma=1.9114778042830651\n",
      "Gradient Descent(26/4999): loss=0.5065935611007898, w0=-0.3195716655121377, gamma=0.6489333352881591\n",
      "Gradient Descent(27/4999): loss=0.4183601693481003, w0=-0.31638691776349953, gamma=0.05669967304172642\n",
      "Gradient Descent(28/4999): loss=0.5889801058988778, w0=-0.31628922888964217, gamma=0.04133612866466091\n",
      "Gradient Descent(29/4999): loss=0.435649377818162, w0=-0.3162220482191918, gamma=0.03649159385411313\n",
      "Gradient Descent(30/4999): loss=0.4039363171083821, w0=-0.31616519255639824, gamma=0.04374464218276925\n",
      "Gradient Descent(31/4999): loss=0.40095929671401026, w0=-0.3160995234252051, gamma=0.07945615279807444\n",
      "Gradient Descent(32/4999): loss=0.3997111093664846, w0=-0.3159854622566482, gamma=0.08369156844521061\n",
      "Gradient Descent(33/4999): loss=0.39828194371669756, w0=-0.31587486700781225, gamma=0.3672461201191961\n",
      "Gradient Descent(34/4999): loss=0.3969483172905996, w0=-0.3154301807974871, gamma=0.20798712916321516\n",
      "Gradient Descent(35/4999): loss=0.3913723281884083, w0=-0.3152708250531385, gamma=0.03674548743644169\n",
      "Gradient Descent(36/4999): loss=0.3890354545107032, w0=-0.31524852697079364, gamma=0.03509721547490994\n",
      "Gradient Descent(37/4999): loss=0.3879722693724208, w0=-0.31522801170177034, gamma=0.06158085040123517\n",
      "Gradient Descent(38/4999): loss=0.3875090314785489, w0=-0.3151932793815762, gamma=5.1716883654395955\n",
      "Gradient Descent(39/4999): loss=0.38670745983080274, w0=-0.31245601136485535, gamma=5.357461962854489\n",
      "Gradient Descent(40/4999): loss=0.34242012822638346, w0=-0.3242852264884959, gamma=0.1323417057712481\n",
      "Gradient Descent(41/4999): loss=0.33048967262174617, w0=-0.3230119369630308, gamma=0.03958805001730535\n",
      "Gradient Descent(42/4999): loss=0.3402413305263456, w0=-0.32268145841687856, gamma=0.0363019862052175\n",
      "Gradient Descent(43/4999): loss=0.32721835965035295, w0=-0.3223904087519262, gamma=0.03944563698584195\n",
      "Gradient Descent(44/4999): loss=0.32678039389464864, w0=-0.32208563563698167, gamma=0.0563943419053915\n",
      "Gradient Descent(45/4999): loss=0.3266686285898685, w0=-0.3216670973792124, gamma=0.08783935971964549\n",
      "Gradient Descent(46/4999): loss=0.32653938946990446, w0=-0.3210519497891181, gamma=0.6513368199421373\n",
      "Gradient Descent(47/4999): loss=0.3263484147468643, w0=-0.31689124288566317, gamma=0.921495599554992\n",
      "Gradient Descent(48/4999): loss=0.3250927458397201, w0=-0.3148388483646662, gamma=0.039661254879278685\n",
      "Gradient Descent(49/4999): loss=0.32437277146636007, w0=-0.3148319136589894, gamma=0.03547737946694346\n",
      "Gradient Descent(50/4999): loss=0.32365518822528877, w0=-0.314825956522282, gamma=0.0375309505353758\n",
      "Gradient Descent(51/4999): loss=0.32359081262297634, w0=-0.3148198781399393, gamma=0.08213049470630931\n",
      "Gradient Descent(52/4999): loss=0.32353859870362234, w0=-0.31480707579093775, gamma=0.13369846777225433\n",
      "Gradient Descent(53/4999): loss=0.3234273084466524, w0=-0.31478794677649846, gamma=0.3938681660158545\n",
      "Gradient Descent(54/4999): loss=0.3232491986005362, w0=-0.3147391280857239, gamma=0.27936799625394915\n",
      "Gradient Descent(55/4999): loss=0.32273612568885524, w0=-0.31471813970206364, gamma=0.057933571985287306\n",
      "Gradient Descent(56/4999): loss=0.32239961290052405, w0=-0.3147150031955502, gamma=0.051425046966527915\n",
      "Gradient Descent(57/4999): loss=0.32231277560972404, w0=-0.31471238035365745, gamma=0.04071422255873016\n",
      "Gradient Descent(58/4999): loss=0.32224944840016057, w0=-0.31471041058503935, gamma=0.05469296678749777\n",
      "Gradient Descent(59/4999): loss=0.3221997293410016, w0=-0.3147078722522758, gamma=0.7152998941353722\n",
      "Gradient Descent(60/4999): loss=0.32213329036252447, w0=-0.3146764904325416, gamma=1.9301760136469859\n",
      "Gradient Descent(61/4999): loss=0.32128193988457676, w0=-0.3146523816927799, gamma=0.04052526726479159\n",
      "Gradient Descent(62/4999): loss=0.3197447332984198, w0=-0.3146528525276368, gamma=0.03504214645900503\n",
      "Gradient Descent(63/4999): loss=0.3191586096381825, w0=-0.3146532431588691, gamma=0.035216486297625235\n",
      "Gradient Descent(64/4999): loss=0.3191109068790232, w0=-0.31465362197688945, gamma=0.17044972020371374\n",
      "Gradient Descent(65/4999): loss=0.31907710201568606, w0=-0.31465539090740496, gamma=0.2591404143768104\n",
      "Gradient Descent(66/4999): loss=0.3189150454551936, w0=-0.3146576218702791, gamma=0.4345953869980729\n",
      "Gradient Descent(67/4999): loss=0.31867238483411264, w0=-0.3146603937744286, gamma=0.21257109267321686\n",
      "Gradient Descent(68/4999): loss=0.31827576500801463, w0=-0.3146611603529411, gamma=0.09627565687883528\n",
      "Gradient Descent(69/4999): loss=0.3180876079368114, w0=-0.3146614337414638, gamma=0.08293562938057993\n",
      "Gradient Descent(70/4999): loss=0.3180004437409254, w0=-0.31466164657541634, gamma=0.10327197801722149\n",
      "Gradient Descent(71/4999): loss=0.3179278831165385, w0=-0.31466188961783587, gamma=0.44202229628202705\n",
      "Gradient Descent(72/4999): loss=0.3178382867088887, w0=-0.31466282245212085, gamma=0.3222925259567771\n",
      "Gradient Descent(73/4999): loss=0.3174602802759034, w0=-0.3146632019657551, gamma=0.04127767947161006\n",
      "Gradient Descent(74/4999): loss=0.3172091081095409, w0=-0.31466323490659526, gamma=0.03655147889138176\n",
      "Gradient Descent(75/4999): loss=0.31715896296822965, w0=-0.31466326287174773, gamma=0.03882460518820329\n",
      "Gradient Descent(76/4999): loss=0.3171285755724603, w0=-0.314663291490309, gamma=0.13450260730619223\n",
      "Gradient Descent(77/4999): loss=0.3170970138946143, w0=-0.31466338678618233, gamma=0.9663726217850173\n",
      "Gradient Descent(78/4999): loss=0.3169882375233128, w0=-0.3146639793754222, gamma=0.5308673356631496\n",
      "Gradient Descent(79/4999): loss=0.31622631452590677, w0=-0.3146639903221848, gamma=0.05346676060503356\n",
      "Gradient Descent(80/4999): loss=0.31593058989083145, w0=-0.3146639908394065, gamma=0.048440246412035814\n",
      "Gradient Descent(81/4999): loss=0.3157945768515691, w0=-0.3146639912829486, gamma=0.035897375693955576\n",
      "Gradient Descent(82/4999): loss=0.3157567327900663, w0=-0.3146639915957198, gamma=0.036738801639244104\n",
      "Gradient Descent(83/4999): loss=0.31573044676826684, w0=-0.3146639919043313, gamma=0.34561844606817044\n",
      "Gradient Descent(84/4999): loss=0.31570406993293004, w0=-0.31466399470091533, gamma=1.338048194317869\n",
      "Gradient Descent(85/4999): loss=0.3154579244201533, w0=-0.3146640017857634, gamma=0.16324164291004203\n",
      "Gradient Descent(86/4999): loss=0.31455558397001054, w0=-0.3146640014935493, gamma=0.0713542620907695\n",
      "Gradient Descent(87/4999): loss=0.3144599832062968, w0=-0.31466400138666806, gamma=0.06284143259785174\n",
      "Gradient Descent(88/4999): loss=0.31439254015862494, w0=-0.31466400129925487, gamma=0.03550422258158608\n",
      "Gradient Descent(89/4999): loss=0.31435250646242635, w0=-0.3146640012529715, gamma=0.035728049468843386\n",
      "Gradient Descent(90/4999): loss=0.3143295660931453, w0=-0.31466400120804977, gamma=3.214789360842958\n",
      "Gradient Descent(91/4999): loss=0.3143071391816798, w0=-0.31466399731042166, gamma=22.24848095307657\n",
      "Gradient Descent(92/4999): loss=0.31239315954124763, w0=-0.31466405704530936, gamma=0.2027007314838462\n",
      "Gradient Descent(93/4999): loss=0.30635485169059834, w0=-0.31466404548082283, gamma=0.054583347934430794\n",
      "Gradient Descent(94/4999): loss=0.31347942751247004, w0=-0.31466404299797157, gamma=0.04099718542627731\n",
      "Gradient Descent(95/4999): loss=0.30594121210567843, w0=-0.31466404123490843, gamma=0.0361431123359167\n",
      "Gradient Descent(96/4999): loss=0.3047143209511731, w0=-0.3146640397443116, gamma=0.041285622460511345\n",
      "Gradient Descent(97/4999): loss=0.30464040513256285, w0=-0.31466403810317, gamma=0.07203037773602793\n",
      "Gradient Descent(98/4999): loss=0.30462416310114565, w0=-0.31466403535810755, gamma=0.07294592913865372\n",
      "Gradient Descent(99/4999): loss=0.30461238746239117, w0=-0.31466403277839444, gamma=0.11038456690124875\n",
      "Gradient Descent(100/4999): loss=0.3046030405112696, w0=-0.3146640291594344, gamma=0.10041269032796249\n",
      "Gradient Descent(101/4999): loss=0.30458897352327086, w0=-0.314664026230792, gamma=0.049005588366276484\n",
      "Gradient Descent(102/4999): loss=0.3045762697443249, w0=-0.3146640249450119, gamma=0.03900007196804158\n",
      "Gradient Descent(103/4999): loss=0.30457007798597924, w0=-0.31466402397189625, gamma=0.05980244197723044\n",
      "Gradient Descent(104/4999): loss=0.30456517412303624, w0=-0.3146640225379221, gamma=1.7707833770329424\n",
      "Gradient Descent(105/4999): loss=0.30455767886425333, w0=-0.31466398261641115, gamma=3.180514922003565\n",
      "Gradient Descent(106/4999): loss=0.30434209969441345, w0=-0.3146640378840626, gamma=0.03630155689869387\n",
      "Gradient Descent(107/4999): loss=0.30440415348037997, w0=-0.3146640365085714, gamma=0.03503205998513732\n",
      "Gradient Descent(108/4999): loss=0.3039956871095156, w0=-0.31466403522936737, gamma=0.03511912837309493\n",
      "Gradient Descent(109/4999): loss=0.30399182189220203, w0=-0.31466403399190856, gamma=6.628145361120166\n",
      "Gradient Descent(110/4999): loss=0.303988486373683, w0=-0.31466380864441285, gamma=10.485341717179876\n",
      "Gradient Descent(111/4999): loss=0.30340638958637867, w0=-0.31466581500243396, gamma=0.12047192180317613\n",
      "Gradient Descent(112/4999): loss=0.30491545417934063, w0=-0.314665596344774, gamma=0.10508569031849554\n",
      "Gradient Descent(113/4999): loss=0.3028271871375328, w0=-0.31466542859105534, gamma=0.06068529910789814\n",
      "Gradient Descent(114/4999): loss=0.3027678288334273, w0=-0.31466534189616113, gamma=0.05470225852920544\n",
      "Gradient Descent(115/4999): loss=0.3027182330879501, w0=-0.3146652684910329, gamma=0.05692173846796848\n",
      "Gradient Descent(116/4999): loss=0.3027145688188425, w0=-0.31466519628592515, gamma=0.08892233024493323\n",
      "Gradient Descent(117/4999): loss=0.30271177739963007, w0=-0.3146650899087792, gamma=0.14711856819637384\n",
      "Gradient Descent(118/4999): loss=0.3027074757205504, w0=-0.31466492956194453, gamma=1.0874637327112258\n",
      "Gradient Descent(119/4999): loss=0.3027003900643061, w0=-0.3146639186895335, gamma=2.0124413235236354\n",
      "Gradient Descent(120/4999): loss=0.3026485821929534, w0=-0.31466408230815135, gamma=0.04200798152331273\n",
      "Gradient Descent(121/4999): loss=0.30257192367069086, w0=-0.31466407885026054, gamma=0.035961844322825565\n",
      "Gradient Descent(122/4999): loss=0.3025553349663686, w0=-0.31466407601440993, gamma=0.038839372399204236\n",
      "Gradient Descent(123/4999): loss=0.3025528523810748, w0=-0.3146640730617882, gamma=0.07865095008658386\n",
      "Gradient Descent(124/4999): loss=0.30255102224534386, w0=-0.3146640673148628, gamma=0.0846533831342514\n",
      "Gradient Descent(125/4999): loss=0.3025475219759466, w0=-0.3146640616158438, gamma=0.35217495903427964\n",
      "Gradient Descent(126/4999): loss=0.30254381093656946, w0=-0.31466403991383957, gamma=0.6193980543308173\n",
      "Gradient Descent(127/4999): loss=0.3025284408700483, w0=-0.3146640151869789, gamma=0.04794390225568588\n",
      "Gradient Descent(128/4999): loss=0.3025020748981257, w0=-0.31466401445852216, gamma=0.036216871339533045\n",
      "Gradient Descent(129/4999): loss=0.3024997008378906, w0=-0.3146640139346277, gamma=0.03552512725575368\n",
      "Gradient Descent(130/4999): loss=0.3024981163041772, w0=-0.31466401343935096, gamma=0.19208573517058677\n",
      "Gradient Descent(131/4999): loss=0.3024966044306493, w0=-0.31466401085650625, gamma=0.9037343516830876\n",
      "Gradient Descent(132/4999): loss=0.3024884488467831, w0=-0.314664001038816, gamma=0.6367105339082019\n",
      "Gradient Descent(133/4999): loss=0.3024504695984944, w0=-0.31466400037295084, gamma=0.05748393492441658\n",
      "Gradient Descent(134/4999): loss=0.3024262094087872, w0=-0.314664000351111, gamma=0.052771605822145164\n",
      "Gradient Descent(135/4999): loss=0.30242196664680454, w0=-0.3146640003322139, gamma=0.041517650431504495\n",
      "Gradient Descent(136/4999): loss=0.3024197892632484, w0=-0.31466400031813135, gamma=0.04246664006864177\n",
      "Gradient Descent(137/4999): loss=0.30241809908175654, w0=-0.3146640003043249, gamma=0.12573982599496106\n",
      "Gradient Descent(138/4999): loss=0.30241637813245836, w0=-0.3146640002651812, gamma=0.3784020041124825\n",
      "Gradient Descent(139/4999): loss=0.30241129576764536, w0=-0.31466400016219354, gamma=0.3319485857709357\n",
      "Gradient Descent(140/4999): loss=0.3023960825578798, w0=-0.3146640001060341, gamma=0.0381128864948004\n",
      "Gradient Descent(141/4999): loss=0.30238310731365764, w0=-0.31466400010172646, gamma=0.03507573445641635\n",
      "Gradient Descent(142/4999): loss=0.30238134736128364, w0=-0.3146640000979131, gamma=0.04051986454160933\n",
      "Gradient Descent(143/4999): loss=0.3023799583351847, w0=-0.3146640000936624, gamma=12.156487972145948\n",
      "Gradient Descent(144/4999): loss=0.3023783574600143, w0=-0.31466399887005697, gamma=29.300726355729807\n",
      "Gradient Descent(145/4999): loss=0.3019350713987554, w0=-0.3146640317696556, gamma=0.0807554085722334\n",
      "Gradient Descent(146/4999): loss=0.3067370859602893, w0=-0.3146640292034935, gamma=0.07511883899313608\n",
      "Gradient Descent(147/4999): loss=0.3013856960679289, w0=-0.31466402700922, gamma=0.05329058908624976\n",
      "Gradient Descent(148/4999): loss=0.3012594339371653, w0=-0.3146640255694957, gamma=0.03882245621031826\n",
      "Gradient Descent(149/4999): loss=0.30122846527738484, w0=-0.3146640245765429, gamma=0.039366219545127236\n",
      "Gradient Descent(150/4999): loss=0.3012186387865406, w0=-0.3146640236087708, gamma=0.11263548486824078\n",
      "Gradient Descent(151/4999): loss=0.30121485257886815, w0=-0.3146640209487653, gamma=0.15443223956349153\n",
      "Gradient Descent(152/4999): loss=0.3012093994095823, w0=-0.31466401771247443, gamma=0.09764452950861885\n",
      "Gradient Descent(153/4999): loss=0.30120664350617027, w0=-0.31466401598223587, gamma=0.03509204926229775\n",
      "Gradient Descent(154/4999): loss=0.3012051652807632, w0=-0.3146640154211304, gamma=0.03504679094621948\n",
      "Gradient Descent(155/4999): loss=0.30120447980653414, w0=-0.3146640148804136, gamma=2.121067677198157\n",
      "Gradient Descent(156/4999): loss=0.30120391939142194, w0=-0.31466398330257545, gamma=8.182330173336247\n",
      "Gradient Descent(157/4999): loss=0.3011701986075877, w0=-0.31466411986674214, gamma=0.12036477562193136\n",
      "Gradient Descent(158/4999): loss=0.3010549919493129, w0=-0.31466410543813295, gamma=0.07155503016156613\n",
      "Gradient Descent(159/4999): loss=0.30104672728767673, w0=-0.3146640978929829, gamma=0.05958828680732397\n",
      "Gradient Descent(160/4999): loss=0.3010423327619827, w0=-0.3146640920592738, gamma=0.05578161316519086\n",
      "Gradient Descent(161/4999): loss=0.3010412286998449, w0=-0.3146640869236528, gamma=0.09594650373930734\n",
      "Gradient Descent(162/4999): loss=0.3010403997860887, w0=-0.3146640785829335, gamma=0.16173316447416447\n",
      "Gradient Descent(163/4999): loss=0.3010390046562133, w0=-0.31466406587228923, gamma=0.1087468517731875\n",
      "Gradient Descent(164/4999): loss=0.30103667837266856, w0=-0.3146640587080933, gamma=0.03763004539991847\n",
      "Gradient Descent(165/4999): loss=0.30103512005801836, w0=-0.31466405649863105, gamma=0.03561637263085111\n",
      "Gradient Descent(166/4999): loss=0.3010345781689589, w0=-0.3146640544860954, gamma=0.1358391860833897\n",
      "Gradient Descent(167/4999): loss=0.3010340677492146, w0=-0.31466404708375945, gamma=27.793005943609103\n",
      "Gradient Descent(168/4999): loss=0.3010321221455811, w0=-0.31466273828218, gamma=12.59880139220113\n",
      "Gradient Descent(169/4999): loss=0.30065921746877333, w0=-0.31467863432327425, gamma=0.043838499939868825\n",
      "Gradient Descent(170/4999): loss=0.30643921134898494, w0=-0.31467799277617453, gamma=0.040137798411965256\n",
      "Gradient Descent(171/4999): loss=0.3007951144039413, w0=-0.3146774311366571, gamma=0.03815057873634639\n",
      "Gradient Descent(172/4999): loss=0.3005318080307513, w0=-0.3146769187307458, gamma=0.042320129849132845\n",
      "Gradient Descent(173/4999): loss=0.3005208534845254, w0=-0.31467637200807863, gamma=0.05541100189772175\n",
      "Gradient Descent(174/4999): loss=0.3005194370347382, w0=-0.3146756864623167, gamma=0.08836864925721034\n",
      "Gradient Descent(175/4999): loss=0.30051857022385886, w0=-0.3146746537447912, gamma=0.15345868833010942\n",
      "Gradient Descent(176/4999): loss=0.3005174743071659, w0=-0.3146730188339853, gamma=0.08608924074756936\n",
      "Gradient Descent(177/4999): loss=0.3005158271634819, w0=-0.31467224240879554, gamma=0.03545898672433584\n",
      "Gradient Descent(178/4999): loss=0.30051494631742126, w0=-0.3146719501410763, gamma=0.03513324028228442\n",
      "Gradient Descent(179/4999): loss=0.3005145575894572, w0=-0.31467167082660674, gamma=0.251483133796412\n",
      "Gradient Descent(180/4999): loss=0.30051419448517946, w0=-0.3146697417412831, gamma=0.4958425424895057\n",
      "Gradient Descent(181/4999): loss=0.3005115990061291, w0=-0.3146668947381191, gamma=0.8360372389166691\n",
      "Gradient Descent(182/4999): loss=0.3005064964085546, w0=-0.3146644746232403, gamma=0.06765890665713803\n",
      "Gradient Descent(183/4999): loss=0.3004979714513898, w0=-0.31466444251026426, gamma=0.05376939122998601\n",
      "Gradient Descent(184/4999): loss=0.300497240279701, w0=-0.3146644187163701, gamma=0.05294332889318948\n",
      "Gradient Descent(185/4999): loss=0.3004966886217486, w0=-0.31466439654775086, gamma=0.06787759215755627\n",
      "Gradient Descent(186/4999): loss=0.30049614855905854, w0=-0.3146643696305563, gamma=0.14627519578559894\n",
      "Gradient Descent(187/4999): loss=0.30049545645797365, w0=-0.3146643155617225, gamma=0.15938166787006555\n",
      "Gradient Descent(188/4999): loss=0.3004939658926837, w0=-0.31466426526582286, gamma=0.04729134770300524\n",
      "Gradient Descent(189/4999): loss=0.300492343847563, w0=-0.3146642527207046, gamma=0.03600014488971899\n",
      "Gradient Descent(190/4999): loss=0.3004918621552334, w0=-0.31466424362246376, gamma=0.04292854652568927\n",
      "Gradient Descent(191/4999): loss=0.30049149594276103, w0=-0.3146642331637969, gamma=9.285062065510523\n",
      "Gradient Descent(192/4999): loss=0.3004910594503531, w0=-0.31466206815671877, gamma=62.69612249481539\n",
      "Gradient Descent(193/4999): loss=0.30039799323795835, w0=-0.3147831867906835, gamma=0.03808934685311415\n",
      "Gradient Descent(194/4999): loss=0.3005835510959653, w0=-0.3147786470434042, gamma=0.03503381529998572\n",
      "Gradient Descent(195/4999): loss=0.2998627876087865, w0=-0.31477463051981547, gamma=0.03520245746486571\n",
      "Gradient Descent(196/4999): loss=0.299856350387007, w0=-0.31477073605340067, gamma=0.25692675476842963\n",
      "Gradient Descent(197/4999): loss=0.29985554099993733, w0=-0.3147433127037835, gamma=0.2774012217971293\n",
      "Gradient Descent(198/4999): loss=0.2998510279153879, w0=-0.31472131126090785, gamma=0.3721781484054878\n",
      "Gradient Descent(199/4999): loss=0.29984756113820876, w0=-0.31469998125933607, gamma=0.12570183580161762\n",
      "Gradient Descent(200/4999): loss=0.29984393195753606, w0=-0.3146954583481042, gamma=0.07626002549352294\n",
      "Gradient Descent(201/4999): loss=0.2998427279434444, w0=-0.31469305933314257, gamma=0.07634154811373352\n",
      "Gradient Descent(202/4999): loss=0.2998419973529805, w0=-0.31469084089812965, gamma=0.43323911805755266\n",
      "Gradient Descent(203/4999): loss=0.29984136413457396, w0=-0.31467921236806773, gamma=1.0990886483741658\n",
      "Gradient Descent(204/4999): loss=0.2998379172115306, w0=-0.31466249261933066, gamma=0.08674224354241647\n",
      "Gradient Descent(205/4999): loss=0.2998302711592078, w0=-0.3146626233723064, gamma=0.041054093051203853\n",
      "Gradient Descent(206/4999): loss=0.29982968080621025, w0=-0.3146626798882212, gamma=0.036166970079186594\n",
      "Gradient Descent(207/4999): loss=0.29982913621643376, w0=-0.314662727632412, gamma=0.03767645467502654\n",
      "Gradient Descent(208/4999): loss=0.29982888017056414, w0=-0.3146627755704488, gamma=0.06745518972814044\n",
      "Gradient Descent(209/4999): loss=0.29982862332168786, w0=-0.3146628581641057, gamma=0.15817130592471348\n",
      "Gradient Descent(210/4999): loss=0.2998281645606489, w0=-0.31466303876867635, gamma=0.9467350237604327\n",
      "Gradient Descent(211/4999): loss=0.29982709070008484, w0=-0.3146639487934294, gamma=0.5012263514552523\n",
      "Gradient Descent(212/4999): loss=0.2998206993951191, w0=-0.31466397445601524, gamma=0.04811606129611872\n",
      "Gradient Descent(213/4999): loss=0.29981750775799954, w0=-0.31466397568475557, gamma=0.04229156255289943\n",
      "Gradient Descent(214/4999): loss=0.2998170348377024, w0=-0.3146639767127902, gamma=0.03638231986938149\n",
      "Gradient Descent(215/4999): loss=0.2998167447588836, w0=-0.31466397755977915, gamma=0.038529427420308335\n",
      "Gradient Descent(216/4999): loss=0.299816502133299, w0=-0.31466397842411925, gamma=0.1955386743187732\n",
      "Gradient Descent(217/4999): loss=0.2998162456718816, w0=-0.3146639826416745, gamma=1.2515130339013694\n",
      "Gradient Descent(218/4999): loss=0.29981494498162153, w0=-0.31466400435711633, gamma=0.29083944649173965\n",
      "Gradient Descent(219/4999): loss=0.2998066522467313, w0=-0.3146640030878673, gamma=0.05457012926573028\n",
      "Gradient Descent(220/4999): loss=0.29980480457195646, w0=-0.31466400291898156, gamma=0.05049273425308623\n",
      "Gradient Descent(221/4999): loss=0.2998043759408893, w0=-0.3146640027712422, gamma=0.035648915948648614\n",
      "Gradient Descent(222/4999): loss=0.2998040422573846, w0=-0.3146640026722019, gamma=0.03609833419832783\n",
      "Gradient Descent(223/4999): loss=0.299803807531486, w0=-0.3146640025754882, gamma=1.480343180365486\n",
      "Gradient Descent(224/4999): loss=0.29980357026063764, w0=-0.3146639987525623, gamma=8.043369956326536\n",
      "Gradient Descent(225/4999): loss=0.29979386028695526, w0=-0.3146640087301192, gamma=0.09438417961234759\n",
      "Gradient Descent(226/4999): loss=0.29974293005215036, w0=-0.3146640079054781, gamma=0.06163504438880249\n",
      "Gradient Descent(227/4999): loss=0.2997415464161369, w0=-0.31466400741779516, gamma=0.05079699793122709\n",
      "Gradient Descent(228/4999): loss=0.29974089883661525, w0=-0.3146640070406403, gamma=0.035717941290467056\n",
      "Gradient Descent(229/4999): loss=0.2997405637919288, w0=-0.3146640067889148, gamma=0.035483446478190614\n",
      "Gradient Descent(230/4999): loss=0.2997403351828535, w0=-0.3146640065477741, gamma=0.14797707643660027\n",
      "Gradient Descent(231/4999): loss=0.2997401113352514, w0=-0.31466400557782503, gamma=0.2561278970421854\n",
      "Gradient Descent(232/4999): loss=0.299739178500614, w0=-0.31466400414740814, gamma=0.3932792741797925\n",
      "Gradient Descent(233/4999): loss=0.2997375655692943, w0=-0.3146640025135851, gamma=0.11432631051750149\n",
      "Gradient Descent(234/4999): loss=0.29973509307719864, w0=-0.31466400222542174, gamma=0.06659949205483386\n",
      "Gradient Descent(235/4999): loss=0.2997343745283092, w0=-0.31466400207674694, gamma=0.05541070330810599\n",
      "Gradient Descent(236/4999): loss=0.2997339559040572, w0=-0.31466400196128785, gamma=0.04387249232370547\n",
      "Gradient Descent(237/4999): loss=0.29973360810596844, w0=-0.31466400187493637, gamma=0.05884367042660719\n",
      "Gradient Descent(238/4999): loss=0.2997333328060314, w0=-0.31466400176419934, gamma=0.40451027073738316\n",
      "Gradient Descent(239/4999): loss=0.2997329636517473, w0=-0.3146640010477517, gamma=0.8288356244878565\n",
      "Gradient Descent(240/4999): loss=0.2997304275761476, w0=-0.314664000173579, gamma=0.04217645083044675\n",
      "Gradient Descent(241/4999): loss=0.2997252625544035, w0=-0.3146640001659651, gamma=0.03504594010245453\n",
      "Gradient Descent(242/4999): loss=0.2997249799376081, w0=-0.31466400015990525, gamma=0.035360461455050736\n",
      "Gradient Descent(243/4999): loss=0.2997247604558407, w0=-0.3146640001540053, gamma=18.648982045145036\n",
      "Gradient Descent(244/4999): loss=0.2997245398880308, w0=-0.31466399715242216, gamma=42.41411750969845\n",
      "Gradient Descent(245/4999): loss=0.2996104553645099, w0=-0.3146641176367734, gamma=0.08088609321999943\n",
      "Gradient Descent(246/4999): loss=0.3000159322232809, w0=-0.3146641081210366, gamma=0.07644880869557465\n",
      "Gradient Descent(247/4999): loss=0.29939328591060077, w0=-0.3146640998547885, gamma=0.08806043867994777\n",
      "Gradient Descent(248/4999): loss=0.2993809556652537, w0=-0.3146640910609284, gamma=0.12441802985996432\n",
      "Gradient Descent(249/4999): loss=0.299377957657983, w0=-0.3146640797304547, gamma=0.06114652102601115\n",
      "Gradient Descent(250/4999): loss=0.2993765647756765, w0=-0.31466407485479586, gamma=0.03555973691873471\n",
      "Gradient Descent(251/4999): loss=0.2993760380160166, w0=-0.31466407219273546, gamma=0.03544826475551369\n",
      "Gradient Descent(252/4999): loss=0.29937569429585387, w0=-0.31466406963338545, gamma=0.21761287825219808\n",
      "Gradient Descent(253/4999): loss=0.29937545329518755, w0=-0.31466405447877344, gamma=0.2860219557937007\n",
      "Gradient Descent(254/4999): loss=0.2993740185617287, w0=-0.3146640388946892, gamma=1.1201612951176336\n",
      "Gradient Descent(255/4999): loss=0.29937220635494893, w0=-0.3146639953186927, gamma=0.3456991673491707\n",
      "Gradient Descent(256/4999): loss=0.2993655671837832, w0=-0.3146639969346502, gamma=0.04853524886511776\n",
      "Gradient Descent(257/4999): loss=0.2993646982907423, w0=-0.31466399708309545, gamma=0.04216664386473002\n",
      "Gradient Descent(258/4999): loss=0.2993634477418811, w0=-0.31466399720580285, gamma=0.03636683063505244\n",
      "Gradient Descent(259/4999): loss=0.29936316189292306, w0=-0.31466399730717, gamma=0.0404311611394528\n",
      "Gradient Descent(260/4999): loss=0.2993629634318451, w0=-0.31466399741576745, gamma=0.1395575088389102\n",
      "Gradient Descent(261/4999): loss=0.29936274683487785, w0=-0.31466399777546117, gamma=0.29249359271980174\n",
      "Gradient Descent(262/4999): loss=0.29936200384335987, w0=-0.3146639984241224, gamma=0.3986510909243754\n",
      "Gradient Descent(263/4999): loss=0.299360461701834, w0=-0.31466399904961906, gamma=0.07602535385038427\n",
      "Gradient Descent(264/4999): loss=0.29935840228031546, w0=-0.3146639991213518, gamma=0.04494681420698582\n",
      "Gradient Descent(265/4999): loss=0.29935800147800634, w0=-0.3146639991605366, gamma=0.036143023810504944\n",
      "Gradient Descent(266/4999): loss=0.299357765497252, w0=-0.31466399919063, gamma=0.039537551696627316\n",
      "Gradient Descent(267/4999): loss=0.29935757982723027, w0=-0.3146639992223599, gamma=1.0093111635796417\n",
      "Gradient Descent(268/4999): loss=0.2993573774024164, w0=-0.3146640000003329, gamma=3.1170719717037234\n",
      "Gradient Descent(269/4999): loss=0.29935225313688846, w0=-0.3146639999779648, gamma=0.09210249652554621\n",
      "Gradient Descent(270/4999): loss=0.2993372860575981, w0=-0.3146639999793642, gamma=0.0490754346543942\n",
      "Gradient Descent(271/4999): loss=0.299336693795088, w0=-0.31466399998004124, gamma=0.03883434810494839\n",
      "Gradient Descent(272/4999): loss=0.2993362751899449, w0=-0.31466399998055067, gamma=0.035563277932189725\n",
      "Gradient Descent(273/4999): loss=0.299336070215751, w0=-0.3146639999809991, gamma=0.043416230823940985\n",
      "Gradient Descent(274/4999): loss=0.29933589880598754, w0=-0.31466399998152705, gamma=0.1263511826150551\n",
      "Gradient Descent(275/4999): loss=0.29933569011242384, w0=-0.3146639999829968, gamma=0.211514469479738\n",
      "Gradient Descent(276/4999): loss=0.29933508338366155, w0=-0.31466399998514644, gamma=0.38186821206449445\n",
      "Gradient Descent(277/4999): loss=0.29933406867359247, w0=-0.31466399998820654, gamma=0.11408384603989462\n",
      "Gradient Descent(278/4999): loss=0.2993322388359767, w0=-0.31466399998877165, gamma=0.05169101316061953\n",
      "Gradient Descent(279/4999): loss=0.29933169278594707, w0=-0.3146639999889985, gamma=0.04136962956104133\n",
      "Gradient Descent(280/4999): loss=0.2993314448807776, w0=-0.3146639999891707, gamma=0.03893092931081906\n",
      "Gradient Descent(281/4999): loss=0.29933124687077683, w0=-0.31466399998932604, gamma=0.1382609606580196\n",
      "Gradient Descent(282/4999): loss=0.2993310606077565, w0=-0.31466399998985617, gamma=4.55074476251976\n",
      "Gradient Descent(283/4999): loss=0.2993303992597469, w0=-0.3146640000048932, gamma=0.9234058450206568\n",
      "Gradient Descent(284/4999): loss=0.29930872834930694, w0=-0.3146639999940621, gamma=0.035874295176770106\n",
      "Gradient Descent(285/4999): loss=0.2993063847049946, w0=-0.31466399999402994, gamma=0.03526266329968516\n",
      "Gradient Descent(286/4999): loss=0.2993042287526022, w0=-0.3146639999939994, gamma=0.0553955371224148\n",
      "Gradient Descent(287/4999): loss=0.2993040482135436, w0=-0.31466399999395317, gamma=0.06555909600981367\n",
      "Gradient Descent(288/4999): loss=0.2993037845272501, w0=-0.3146639999939015, gamma=0.08206432451341635\n",
      "Gradient Descent(289/4999): loss=0.2993034766235087, w0=-0.31466399999384104, gamma=0.9396970209645851\n",
      "Gradient Descent(290/4999): loss=0.2993030914651984, w0=-0.31466399999320555, gamma=5.488847067820048\n",
      "Gradient Descent(291/4999): loss=0.29929868529282444, w0=-0.31466399999298567, gamma=0.047010809257391345\n",
      "Gradient Descent(292/4999): loss=0.29927351260264573, w0=-0.31466399999299427, gamma=0.035460816959225004\n",
      "Gradient Descent(293/4999): loss=0.2992729474932072, w0=-0.3146639999930005, gamma=0.03540080896240347\n",
      "Gradient Descent(294/4999): loss=0.2992727393932447, w0=-0.3146639999930065, gamma=0.08389233742769073\n",
      "Gradient Descent(295/4999): loss=0.29927257548014424, w0=-0.3146639999930202, gamma=0.1073861883263625\n",
      "Gradient Descent(296/4999): loss=0.29927218888536006, w0=-0.31466399999303624, gamma=0.18959152036880436\n",
      "Gradient Descent(297/4999): loss=0.29927169516228896, w0=-0.31466399999306155, gamma=0.2993892332018063\n",
      "Gradient Descent(298/4999): loss=0.29927082423357115, w0=-0.314663999993094, gamma=0.12527931562363098\n",
      "Gradient Descent(299/4999): loss=0.29926945006463024, w0=-0.31466399999310357, gamma=0.05841347761179115\n",
      "Gradient Descent(300/4999): loss=0.29926887551385817, w0=-0.31466399999310746, gamma=0.04878841971391411\n",
      "Gradient Descent(301/4999): loss=0.2992686074275031, w0=-0.3146639999931105, gamma=0.04314830170714072\n",
      "Gradient Descent(302/4999): loss=0.29926838373961445, w0=-0.3146639999931131, gamma=0.0869276371246281\n",
      "Gradient Descent(303/4999): loss=0.2992681859515822, w0=-0.3146639999931181, gamma=0.9940887942747663\n",
      "Gradient Descent(304/4999): loss=0.29926778755509703, w0=-0.3146639999931701, gamma=0.4749003928530177\n",
      "Gradient Descent(305/4999): loss=0.29926323621824613, w0=-0.3146639999931705, gamma=0.03561706589713806\n",
      "Gradient Descent(306/4999): loss=0.2992611079767302, w0=-0.3146639999931705, gamma=0.035035856558170195\n",
      "Gradient Descent(307/4999): loss=0.2992609048740043, w0=-0.3146639999931706, gamma=0.04837395503732171\n",
      "Gradient Descent(308/4999): loss=0.2992607450673622, w0=-0.31466399999317063, gamma=10.600564397941802\n",
      "Gradient Descent(309/4999): loss=0.29926052446452156, w0=-0.3146639999931791, gamma=82.18017169459563\n",
      "Gradient Descent(310/4999): loss=0.29921257995278416, w0=-0.31466399999310957, gamma=0.077458350054226\n",
      "Gradient Descent(311/4999): loss=0.2991950805458081, w0=-0.3146639999931222, gamma=0.07153987338050304\n",
      "Gradient Descent(312/4999): loss=0.2988760166339517, w0=-0.3146639999931292, gamma=0.06178228614789464\n",
      "Gradient Descent(313/4999): loss=0.2988720404215655, w0=-0.31466399999313516, gamma=0.0575012225440947\n",
      "Gradient Descent(314/4999): loss=0.29887139794581613, w0=-0.3146639999931404, gamma=0.055666041427973406\n",
      "Gradient Descent(315/4999): loss=0.2988710767487667, w0=-0.31466399999314515, gamma=0.057386225715033067\n",
      "Gradient Descent(316/4999): loss=0.2988708310596079, w0=-0.3146639999931498, gamma=0.06342249141483139\n",
      "Gradient Descent(317/4999): loss=0.2988706072093503, w0=-0.31466399999315464, gamma=0.06587135950449305\n",
      "Gradient Descent(318/4999): loss=0.298870375109882, w0=-0.31466399999315936, gamma=0.05598651674981496\n",
      "Gradient Descent(319/4999): loss=0.2988701413495929, w0=-0.3146639999931631, gamma=0.046605057483346486\n",
      "Gradient Descent(320/4999): loss=0.2988699450377072, w0=-0.314663999993166, gamma=0.05022543477758016\n",
      "Gradient Descent(321/4999): loss=0.29886978242417145, w0=-0.3146639999931691, gamma=0.09200688921724078\n",
      "Gradient Descent(322/4999): loss=0.2988696076220869, w0=-0.31466399999317435, gamma=0.14254114976186283\n",
      "Gradient Descent(323/4999): loss=0.2988692878737754, w0=-0.31466399999318173, gamma=0.07045735775138336\n",
      "Gradient Descent(324/4999): loss=0.2988687930955493, w0=-0.31466399999318484, gamma=0.03739121462962546\n",
      "Gradient Descent(325/4999): loss=0.29886854864575746, w0=-0.3146639999931864, gamma=0.03726158684218623\n",
      "Gradient Descent(326/4999): loss=0.29886841889101334, w0=-0.3146639999931879, gamma=0.4851665762289414\n",
      "Gradient Descent(327/4999): loss=0.29886828965348017, w0=-0.3146639999932063, gamma=6.596554596889865\n",
      "Gradient Descent(328/4999): loss=0.2988666076919304, w0=-0.31466399999333317, gamma=0.6179859715995494\n",
      "Gradient Descent(329/4999): loss=0.2988438754010779, w0=-0.31466399999326405, gamma=0.035387682739219176\n",
      "Gradient Descent(330/4999): loss=0.2988433236090431, w0=-0.3146639999932625, gamma=0.03503978260703853\n",
      "Gradient Descent(331/4999): loss=0.2988416435528278, w0=-0.31466399999326106, gamma=0.07117413596233181\n",
      "Gradient Descent(332/4999): loss=0.2988415214518999, w0=-0.3146639999932583, gamma=0.17744995715667186\n",
      "Gradient Descent(333/4999): loss=0.29884127596513915, w0=-0.31466399999325184, gamma=0.18286827028844785\n",
      "Gradient Descent(334/4999): loss=0.2988406696082147, w0=-0.31466399999324635, gamma=7.455692855942219\n",
      "Gradient Descent(335/4999): loss=0.2988400466210524, w0=-0.31466399999306394, gamma=8.903354528455026\n",
      "Gradient Descent(336/4999): loss=0.29881478387205984, w0=-0.31466399999449074, gamma=0.036439109528816876\n",
      "Gradient Descent(337/4999): loss=0.2987924094904966, w0=-0.3146639999944444, gamma=0.03503322706354054\n",
      "Gradient Descent(338/4999): loss=0.2987850584649424, w0=-0.31466399999440126, gamma=0.035501730842704825\n",
      "Gradient Descent(339/4999): loss=0.298784927912929, w0=-0.3146639999943591, gamma=0.17982280649756788\n",
      "Gradient Descent(340/4999): loss=0.298784808099383, w0=-0.31466399999415323, gamma=0.20253857990851823\n",
      "Gradient Descent(341/4999): loss=0.2987842050635532, w0=-0.3146639999939631, gamma=0.3487357100969717\n",
      "Gradient Descent(342/4999): loss=0.2987835295018096, w0=-0.31466399999370215, gamma=0.33426373345321087\n",
      "Gradient Descent(343/4999): loss=0.29878237024503, w0=-0.3146639999935395, gamma=0.13645650537690288\n",
      "Gradient Descent(344/4999): loss=0.29878126416077805, w0=-0.3146639999934953, gamma=0.10729177063155952\n",
      "Gradient Descent(345/4999): loss=0.29878081224252206, w0=-0.31466399999346534, gamma=0.12299846361619823\n",
      "Gradient Descent(346/4999): loss=0.2987804579979315, w0=-0.3146639999934347, gamma=0.1576629702123567\n",
      "Gradient Descent(347/4999): loss=0.2987800523674538, w0=-0.3146639999934003, gamma=0.09433270224426327\n",
      "Gradient Descent(348/4999): loss=0.29877953290328974, w0=-0.31466399999338296, gamma=0.04926638692811724\n",
      "Gradient Descent(349/4999): loss=0.2987792223501216, w0=-0.31466399999337474, gamma=0.04056214130974037\n",
      "Gradient Descent(350/4999): loss=0.2987790601606859, w0=-0.3146639999933683, gamma=0.05706629995701024\n",
      "Gradient Descent(351/4999): loss=0.2987789267073493, w0=-0.31466399999335964, gamma=0.7408018648536298\n",
      "Gradient Descent(352/4999): loss=0.2987787390095332, w0=-0.3146639999932538, gamma=1.428252754060951\n",
      "Gradient Descent(353/4999): loss=0.2987763052849796, w0=-0.31466399999320166, gamma=0.038666106320979825\n",
      "Gradient Descent(354/4999): loss=0.2987716836980174, w0=-0.31466399999320227, gamma=0.03503384619099663\n",
      "Gradient Descent(355/4999): loss=0.2987715048671592, w0=-0.3146639999932028, gamma=0.035172158542118734\n",
      "Gradient Descent(356/4999): loss=0.29877138999500197, w0=-0.3146639999932034, gamma=5.254099448390371\n",
      "Gradient Descent(357/4999): loss=0.29877127525382907, w0=-0.31466399999328015, gamma=15.724867040414763\n",
      "Gradient Descent(358/4999): loss=0.29875418622707867, w0=-0.31466399999230565, gamma=0.05991852758709683\n",
      "Gradient Descent(359/4999): loss=0.2987077366011791, w0=-0.3146639999923597, gamma=0.05365031211738399\n",
      "Gradient Descent(360/4999): loss=0.2987037355454085, w0=-0.31466399999240524, gamma=0.0538972924251563\n",
      "Gradient Descent(361/4999): loss=0.29870351132596473, w0=-0.31466399999244854, gamma=0.1232332857113579\n",
      "Gradient Descent(362/4999): loss=0.2987033405696905, w0=-0.3146639999925422, gamma=0.188867484555202\n",
      "Gradient Descent(363/4999): loss=0.2987029510903001, w0=-0.31466399999266803, gamma=1.0799288458878933\n",
      "Gradient Descent(364/4999): loss=0.2987023551279803, w0=-0.31466399999325206, gamma=0.373384584313187\n",
      "Gradient Descent(365/4999): loss=0.2986989600241382, w0=-0.3146639999932364, gamma=0.03673795294991826\n",
      "Gradient Descent(366/4999): loss=0.298697844453922, w0=-0.3146639999932354, gamma=0.03518470418960871\n",
      "Gradient Descent(367/4999): loss=0.2986976776457928, w0=-0.3146639999932345, gamma=0.05118580427694223\n",
      "Gradient Descent(368/4999): loss=0.29869756709213113, w0=-0.3146639999932333, gamma=0.09895341025441388\n",
      "Gradient Descent(369/4999): loss=0.29869740695049113, w0=-0.314663999993231, gamma=0.12854557766008254\n",
      "Gradient Descent(370/4999): loss=0.2986970978557471, w0=-0.31466399999322836, gamma=1.230083115997159\n",
      "Gradient Descent(371/4999): loss=0.2986966965564902, w0=-0.31466399999320616, gamma=1.5599349735628203\n",
      "Gradient Descent(372/4999): loss=0.2986928610602941, w0=-0.3146639999932132, gamma=0.053890234380714414\n",
      "Gradient Descent(373/4999): loss=0.2986880738042217, w0=-0.31466399999321304, gamma=0.04020935707379891\n",
      "Gradient Descent(374/4999): loss=0.2986878549249603, w0=-0.31466399999321293, gamma=0.03638279302323347\n",
      "Gradient Descent(375/4999): loss=0.29868772110278946, w0=-0.3146639999932128, gamma=0.044926456967729206\n",
      "Gradient Descent(376/4999): loss=0.2986876077688759, w0=-0.3146639999932127, gamma=0.07714806429748479\n",
      "Gradient Descent(377/4999): loss=0.2986874683782548, w0=-0.31466399999321254, gamma=0.10401160389747145\n",
      "Gradient Descent(378/4999): loss=0.2986872292038288, w0=-0.3146639999932123, gamma=1.508362623666949\n",
      "Gradient Descent(379/4999): loss=0.2986869068239008, w0=-0.31466399999320954, gamma=0.600291886501046\n",
      "Gradient Descent(380/4999): loss=0.29868223586745174, w0=-0.3146639999932098, gamma=0.035488501201797744\n",
      "Gradient Descent(381/4999): loss=0.29868043049399357, w0=-0.3146639999932098, gamma=0.03505567581197716\n",
      "Gradient Descent(382/4999): loss=0.2986802715723957, w0=-0.3146639999932098, gamma=0.0572507721395072\n",
      "Gradient Descent(383/4999): loss=0.29868016330165964, w0=-0.3146639999932098, gamma=0.10444527243587848\n",
      "Gradient Descent(384/4999): loss=0.29867998657301126, w0=-0.3146639999932098, gamma=0.6416266017947161\n",
      "Gradient Descent(385/4999): loss=0.2986796642340295, w0=-0.31466399999320976, gamma=2.656145811562069\n",
      "Gradient Descent(386/4999): loss=0.2986776849463054, w0=-0.3146639999932089, gamma=0.13018654440206284\n",
      "Gradient Descent(387/4999): loss=0.2986695209044848, w0=-0.3146639999932089, gamma=0.0726219305898654\n",
      "Gradient Descent(388/4999): loss=0.2986691166857646, w0=-0.3146639999932088, gamma=0.06970498169392046\n",
      "Gradient Descent(389/4999): loss=0.2986688863409863, w0=-0.3146639999932088, gamma=0.03614585912121172\n",
      "Gradient Descent(390/4999): loss=0.2986686725863001, w0=-0.3146639999932088, gamma=0.0359416829382084\n",
      "Gradient Descent(391/4999): loss=0.29866856167816985, w0=-0.3146639999932088, gamma=0.5111454894669353\n",
      "Gradient Descent(392/4999): loss=0.2986684514897503, w0=-0.3146639999932086, gamma=1.496134360741117\n",
      "Gradient Descent(393/4999): loss=0.2986668849608901, w0=-0.31466399999320793, gamma=0.939318820055892\n",
      "Gradient Descent(394/4999): loss=0.29866230648599423, w0=-0.31466399999320765, gamma=0.05766987250660161\n",
      "Gradient Descent(395/4999): loss=0.2986594561402769, w0=-0.31466399999320765, gamma=0.035428735600375336\n",
      "Gradient Descent(396/4999): loss=0.29865927041978146, w0=-0.31466399999320765, gamma=0.03547028765818276\n",
      "Gradient Descent(397/4999): loss=0.2986591563423788, w0=-0.31466399999320765, gamma=0.21655114946045542\n",
      "Gradient Descent(398/4999): loss=0.2986590475580795, w0=-0.31466399999320754, gamma=0.2375212266120511\n",
      "Gradient Descent(399/4999): loss=0.29865838583119236, w0=-0.31466399999320743, gamma=0.2701512035779207\n",
      "Gradient Descent(400/4999): loss=0.2986576620686561, w0=-0.3146639999932073, gamma=0.12254901370073461\n",
      "Gradient Descent(401/4999): loss=0.2986568393007241, w0=-0.31466399999320727, gamma=0.06768196900807641\n",
      "Gradient Descent(402/4999): loss=0.2986564661574893, w0=-0.3146639999932072, gamma=0.04020150670406953\n",
      "Gradient Descent(403/4999): loss=0.2986562601084121, w0=-0.3146639999932072, gamma=0.037669927001993356\n",
      "Gradient Descent(404/4999): loss=0.2986561377260019, w0=-0.3146639999932072, gamma=0.15065400892369604\n",
      "Gradient Descent(405/4999): loss=0.29865602307891, w0=-0.31466399999320716, gamma=9.824148402901075\n",
      "Gradient Descent(406/4999): loss=0.2986555646246185, w0=-0.31466399999320305, gamma=1.7863370344784903\n",
      "Gradient Descent(407/4999): loss=0.29862580751814116, w0=-0.31466399999320244, gamma=0.035728411316595106\n",
      "Gradient Descent(408/4999): loss=0.2986266236929761, w0=-0.31466399999320227, gamma=0.03516344726525535\n",
      "Gradient Descent(409/4999): loss=0.2986204221869407, w0=-0.31466399999320227, gamma=0.0646029184609457\n",
      "Gradient Descent(410/4999): loss=0.29862027409113606, w0=-0.31466399999320227, gamma=0.07891017505194362\n",
      "Gradient Descent(411/4999): loss=0.29862006402562036, w0=-0.3146639999932022, gamma=0.0797054535543402\n",
      "Gradient Descent(412/4999): loss=0.29861982820364574, w0=-0.31466399999320216, gamma=0.1680189922086134\n",
      "Gradient Descent(413/4999): loss=0.29861959068927785, w0=-0.31466399999320205, gamma=0.38324318183853395\n",
      "Gradient Descent(414/4999): loss=0.2986190901009425, w0=-0.3146639999932019, gamma=0.08246161081449908\n",
      "Gradient Descent(415/4999): loss=0.29861794885606663, w0=-0.3146639999932018, gamma=0.038705153144514066\n",
      "Gradient Descent(416/4999): loss=0.2986177034212083, w0=-0.3146639999932018, gamma=0.03594566421592813\n",
      "Gradient Descent(417/4999): loss=0.29861758805007804, w0=-0.3146639999932018, gamma=0.06411755443677973\n",
      "Gradient Descent(418/4999): loss=0.2986174810683226, w0=-0.31466399999320177, gamma=1.294154733551455\n",
      "Gradient Descent(419/4999): loss=0.29861729026029804, w0=-0.31466399999320116, gamma=21.224243187956464\n",
      "Gradient Descent(420/4999): loss=0.2986134414144329, w0=-0.31466399999319206, gamma=0.0716452074082206\n",
      "Gradient Descent(421/4999): loss=0.29855186049690124, w0=-0.31466399999319206, gamma=0.039892853395745276\n",
      "Gradient Descent(422/4999): loss=0.2985512588472945, w0=-0.31466399999319195, gamma=0.035688073668901095\n",
      "Gradient Descent(423/4999): loss=0.2985506825432246, w0=-0.3146639999931919, gamma=0.0370977094895595\n",
      "Gradient Descent(424/4999): loss=0.2985505690531715, w0=-0.31466399999319183, gamma=0.05360227051307013\n",
      "Gradient Descent(425/4999): loss=0.29855046242355177, w0=-0.3146639999931918, gamma=0.05820909029777974\n",
      "Gradient Descent(426/4999): loss=0.298550308858316, w0=-0.3146639999931917, gamma=10.36800013319659\n",
      "Gradient Descent(427/4999): loss=0.2985501421512011, w0=-0.3146639999931822, gamma=24.526752389515472\n",
      "Gradient Descent(428/4999): loss=0.29852057309070823, w0=-0.3146639999932568, gamma=0.03836970793782493\n",
      "Gradient Descent(429/4999): loss=0.29850926660714483, w0=-0.31466399999325406, gamma=0.0352099530760657\n",
      "Gradient Descent(430/4999): loss=0.29845563770169564, w0=-0.31466399999325123, gamma=0.04339250142995889\n",
      "Gradient Descent(431/4999): loss=0.29845387034210275, w0=-0.31466399999324784, gamma=0.1339654692742386\n",
      "Gradient Descent(432/4999): loss=0.29845292294946135, w0=-0.31466399999323785, gamma=0.15019302128383696\n",
      "Gradient Descent(433/4999): loss=0.29845160374687446, w0=-0.314663999993228, gamma=0.1657240239001836\n",
      "Gradient Descent(434/4999): loss=0.29845105329027377, w0=-0.31466399999321865, gamma=0.11917492930738866\n",
      "Gradient Descent(435/4999): loss=0.29845058693764054, w0=-0.314663999993213, gamma=0.03932347427268941\n",
      "Gradient Descent(436/4999): loss=0.2984502656268384, w0=-0.3146639999932113, gamma=0.03510950926836349\n",
      "Gradient Descent(437/4999): loss=0.2984501541893246, w0=-0.31466399999320993, gamma=0.03803970318914754\n",
      "Gradient Descent(438/4999): loss=0.2984500588485141, w0=-0.31466399999320843, gamma=0.25209475825999933\n",
      "Gradient Descent(439/4999): loss=0.29844995565595434, w0=-0.31466399999319905, gamma=0.302652817884207\n",
      "Gradient Descent(440/4999): loss=0.29844927209488675, w0=-0.31466399999319056, gamma=0.34146251673760675\n",
      "Gradient Descent(441/4999): loss=0.2984484518883435, w0=-0.3146639999931838, gamma=0.1184651358625039\n",
      "Gradient Descent(442/4999): loss=0.298447527011671, w0=-0.31466399999318223, gamma=0.06367912423402045\n",
      "Gradient Descent(443/4999): loss=0.2984472061848713, w0=-0.3146639999931815, gamma=0.05358757781032922\n",
      "Gradient Descent(444/4999): loss=0.2984470337053111, w0=-0.3146639999931809, gamma=0.057941849793672326\n",
      "Gradient Descent(445/4999): loss=0.29844688863067875, w0=-0.3146639999931803, gamma=0.12222238805280722\n",
      "Gradient Descent(446/4999): loss=0.29844673178407416, w0=-0.31466399999317907, gamma=0.23872715284829812\n",
      "Gradient Descent(447/4999): loss=0.2984464009684965, w0=-0.31466399999317696, gamma=0.06795694919042856\n",
      "Gradient Descent(448/4999): loss=0.29844575498982584, w0=-0.3146639999931765, gamma=0.03600486968523883\n",
      "Gradient Descent(449/4999): loss=0.2984455711374344, w0=-0.3146639999931763, gamma=0.036133001543060414\n",
      "Gradient Descent(450/4999): loss=0.2984454737026417, w0=-0.31466399999317607, gamma=1.4577988060954217\n",
      "Gradient Descent(451/4999): loss=0.29844537596334236, w0=-0.31466399999316724, gamma=345.2583271430615\n",
      "Gradient Descent(452/4999): loss=0.29844143481252017, w0=-0.3146639999938286, gamma=0.901989100584283\n",
      "Gradient Descent(453/4999): loss=0.2976268524984662, w0=-0.31466399999304023, gamma=0.0350479505951679\n",
      "Gradient Descent(454/4999): loss=0.3004029525216653, w0=-0.31466399999303407, gamma=0.035033525135214724\n",
      "Gradient Descent(455/4999): loss=0.2976212315674878, w0=-0.31466399999303113, gamma=0.10132579779632442\n",
      "Gradient Descent(456/4999): loss=0.2976207626435014, w0=-0.31466399999302297, gamma=0.10274546287468668\n",
      "Gradient Descent(457/4999): loss=0.2976202629405587, w0=-0.3146639999930154, gamma=0.07646499834805669\n",
      "Gradient Descent(458/4999): loss=0.29762006938809354, w0=-0.31466399999301037, gamma=0.058880077730368366\n",
      "Gradient Descent(459/4999): loss=0.2976199253973164, w0=-0.31466399999300676, gamma=0.07664258224222537\n",
      "Gradient Descent(460/4999): loss=0.29761981453161346, w0=-0.3146639999930024, gamma=1.5019518805133616\n",
      "Gradient Descent(461/4999): loss=0.297619670228795, w0=-0.3146639999929227, gamma=4.913679120088429\n",
      "Gradient Descent(462/4999): loss=0.29761684306107905, w0=-0.3146639999930436, gamma=0.07645048626681483\n",
      "Gradient Descent(463/4999): loss=0.2976076306517304, w0=-0.31466399999303574, gamma=0.052680710392009965\n",
      "Gradient Descent(464/4999): loss=0.2976074683437094, w0=-0.3146639999930307, gamma=0.04688931420997486\n",
      "Gradient Descent(465/4999): loss=0.2976073645745214, w0=-0.31466399999302647, gamma=0.039750466027080045\n",
      "Gradient Descent(466/4999): loss=0.297607276033303, w0=-0.314663999993023, gamma=0.05742999931913257\n",
      "Gradient Descent(467/4999): loss=0.2976072012196195, w0=-0.31466399999301825, gamma=0.18804853186240597\n",
      "Gradient Descent(468/4999): loss=0.2976070932767198, w0=-0.31466399999300354, gamma=0.200255830893937\n",
      "Gradient Descent(469/4999): loss=0.2976067402683146, w0=-0.3146639999929908, gamma=0.04030112246173876\n",
      "Gradient Descent(470/4999): loss=0.29760636467511914, w0=-0.3146639999929887, gamma=0.03519907969799681\n",
      "Gradient Descent(471/4999): loss=0.29760628902142855, w0=-0.314663999992987, gamma=0.0423996446106566\n",
      "Gradient Descent(472/4999): loss=0.29760622299912126, w0=-0.314663999992985, gamma=44.26633237733676\n",
      "Gradient Descent(473/4999): loss=0.2976061434740927, w0=-0.31466399999096845, gamma=72.92263051160865\n",
      "Gradient Descent(474/4999): loss=0.2975235969172771, w0=-0.314664000130272, gamma=0.16289475929733\n",
      "Gradient Descent(475/4999): loss=0.2975274449074826, w0=-0.3146640001078754, gamma=0.04383191698565852\n",
      "Gradient Descent(476/4999): loss=0.2975340118099834, w0=-0.3146640001028303, gamma=0.03510122218073748\n",
      "Gradient Descent(477/4999): loss=0.29740633153511153, w0=-0.3146640000989681, gamma=0.03556687734788201\n",
      "Gradient Descent(478/4999): loss=0.297396107878152, w0=-0.31466400009519185, gamma=0.2228779093823503\n",
      "Gradient Descent(479/4999): loss=0.2973947225723644, w0=-0.31466400007237, gamma=0.22812426516908982\n",
      "Gradient Descent(480/4999): loss=0.29739095930496096, w0=-0.3146640000542176, gamma=0.06670542132257581\n",
      "Gradient Descent(481/4999): loss=0.29739044144237253, w0=-0.3146640000501204, gamma=0.05468527371458489\n",
      "Gradient Descent(482/4999): loss=0.2973902684647035, w0=-0.3146640000469856, gamma=0.06416162230736748\n",
      "Gradient Descent(483/4999): loss=0.29739014483740117, w0=-0.31466400004350864, gamma=1.0867706387911868\n",
      "Gradient Descent(484/4999): loss=0.29739000273404487, w0=-0.31466399998839456, gamma=1.361213679558198\n",
      "Gradient Descent(485/4999): loss=0.29738771821507715, w0=-0.31466399999437694, gamma=0.03635883676572294\n",
      "Gradient Descent(486/4999): loss=0.2973867147240471, w0=-0.314663999994319, gamma=0.03505896284092092\n",
      "Gradient Descent(487/4999): loss=0.2973851170426435, w0=-0.3146639999942652, gamma=0.03976104602242136\n",
      "Gradient Descent(488/4999): loss=0.2973850491055888, w0=-0.3146639999942063, gamma=0.11659793640942777\n",
      "Gradient Descent(489/4999): loss=0.2973849761842439, w0=-0.31466399999404054, gamma=0.1480619970623795\n",
      "Gradient Descent(490/4999): loss=0.297384765297341, w0=-0.3146639999938546, gamma=0.17380517196964093\n",
      "Gradient Descent(491/4999): loss=0.2973844997723106, w0=-0.3146639999936685, gamma=0.09608105654648609\n",
      "Gradient Descent(492/4999): loss=0.29738418884996376, w0=-0.3146639999935835, gamma=0.06147011345536588\n",
      "Gradient Descent(493/4999): loss=0.2973840171551277, w0=-0.31466399999353434, gamma=0.06468760006187377\n",
      "Gradient Descent(494/4999): loss=0.29738390736408177, w0=-0.31466399999348577, gamma=0.4313361809467795\n",
      "Gradient Descent(495/4999): loss=0.2973837919334574, w0=-0.31466399999318284, gamma=1.5804311523481869\n",
      "Gradient Descent(496/4999): loss=0.2973830231563174, w0=-0.31466399999255057, gamma=0.06302980505087806\n",
      "Gradient Descent(497/4999): loss=0.2973802265130688, w0=-0.31466399999256506, gamma=0.0363654565204645\n",
      "Gradient Descent(498/4999): loss=0.29738011101900097, w0=-0.3146639999925729, gamma=0.03510645569082962\n",
      "Gradient Descent(499/4999): loss=0.2973800425562939, w0=-0.31466399999258016, gamma=0.04207885372803969\n",
      "Gradient Descent(500/4999): loss=0.2973799804981218, w0=-0.3146639999925886, gamma=0.17378207933296158\n",
      "Gradient Descent(501/4999): loss=0.2973799061307759, w0=-0.3146639999926219, gamma=1.702194732183155\n",
      "Gradient Descent(502/4999): loss=0.297379599031216, w0=-0.3146639999928909, gamma=0.49253306113321377\n",
      "Gradient Descent(503/4999): loss=0.29737659306644576, w0=-0.31466399999283523, gamma=0.053911653813951715\n",
      "Gradient Descent(504/4999): loss=0.2973757341785694, w0=-0.3146639999928321, gamma=0.05360030716745448\n",
      "Gradient Descent(505/4999): loss=0.2973756290567526, w0=-0.3146639999928292, gamma=0.043286764036701134\n",
      "Gradient Descent(506/4999): loss=0.2973755345308528, w0=-0.3146639999928269, gamma=0.05471625120319858\n",
      "Gradient Descent(507/4999): loss=0.29737545819718186, w0=-0.3146639999928242, gamma=0.34514826142010413\n",
      "Gradient Descent(508/4999): loss=0.29737536171147283, w0=-0.3146639999928079, gamma=1.0205760693467603\n",
      "Gradient Descent(509/4999): loss=0.2973747531260154, w0=-0.3146639999927761, gamma=0.04845187123540975\n",
      "Gradient Descent(510/4999): loss=0.29737295466992203, w0=-0.3146639999927761, gamma=0.035079628281097716\n",
      "Gradient Descent(511/4999): loss=0.297372868785022, w0=-0.3146639999927761, gamma=0.03517731287739284\n",
      "Gradient Descent(512/4999): loss=0.297372806885762, w0=-0.3146639999927761, gamma=0.47706898614368426\n",
      "Gradient Descent(513/4999): loss=0.2973727449025725, w0=-0.31466399999277583, gamma=1.1749491023964262\n",
      "Gradient Descent(514/4999): loss=0.29737190436164057, w0=-0.3146639999927749, gamma=0.141724784756603\n",
      "Gradient Descent(515/4999): loss=0.2973698352573057, w0=-0.3146639999927747, gamma=0.08359945876360754\n",
      "Gradient Descent(516/4999): loss=0.297369585532792, w0=-0.3146639999927746, gamma=0.07929586845841961\n",
      "Gradient Descent(517/4999): loss=0.29736943818057654, w0=-0.3146639999927745, gamma=0.12778522866817682\n",
      "Gradient Descent(518/4999): loss=0.29736929858501265, w0=-0.31466399999277433, gamma=0.5927873934725791\n",
      "Gradient Descent(519/4999): loss=0.29736907363947956, w0=-0.3146639999927737, gamma=0.12767060892238405\n",
      "Gradient Descent(520/4999): loss=0.2973680302719545, w0=-0.31466399999277356, gamma=0.03544566350734164\n",
      "Gradient Descent(521/4999): loss=0.29736780574338345, w0=-0.3146639999927735, gamma=0.03508765796517473\n",
      "Gradient Descent(522/4999): loss=0.2973677432053813, w0=-0.31466399999277345, gamma=0.3561009441169607\n",
      "Gradient Descent(523/4999): loss=0.2973676814633538, w0=-0.3146639999927731, gamma=7.443336451194948\n",
      "Gradient Descent(524/4999): loss=0.2973670548834937, w0=-0.31466399999276495, gamma=0.5750959144580282\n",
      "Gradient Descent(525/4999): loss=0.29735397209158204, w0=-0.31466399999276434, gamma=0.07270609478233235\n",
      "Gradient Descent(526/4999): loss=0.2973530367479799, w0=-0.31466399999276434, gamma=0.07239515317966434\n",
      "Gradient Descent(527/4999): loss=0.29735283480623836, w0=-0.3146639999927643, gamma=0.03691012157107704\n",
      "Gradient Descent(528/4999): loss=0.29735270787472784, w0=-0.31466399999276423, gamma=0.03528657020098864\n",
      "Gradient Descent(529/4999): loss=0.2973526431042641, w0=-0.3146639999927642, gamma=0.08181100334311465\n",
      "Gradient Descent(530/4999): loss=0.297352581249951, w0=-0.31466399999276407, gamma=0.5044768058472947\n",
      "Gradient Descent(531/4999): loss=0.2973524378469015, w0=-0.31466399999276357, gamma=7.819655635622303\n",
      "Gradient Descent(532/4999): loss=0.29735155364845844, w0=-0.31466399999275496, gamma=0.3550282612821688\n",
      "Gradient Descent(533/4999): loss=0.29733786547383156, w0=-0.31466399999275463, gamma=0.05622983779003276\n",
      "Gradient Descent(534/4999): loss=0.29733729765139966, w0=-0.3146639999927546, gamma=0.04375186735189548\n",
      "Gradient Descent(535/4999): loss=0.2973371514843281, w0=-0.3146639999927545, gamma=0.03621146137463627\n",
      "Gradient Descent(536/4999): loss=0.297337068896489, w0=-0.31466399999275446, gamma=0.04077795454510917\n",
      "Gradient Descent(537/4999): loss=0.29733700494205895, w0=-0.3146639999927544, gamma=0.10137076828942086\n",
      "Gradient Descent(538/4999): loss=0.29733693355323393, w0=-0.3146639999927543, gamma=0.10451743803424984\n",
      "Gradient Descent(539/4999): loss=0.2973367564604693, w0=-0.3146639999927542, gamma=0.26377928025295216\n",
      "Gradient Descent(540/4999): loss=0.2973365739924754, w0=-0.3146639999927539, gamma=0.09699484475460567\n",
      "Gradient Descent(541/4999): loss=0.2973361135180178, w0=-0.3146639999927538, gamma=0.036562593153427685\n",
      "Gradient Descent(542/4999): loss=0.2973359442226908, w0=-0.31466399999275374, gamma=0.035511118487487545\n",
      "Gradient Descent(543/4999): loss=0.2973358803860692, w0=-0.3146639999927537, gamma=0.27134255367309507\n",
      "Gradient Descent(544/4999): loss=0.2973358184035566, w0=-0.3146639999927534, gamma=43.09810357031205\n",
      "Gradient Descent(545/4999): loss=0.2973353448116735, w0=-0.31466399999271355, gamma=2.9875263040112303\n",
      "Gradient Descent(546/4999): loss=0.2972604955349547, w0=-0.31466399999269284, gamma=0.05129771585614303\n",
      "Gradient Descent(547/4999): loss=0.2972751910911061, w0=-0.3146639999926935, gamma=0.0468984367424044\n",
      "Gradient Descent(548/4999): loss=0.2972560957473815, w0=-0.3146639999926939, gamma=0.03603715079596743\n",
      "Gradient Descent(549/4999): loss=0.29725528764911435, w0=-0.3146639999926943, gamma=0.03533412718845617\n",
      "Gradient Descent(550/4999): loss=0.2972551451501533, w0=-0.3146639999926946, gamma=0.05078755058545631\n",
      "Gradient Descent(551/4999): loss=0.2972550841102499, w0=-0.31466399999269506, gamma=0.05801110228714153\n",
      "Gradient Descent(552/4999): loss=0.29725499716443193, w0=-0.31466399999269556, gamma=0.08553836889992171\n",
      "Gradient Descent(553/4999): loss=0.29725489796248733, w0=-0.31466399999269623, gamma=3.076971838792855\n",
      "Gradient Descent(554/4999): loss=0.29725475169402554, w0=-0.31466399999271816, gamma=4.739318977645875\n",
      "Gradient Descent(555/4999): loss=0.2972494921137104, w0=-0.31466399999263284, gamma=0.04005871315512755\n",
      "Gradient Descent(556/4999): loss=0.29724153343885074, w0=-0.31466399999263533, gamma=0.035141131669734156\n",
      "Gradient Descent(557/4999): loss=0.2972413466541571, w0=-0.31466399999263744, gamma=0.037822207780955436\n",
      "Gradient Descent(558/4999): loss=0.297241281676272, w0=-0.31466399999263966, gamma=0.2167598941052803\n",
      "Gradient Descent(559/4999): loss=0.297241215175854, w0=-0.31466399999265177, gamma=0.2269560932162965\n",
      "Gradient Descent(560/4999): loss=0.29724084111648436, w0=-0.3146639999926616, gamma=0.0779331913836574\n",
      "Gradient Descent(561/4999): loss=0.29724045443267977, w0=-0.3146639999926642, gamma=0.05897344530224483\n",
      "Gradient Descent(562/4999): loss=0.297240321596458, w0=-0.31466399999266603, gamma=0.057323582167748174\n",
      "Gradient Descent(563/4999): loss=0.29724022111422965, w0=-0.3146639999926677, gamma=0.11716389928323925\n",
      "Gradient Descent(564/4999): loss=0.2972401234529424, w0=-0.31466399999267086, gamma=0.2619116060501986\n",
      "Gradient Descent(565/4999): loss=0.2972399238493297, w0=-0.3146639999926771, gamma=0.07715925660392167\n",
      "Gradient Descent(566/4999): loss=0.29723947768605613, w0=-0.3146639999926784, gamma=0.0360398249689361\n",
      "Gradient Descent(567/4999): loss=0.29723934625950227, w0=-0.31466399999267897, gamma=0.035726804128779016\n",
      "Gradient Descent(568/4999): loss=0.2972392848612167, w0=-0.3146639999926795, gamma=0.8708598570527885\n",
      "Gradient Descent(569/4999): loss=0.2972392240082547, w0=-0.3146639999926924, gamma=654.3002990705562\n",
      "Gradient Descent(570/4999): loss=0.2972377408397399, w0=-0.31466399999332123, gamma=2.756700164049057\n",
      "Gradient Descent(571/4999): loss=0.29620248226434365, w0=-0.31466399998982264, gamma=0.037608829058291486\n",
      "Gradient Descent(572/4999): loss=0.30162602061317045, w0=-0.3146639999899021, gamma=0.03617909985188925\n",
      "Gradient Descent(573/4999): loss=0.2963202772739729, w0=-0.31466399998998, gamma=0.045003821296935836\n",
      "Gradient Descent(574/4999): loss=0.2962105000650077, w0=-0.31466399999007316, gamma=0.05558999123686532\n",
      "Gradient Descent(575/4999): loss=0.29619884901066085, w0=-0.31466399999018313, gamma=0.07081345954930232\n",
      "Gradient Descent(576/4999): loss=0.29619772911577485, w0=-0.3146639999903154, gamma=0.09906522110701504\n",
      "Gradient Descent(577/4999): loss=0.2961974323242125, w0=-0.3146639999904874, gamma=0.07577544647137753\n",
      "Gradient Descent(578/4999): loss=0.29619727553263175, w0=-0.3146639999906059, gamma=0.03614817813657793\n",
      "Gradient Descent(579/4999): loss=0.29619717592318656, w0=-0.31466399999065814, gamma=0.035296985747374\n",
      "Gradient Descent(580/4999): loss=0.29619712513598934, w0=-0.3146639999907073, gamma=0.11091415288849993\n",
      "Gradient Descent(581/4999): loss=0.29619707891290853, w0=-0.3146639999908564, gamma=0.18126442466455317\n",
      "Gradient Descent(582/4999): loss=0.2961969339682631, w0=-0.31466399999107303, gamma=0.19342740041536824\n",
      "Gradient Descent(583/4999): loss=0.29619669751451233, w0=-0.31466399999126227, gamma=0.10600815852604657\n",
      "Gradient Descent(584/4999): loss=0.29619644527145944, w0=-0.3146639999913459, gamma=0.053233063175829196\n",
      "Gradient Descent(585/4999): loss=0.2961963070390134, w0=-0.31466399999138345, gamma=0.04375533575070245\n",
      "Gradient Descent(586/4999): loss=0.29619623762318137, w0=-0.31466399999141265, gamma=0.05055670988798267\n",
      "Gradient Descent(587/4999): loss=0.2961961805697165, w0=-0.31466399999144495, gamma=0.28278373776211996\n",
      "Gradient Descent(588/4999): loss=0.29619611464937146, w0=-0.3146639999916164, gamma=1.2551329822358268\n",
      "Gradient Descent(589/4999): loss=0.2961957459469659, w0=-0.31466399999216216, gamma=0.05996111244113701\n",
      "Gradient Descent(590/4999): loss=0.29619411006762564, w0=-0.3146639999921555, gamma=0.03505160300275254\n",
      "Gradient Descent(591/4999): loss=0.2961940318017636, w0=-0.31466399999215183, gamma=0.03507072888039017\n",
      "Gradient Descent(592/4999): loss=0.2961939859926436, w0=-0.3146639999921483, gamma=68.26474477936384\n",
      "Gradient Descent(593/4999): loss=0.2961939402939897, w0=-0.31466399998551225, gamma=160.057415203932\n",
      "Gradient Descent(594/4999): loss=0.2961055555875068, w0=-0.31466400103074044, gamma=0.4763869307626221\n",
      "Gradient Descent(595/4999): loss=0.29595234443659846, w0=-0.3146640005359594, gamma=0.12225143292176299\n",
      "Gradient Descent(596/4999): loss=0.29597770749764574, w0=-0.31466400046947135, gamma=0.05623379210691306\n",
      "Gradient Descent(597/4999): loss=0.29596517847575876, w0=-0.3146640004426272, gamma=0.04854911812208174\n",
      "Gradient Descent(598/4999): loss=0.29592389094442356, w0=-0.3146640004207544, gamma=0.03675848734103027\n",
      "Gradient Descent(599/4999): loss=0.2959208575301367, w0=-0.31466400040499765, gamma=0.040693339012884785\n",
      "Gradient Descent(600/4999): loss=0.29591968502558147, w0=-0.31466400038819536, gamma=0.20759909684289068\n",
      "Gradient Descent(601/4999): loss=0.2959188145729683, w0=-0.3146640003059656, gamma=0.24097345247478075\n",
      "Gradient Descent(602/4999): loss=0.29591565613227977, w0=-0.3146640002303304, gamma=0.2205928655772641\n",
      "Gradient Descent(603/4999): loss=0.29591321749813226, w0=-0.31466400017777546, gamma=0.04157789130245059\n",
      "Gradient Descent(604/4999): loss=0.29591166863816953, w0=-0.31466400017005464, gamma=0.03534243240220458\n",
      "Gradient Descent(605/4999): loss=0.2959110295893327, w0=-0.3146640001637646, gamma=0.040140252550198464\n",
      "Gradient Descent(606/4999): loss=0.29591075933722266, w0=-0.3146640001568731, gamma=0.7586131670847435\n",
      "Gradient Descent(607/4999): loss=0.29591047374459367, w0=-0.3146640000318578, gamma=1.5307091240695805\n",
      "Gradient Descent(608/4999): loss=0.2959058720675518, w0=-0.31466399997094385, gamma=0.10846906004245305\n",
      "Gradient Descent(609/4999): loss=0.2959043343284626, w0=-0.3146639999732322, gamma=0.07388436896638348\n",
      "Gradient Descent(610/4999): loss=0.29590148773304314, w0=-0.31466399997462186, gamma=0.05075002195621114\n",
      "Gradient Descent(611/4999): loss=0.2959008499259174, w0=-0.31466399997550587, gamma=0.03689661952387305\n",
      "Gradient Descent(612/4999): loss=0.29590067234867695, w0=-0.3146639999761159, gamma=0.03529175394469317\n",
      "Gradient Descent(613/4999): loss=0.2959005973131199, w0=-0.3146639999766779, gamma=0.09481964235169246\n",
      "Gradient Descent(614/4999): loss=0.29590054154760415, w0=-0.31466399997813443, gamma=0.965208903898779\n",
      "Gradient Descent(615/4999): loss=0.2959003932693687, w0=-0.31466399999155514, gamma=2.062863050941875\n",
      "Gradient Descent(616/4999): loss=0.295898960595619, w0=-0.31466399999254313, gamma=0.07583477113175537\n",
      "Gradient Descent(617/4999): loss=0.2958966080825159, w0=-0.31466399999250405, gamma=0.05110691962139321\n",
      "Gradient Descent(618/4999): loss=0.29589626876973824, w0=-0.3146639999924797, gamma=0.04142124312269515\n",
      "Gradient Descent(619/4999): loss=0.29589614202526615, w0=-0.314663999992461, gamma=0.036173938531441636\n",
      "Gradient Descent(620/4999): loss=0.29589608491806185, w0=-0.31466399999244526, gamma=0.05142023918944228\n",
      "Gradient Descent(621/4999): loss=0.29589603986115554, w0=-0.3146639999924238, gamma=0.1759639639146288\n",
      "Gradient Descent(622/4999): loss=0.2958959766340714, w0=-0.314663999992354, gamma=0.18560056890663865\n",
      "Gradient Descent(623/4999): loss=0.2958957624020564, w0=-0.31466399999229333, gamma=0.160041379332728\n",
      "Gradient Descent(624/4999): loss=0.29589553756820314, w0=-0.3146639999922507, gamma=0.041670944798514485\n",
      "Gradient Descent(625/4999): loss=0.29589534403585654, w0=-0.31466399999224137, gamma=0.03551191091828773\n",
      "Gradient Descent(626/4999): loss=0.29589529350459015, w0=-0.31466399999223377, gamma=0.047790849091416565\n",
      "Gradient Descent(627/4999): loss=0.29589525056664656, w0=-0.3146639999922239, gamma=3.262360422328419\n",
      "Gradient Descent(628/4999): loss=0.29589519280075516, w0=-0.3146639999915808, gamma=3.7352030619586394\n",
      "Gradient Descent(629/4999): loss=0.2958912663344891, w0=-0.3146639999932391, gamma=0.056070632597299046\n",
      "Gradient Descent(630/4999): loss=0.2958868235288936, w0=-0.31466399999317096, gamma=0.0384950128756493\n",
      "Gradient Descent(631/4999): loss=0.2958867365305766, w0=-0.31466399999312683, gamma=0.03631909041673267\n",
      "Gradient Descent(632/4999): loss=0.2958866847949492, w0=-0.3146639999930868, gamma=0.060871297712858104\n",
      "Gradient Descent(633/4999): loss=0.2958866409018261, w0=-0.31466399999302214, gamma=0.09672954630013522\n",
      "Gradient Descent(634/4999): loss=0.29588656788460793, w0=-0.3146639999929256, gamma=0.11116399442593738\n",
      "Gradient Descent(635/4999): loss=0.29588645218747156, w0=-0.3146639999928254, gamma=0.15726733950318458\n",
      "Gradient Descent(636/4999): loss=0.29588631929125636, w0=-0.31466399999269945, gamma=0.05914435797432083\n",
      "Gradient Descent(637/4999): loss=0.2958861313101452, w0=-0.31466399999265954, gamma=0.036420554057858226\n",
      "Gradient Descent(638/4999): loss=0.29588606061761497, w0=-0.3146639999926364, gamma=0.037678502836477445\n",
      "Gradient Descent(639/4999): loss=0.2958860170863696, w0=-0.31466399999261335, gamma=0.810614496952013\n",
      "Gradient Descent(640/4999): loss=0.29588597205530837, w0=-0.3146639999921359, gamma=1.6917823177824567\n",
      "Gradient Descent(641/4999): loss=0.2958850033751411, w0=-0.3146639999919471, gamma=0.37464936663897713\n",
      "Gradient Descent(642/4999): loss=0.29588298251162326, w0=-0.3146639999919759, gamma=0.03610306629455366\n",
      "Gradient Descent(643/4999): loss=0.2958825376828066, w0=-0.31466399999197764, gamma=0.03511092437906841\n",
      "Gradient Descent(644/4999): loss=0.29588249203339867, w0=-0.31466399999197925, gamma=0.05209008370697498\n",
      "Gradient Descent(645/4999): loss=0.2958824501023976, w0=-0.3146639999919816, gamma=0.09072465835539208\n",
      "Gradient Descent(646/4999): loss=0.2958823879098181, w0=-0.3146639999919854, gamma=0.171699779247609\n",
      "Gradient Descent(647/4999): loss=0.29588227959883584, w0=-0.314663999991992, gamma=3.327535753763084\n",
      "Gradient Descent(648/4999): loss=0.2958820746252504, w0=-0.31466399999209843, gamma=2.8484659362773788\n",
      "Gradient Descent(649/4999): loss=0.2958781035602407, w0=-0.31466399999188527, gamma=0.06763651488049831\n",
      "Gradient Descent(650/4999): loss=0.29587474693983423, w0=-0.3146639999918946, gamma=0.04885918913964494\n",
      "Gradient Descent(651/4999): loss=0.29587463344982057, w0=-0.31466399999190087, gamma=0.03551026639306475\n",
      "Gradient Descent(652/4999): loss=0.2958745700933664, w0=-0.3146639999919052, gamma=0.03553767531932198\n",
      "Gradient Descent(653/4999): loss=0.2958745268599443, w0=-0.3146639999919094, gamma=0.0897317320017541\n",
      "Gradient Descent(654/4999): loss=0.29587448448443154, w0=-0.31466399999191963, gamma=0.112044541089986\n",
      "Gradient Descent(655/4999): loss=0.2958743775484778, w0=-0.31466399999193123, gamma=0.3403769101859908\n",
      "Gradient Descent(656/4999): loss=0.29587424406115903, w0=-0.31466399999196254, gamma=0.481394009339122\n",
      "Gradient Descent(657/4999): loss=0.29587383862200267, w0=-0.3146639999919918, gamma=0.13614304186686504\n",
      "Gradient Descent(658/4999): loss=0.29587326532311103, w0=-0.31466399999199607, gamma=0.04132372575219609\n",
      "Gradient Descent(659/4999): loss=0.29587310323811306, w0=-0.3146639999919972, gamma=0.03557626227038141\n",
      "Gradient Descent(660/4999): loss=0.29587305399096603, w0=-0.3146639999919981, gamma=0.040852055097975785\n",
      "Gradient Descent(661/4999): loss=0.29587301162773355, w0=-0.3146639999919991, gamma=0.24811809028693635\n",
      "Gradient Descent(662/4999): loss=0.2958729629848311, w0=-0.31466399999200506, gamma=0.8427814421016407\n",
      "Gradient Descent(663/4999): loss=0.295872667559108, w0=-0.3146639999920203, gamma=0.23754333389911664\n",
      "Gradient Descent(664/4999): loss=0.2958716642482327, w0=-0.31466399999202094, gamma=0.0762702392005537\n",
      "Gradient Descent(665/4999): loss=0.2958713816120492, w0=-0.3146639999920211, gamma=0.0626989773868107\n",
      "Gradient Descent(666/4999): loss=0.29587129072306373, w0=-0.3146639999920212, gamma=0.03604504729823068\n",
      "Gradient Descent(667/4999): loss=0.29587121609970884, w0=-0.31466399999202127, gamma=0.035942940186309016\n",
      "Gradient Descent(668/4999): loss=0.29587117319752587, w0=-0.3146639999920213, gamma=0.9468180305331249\n",
      "Gradient Descent(669/4999): loss=0.29587113042219343, w0=-0.31466399999202294, gamma=36.038204176156405\n",
      "Gradient Descent(670/4999): loss=0.2958700037231316, w0=-0.31466399999202194, gamma=0.24059479510340936\n",
      "Gradient Descent(671/4999): loss=0.29582730115789374, w0=-0.3146639999920212, gamma=0.0541199935841857\n",
      "Gradient Descent(672/4999): loss=0.29582739619425474, w0=-0.3146639999920211, gamma=0.04652270483367906\n",
      "Gradient Descent(673/4999): loss=0.2958269391072101, w0=-0.31466399999202105, gamma=0.03646026096847269\n",
      "Gradient Descent(674/4999): loss=0.2958268603233305, w0=-0.314663999992021, gamma=0.037200792816800464\n",
      "Gradient Descent(675/4999): loss=0.29582681443058, w0=-0.31466399999202094, gamma=0.0746815948326972\n",
      "Gradient Descent(676/4999): loss=0.2958267704398666, w0=-0.31466399999202077, gamma=0.07952771026987568\n",
      "Gradient Descent(677/4999): loss=0.2958266825653489, w0=-0.3146639999920206, gamma=0.14411426102542593\n",
      "Gradient Descent(678/4999): loss=0.29582658910058474, w0=-0.3146639999920204, gamma=0.18311231525099164\n",
      "Gradient Descent(679/4999): loss=0.2958264197371431, w0=-0.3146639999920201, gamma=0.05041815926910502\n",
      "Gradient Descent(680/4999): loss=0.29582620455841324, w0=-0.31466399999202005, gamma=0.035895324816111816\n",
      "Gradient Descent(681/4999): loss=0.29582614530882645, w0=-0.31466399999202, gamma=0.03949224040410926\n",
      "Gradient Descent(682/4999): loss=0.2958261031284691, w0=-0.31466399999201994, gamma=7.308972469512533\n",
      "Gradient Descent(683/4999): loss=0.29582605672300316, w0=-0.3146639999920128, gamma=347.9573689048102\n",
      "Gradient Descent(684/4999): loss=0.2958174735992284, w0=-0.3146639999939106, gamma=0.04646439381561516\n",
      "Gradient Descent(685/4999): loss=0.2954628225426945, w0=-0.3146639999938218, gamma=0.03516909080836319\n",
      "Gradient Descent(686/4999): loss=0.29542616748897893, w0=-0.3146639999937571, gamma=0.03536817287190825\n",
      "Gradient Descent(687/4999): loss=0.2954216232572924, w0=-0.3146639999936945, gamma=0.10135543114115322\n",
      "Gradient Descent(688/4999): loss=0.295421449365517, w0=-0.31466399999352135, gamma=0.10420352298663961\n",
      "Gradient Descent(689/4999): loss=0.29542122277221955, w0=-0.31466399999336137, gamma=0.21857024883269247\n",
      "Gradient Descent(690/4999): loss=0.29542109477945927, w0=-0.3146639999930608, gamma=0.30040704907479476\n",
      "Gradient Descent(691/4999): loss=0.29542084403833657, w0=-0.3146639999927378, gamma=0.08588435861076407\n",
      "Gradient Descent(692/4999): loss=0.2954205241966785, w0=-0.3146639999926732, gamma=0.05606955195357168\n",
      "Gradient Descent(693/4999): loss=0.2954204302220619, w0=-0.31466399999263467, gamma=0.04512597261538143\n",
      "Gradient Descent(694/4999): loss=0.2954203701589055, w0=-0.31466399999260536, gamma=0.03782343376918229\n",
      "Gradient Descent(695/4999): loss=0.2954203227090757, w0=-0.31466399999258193, gamma=0.06123506878276108\n",
      "Gradient Descent(696/4999): loss=0.29542028304799267, w0=-0.3146639999925454, gamma=0.5463007564238123\n",
      "Gradient Descent(697/4999): loss=0.2954202189068726, w0=-0.31466399999223954, gamma=0.5882086447030316\n",
      "Gradient Descent(698/4999): loss=0.2954196483479106, w0=-0.31466399999209005, gamma=0.036519238618559505\n",
      "Gradient Descent(699/4999): loss=0.2954190373415717, w0=-0.3146639999920862, gamma=0.03504939840335268\n",
      "Gradient Descent(700/4999): loss=0.29541899776487957, w0=-0.31466399999208267, gamma=0.03819719565335131\n",
      "Gradient Descent(701/4999): loss=0.29541896127025874, w0=-0.31466399999207895, gamma=0.17569204628872248\n",
      "Gradient Descent(702/4999): loss=0.29541892150235954, w0=-0.3146639999920625, gamma=0.5358234991674881\n",
      "Gradient Descent(703/4999): loss=0.29541873859165513, w0=-0.31466399999202116, gamma=0.2959974578294719\n",
      "Gradient Descent(704/4999): loss=0.2954181808103535, w0=-0.31466399999201056, gamma=0.07623026447005443\n",
      "Gradient Descent(705/4999): loss=0.29541787278410286, w0=-0.3146639999920086, gamma=0.07010075207262847\n",
      "Gradient Descent(706/4999): loss=0.29541779338872953, w0=-0.314663999992007, gamma=0.07147154678837934\n",
      "Gradient Descent(707/4999): loss=0.29541772043318515, w0=-0.31466399999200545, gamma=0.23124453682957452\n",
      "Gradient Descent(708/4999): loss=0.29541764605353166, w0=-0.3146639999920008, gamma=1.0430987216333163\n",
      "Gradient Descent(709/4999): loss=0.2954174054090537, w0=-0.31466399999198463, gamma=0.09207967197467741\n",
      "Gradient Descent(710/4999): loss=0.29541632013178576, w0=-0.3146639999919847, gamma=0.03812470416202779\n",
      "Gradient Descent(711/4999): loss=0.2954162244010075, w0=-0.31466399999198474, gamma=0.035735529531509165\n",
      "Gradient Descent(712/4999): loss=0.29541618462406544, w0=-0.31466399999198474, gamma=0.04345456790434708\n",
      "Gradient Descent(713/4999): loss=0.29541614745045125, w0=-0.3146639999919848, gamma=0.1490653646402938\n",
      "Gradient Descent(714/4999): loss=0.29541610224990483, w0=-0.3146639999919849, gamma=2.660156733910871\n",
      "Gradient Descent(715/4999): loss=0.29541594719913894, w0=-0.31466399999198613, gamma=0.8553242915018411\n",
      "Gradient Descent(716/4999): loss=0.295413180960667, w0=-0.3146639999919856, gamma=0.05207058321940853\n",
      "Gradient Descent(717/4999): loss=0.2954123008459439, w0=-0.3146639999919856, gamma=0.049566578709541835\n",
      "Gradient Descent(718/4999): loss=0.29541223816988005, w0=-0.3146639999919856, gamma=0.03531430638231628\n",
      "Gradient Descent(719/4999): loss=0.295412186542224, w0=-0.3146639999919856, gamma=0.035181568832374596\n",
      "Gradient Descent(720/4999): loss=0.2954121498281668, w0=-0.3146639999919856, gamma=1.0611612799968293\n",
      "Gradient Descent(721/4999): loss=0.29541211327834577, w0=-0.3146639999919854, gamma=15.00679657590303\n",
      "Gradient Descent(722/4999): loss=0.2954110109510567, w0=-0.3146639999919867, gamma=0.09998817768170265\n",
      "Gradient Descent(723/4999): loss=0.2953954675531616, w0=-0.3146639999919866, gamma=0.05487668105727854\n",
      "Gradient Descent(724/4999): loss=0.29539535673283057, w0=-0.3146639999919865, gamma=0.05453548919897721\n",
      "Gradient Descent(725/4999): loss=0.2953952847076139, w0=-0.31466399999198646, gamma=0.04449620836032274\n",
      "Gradient Descent(726/4999): loss=0.29539522832625065, w0=-0.3146639999919864, gamma=0.03732947280999642\n",
      "Gradient Descent(727/4999): loss=0.29539518233965534, w0=-0.31466399999198635, gamma=0.06476897262777175\n",
      "Gradient Descent(728/4999): loss=0.29539514376156506, w0=-0.3146639999919863, gamma=5.474245882437191\n",
      "Gradient Descent(729/4999): loss=0.29539507682732485, w0=-0.31466399999198186, gamma=9.251995791928756\n",
      "Gradient Descent(730/4999): loss=0.2953894221555135, w0=-0.31466399999201566, gamma=0.03566688321046168\n",
      "Gradient Descent(731/4999): loss=0.2953805115502306, w0=-0.3146639999920146, gamma=0.035056779401695506\n",
      "Gradient Descent(732/4999): loss=0.2953798512119865, w0=-0.3146639999920136, gamma=0.055157904308103696\n",
      "Gradient Descent(733/4999): loss=0.2953798135608352, w0=-0.31466399999201206, gamma=0.11277725515825388\n",
      "Gradient Descent(734/4999): loss=0.29537975587677034, w0=-0.31466399999200906, gamma=0.12630065543906452\n",
      "Gradient Descent(735/4999): loss=0.2953796394408755, w0=-0.31466399999200606, gamma=0.17818665543712706\n",
      "Gradient Descent(736/4999): loss=0.2953795095110063, w0=-0.3146639999920024, gamma=0.212130238434237\n",
      "Gradient Descent(737/4999): loss=0.29537932624686636, w0=-0.31466399999199884, gamma=0.10057400799061735\n",
      "Gradient Descent(738/4999): loss=0.29537910810162227, w0=-0.3146639999919975, gamma=0.05410298774795776\n",
      "Gradient Descent(739/4999): loss=0.29537900468074485, w0=-0.31466399999199685, gamma=0.03956098651206493\n",
      "Gradient Descent(740/4999): loss=0.29537894904646356, w0=-0.3146639999919964, gamma=0.042477638287490405\n",
      "Gradient Descent(741/4999): loss=0.2953789083675318, w0=-0.31466399999199596, gamma=0.4374110823371562\n",
      "Gradient Descent(742/4999): loss=0.29537886469064434, w0=-0.3146639999919914, gamma=5.720587918516251\n",
      "Gradient Descent(743/4999): loss=0.2953784149514051, w0=-0.3146639999919578, gamma=0.1048374964011843\n",
      "Gradient Descent(744/4999): loss=0.2953725378842352, w0=-0.3146639999919607, gamma=0.035394156322048946\n",
      "Gradient Descent(745/4999): loss=0.29537243411091046, w0=-0.3146639999919616, gamma=0.035052909134195814\n",
      "Gradient Descent(746/4999): loss=0.29537239255910236, w0=-0.31466399999196243, gamma=0.05772377091535867\n",
      "Gradient Descent(747/4999): loss=0.2953723565844404, w0=-0.31466399999196376, gamma=0.09314555068362827\n",
      "Gradient Descent(748/4999): loss=0.29537229735027487, w0=-0.3146639999919658, gamma=0.5709219694971922\n",
      "Gradient Descent(749/4999): loss=0.29537220177202866, w0=-0.31466399999197714, gamma=3.828122840704784\n",
      "Gradient Descent(750/4999): loss=0.29537161598253553, w0=-0.31466399999200995, gamma=0.22820400968897273\n",
      "Gradient Descent(751/4999): loss=0.29536769048625056, w0=-0.3146639999920044, gamma=0.07266941925374695\n",
      "Gradient Descent(752/4999): loss=0.29536745859119834, w0=-0.31466399999200306, gamma=0.07035772459454631\n",
      "Gradient Descent(753/4999): loss=0.29536738166732013, w0=-0.31466399999200184, gamma=0.03539030048274181\n",
      "Gradient Descent(754/4999): loss=0.295367309582176, w0=-0.3146639999920013, gamma=0.03520341708220019\n",
      "Gradient Descent(755/4999): loss=0.29536727330147716, w0=-0.31466399999200073, gamma=1.017181480204772\n",
      "Gradient Descent(756/4999): loss=0.29536723723526076, w0=-0.3146639999919854, gamma=5.932467198701642\n",
      "Gradient Descent(757/4999): loss=0.29536619522190893, w0=-0.3146639999919875, gamma=0.17719264768184095\n",
      "Gradient Descent(758/4999): loss=0.2953601245063854, w0=-0.3146639999919872, gamma=0.05987675721145595\n",
      "Gradient Descent(759/4999): loss=0.295359947228354, w0=-0.3146639999919871, gamma=0.05530570513436096\n",
      "Gradient Descent(760/4999): loss=0.2953598801694149, w0=-0.314663999991987, gamma=0.05367843746726947\n",
      "Gradient Descent(761/4999): loss=0.2953598234786792, w0=-0.31466399999198696, gamma=0.04641935081523567\n",
      "Gradient Descent(762/4999): loss=0.2953597685902584, w0=-0.3146639999919869, gamma=0.051334925469041455\n",
      "Gradient Descent(763/4999): loss=0.2953597211351053, w0=-0.31466399999198685, gamma=0.12002867163660028\n",
      "Gradient Descent(764/4999): loss=0.29535966865947794, w0=-0.31466399999198674, gamma=0.24554867021372917\n",
      "Gradient Descent(765/4999): loss=0.29535954597273856, w0=-0.3146639999919865, gamma=0.0759522677791131\n",
      "Gradient Descent(766/4999): loss=0.2953592950167118, w0=-0.31466399999198646, gamma=0.03580137970616366\n",
      "Gradient Descent(767/4999): loss=0.2953592173988222, w0=-0.31466399999198646, gamma=0.035577660501714285\n",
      "Gradient Descent(768/4999): loss=0.295359180805729, w0=-0.31466399999198646, gamma=1.0951014385413804\n",
      "Gradient Descent(769/4999): loss=0.29535914444825456, w0=-0.31466399999198574, gamma=22.45997556481666\n",
      "Gradient Descent(770/4999): loss=0.29535802545299433, w0=-0.3146639999919887, gamma=2.3483858560801605\n",
      "Gradient Descent(771/4999): loss=0.29533512611491647, w0=-0.3146639999919821, gamma=0.03966687535273703\n",
      "Gradient Descent(772/4999): loss=0.29533416801575174, w0=-0.31466399999198225, gamma=0.03518708648351253\n",
      "Gradient Descent(773/4999): loss=0.2953328313076883, w0=-0.31466399999198247, gamma=0.03827380200746607\n",
      "Gradient Descent(774/4999): loss=0.2953327462216412, w0=-0.3146639999919827, gamma=0.1481793329780521\n",
      "Gradient Descent(775/4999): loss=0.2953326875456659, w0=-0.3146639999919834, gamma=0.20721766071942807\n",
      "Gradient Descent(776/4999): loss=0.29533249408085405, w0=-0.3146639999919842, gamma=0.15270886973013867\n",
      "Gradient Descent(777/4999): loss=0.29533227007570395, w0=-0.3146639999919845, gamma=0.08414601763504823\n",
      "Gradient Descent(778/4999): loss=0.29533211360371014, w0=-0.3146639999919847, gamma=0.07726994065953248\n",
      "Gradient Descent(779/4999): loss=0.2953320267914026, w0=-0.3146639999919848, gamma=0.041199367939760125\n",
      "Gradient Descent(780/4999): loss=0.2953319483741923, w0=-0.31466399999198486, gamma=0.0372890848090241\n",
      "Gradient Descent(781/4999): loss=0.2953319065585967, w0=-0.3146639999919849, gamma=0.08282528628713608\n",
      "Gradient Descent(782/4999): loss=0.29533186874665235, w0=-0.314663999991985, gamma=0.33894430504376916\n",
      "Gradient Descent(783/4999): loss=0.29533178478129124, w0=-0.31466399999198547, gamma=0.3413369546389126\n",
      "Gradient Descent(784/4999): loss=0.29533144132215455, w0=-0.31466399999198574, gamma=0.04208960992061853\n",
      "Gradient Descent(785/4999): loss=0.29533109557580306, w0=-0.31466399999198574, gamma=0.03511726760779513\n",
      "Gradient Descent(786/4999): loss=0.29533105290149914, w0=-0.31466399999198574, gamma=0.03712874156498687\n",
      "Gradient Descent(787/4999): loss=0.2953310173283631, w0=-0.31466399999198574, gamma=39.46313863350113\n",
      "Gradient Descent(788/4999): loss=0.29533097971985467, w0=-0.3146639999920074, gamma=50.09233920669146\n",
      "Gradient Descent(789/4999): loss=0.29529113450130867, w0=-0.3146639999909827, gamma=0.48671324829489043\n",
      "Gradient Descent(790/4999): loss=0.2952504072135489, w0=-0.31466399999148037, gamma=0.049227215200998216\n",
      "Gradient Descent(791/4999): loss=0.2952656093801734, w0=-0.31466399999150574, gamma=0.037438003978717826\n",
      "Gradient Descent(792/4999): loss=0.295244403787522, w0=-0.3146639999915238, gamma=0.03584404119691562\n",
      "Gradient Descent(793/4999): loss=0.2952413982187533, w0=-0.31466399999154054, gamma=0.05775115927038489\n",
      "Gradient Descent(794/4999): loss=0.2952411952732432, w0=-0.3146639999915666, gamma=0.09493263567748673\n",
      "Gradient Descent(795/4999): loss=0.295241010791388, w0=-0.3146639999916069, gamma=0.20843593217413797\n",
      "Gradient Descent(796/4999): loss=0.29524079411762055, w0=-0.3146639999916869, gamma=0.4520972848068546\n",
      "Gradient Descent(797/4999): loss=0.29524042206009715, w0=-0.3146639999918242, gamma=0.09571101183127141\n",
      "Gradient Descent(798/4999): loss=0.29523987914226074, w0=-0.31466399999184, gamma=0.035790416762418335\n",
      "Gradient Descent(799/4999): loss=0.2952398301173789, w0=-0.3146639999918454, gamma=0.03505919690720327\n",
      "Gradient Descent(800/4999): loss=0.29523971146066713, w0=-0.31466399999185046, gamma=0.06265548564613774\n",
      "Gradient Descent(801/4999): loss=0.29523967574247023, w0=-0.3146639999918592, gamma=0.32156183050022297\n",
      "Gradient Descent(802/4999): loss=0.29523961229649015, w0=-0.31466399999190103, gamma=0.6140824246782924\n",
      "Gradient Descent(803/4999): loss=0.2952392899606848, w0=-0.3146639999919551, gamma=0.24161650468320284\n",
      "Gradient Descent(804/4999): loss=0.2952386840472559, w0=-0.31466399999196326, gamma=0.07323959675028889\n",
      "Gradient Descent(805/4999): loss=0.2952384480348027, w0=-0.31466399999196515, gamma=0.06453736080934744\n",
      "Gradient Descent(806/4999): loss=0.29523837400470304, w0=-0.3146639999919667, gamma=0.04851862424834877\n",
      "Gradient Descent(807/4999): loss=0.2952383104447892, w0=-0.31466399999196776, gamma=0.03794803389497786\n",
      "Gradient Descent(808/4999): loss=0.2952382627177675, w0=-0.31466399999196853, gamma=0.03999227337306183\n",
      "Gradient Descent(809/4999): loss=0.2952382253967009, w0=-0.31466399999196937, gamma=0.2741085942840039\n",
      "Gradient Descent(810/4999): loss=0.29523818606720625, w0=-0.31466399999197475, gamma=0.8140042969237022\n",
      "Gradient Descent(811/4999): loss=0.29523791651256626, w0=-0.3146639999919863, gamma=0.29186310676144445\n",
      "Gradient Descent(812/4999): loss=0.2952371161560547, w0=-0.3146639999919871, gamma=0.04194733491652082\n",
      "Gradient Descent(813/4999): loss=0.29523682946418717, w0=-0.31466399999198713, gamma=0.03567314238295843\n",
      "Gradient Descent(814/4999): loss=0.29523678801448594, w0=-0.3146639999919872, gamma=0.03852370076584348\n",
      "Gradient Descent(815/4999): loss=0.2952367529363614, w0=-0.31466399999198724, gamma=0.11855243262194952\n",
      "Gradient Descent(816/4999): loss=0.29523671506747007, w0=-0.31466399999198746, gamma=0.16994152907923737\n",
      "Gradient Descent(817/4999): loss=0.2952365985378494, w0=-0.31466399999198774, gamma=0.6048385211863917\n",
      "Gradient Descent(818/4999): loss=0.2952364315054563, w0=-0.31466399999198846, gamma=0.2700646578709363\n",
      "Gradient Descent(819/4999): loss=0.29523583707957407, w0=-0.31466399999198863, gamma=0.0530012322831721\n",
      "Gradient Descent(820/4999): loss=0.2952355717914025, w0=-0.31466399999198863, gamma=0.04298420160048126\n",
      "Gradient Descent(821/4999): loss=0.29523551963000927, w0=-0.31466399999198863, gamma=0.036505953091660534\n",
      "Gradient Descent(822/4999): loss=0.29523547738802836, w0=-0.31466399999198863, gamma=0.047595135213786104\n",
      "Gradient Descent(823/4999): loss=0.295235441518951, w0=-0.31466399999198863, gamma=0.406226483315884\n",
      "Gradient Descent(824/4999): loss=0.29523539475544425, w0=-0.3146639999919888, gamma=2.13244393532678\n",
      "Gradient Descent(825/4999): loss=0.2952349956453871, w0=-0.31466399999198935, gamma=0.16416836114370448\n",
      "Gradient Descent(826/4999): loss=0.29523290134644703, w0=-0.3146639999919893, gamma=0.050713508046780084\n",
      "Gradient Descent(827/4999): loss=0.2952327406081803, w0=-0.3146639999919893, gamma=0.03694429760649696\n",
      "Gradient Descent(828/4999): loss=0.29523269027911675, w0=-0.3146639999919893, gamma=0.03634136277504889\n",
      "Gradient Descent(829/4999): loss=0.29523265390434644, w0=-0.3146639999919893, gamma=0.07136310093334075\n",
      "Gradient Descent(830/4999): loss=0.29523261822373414, w0=-0.3146639999919893, gamma=0.09493247937317452\n",
      "Gradient Descent(831/4999): loss=0.29523254816906663, w0=-0.3146639999919893, gamma=1.2857093352623739\n",
      "Gradient Descent(832/4999): loss=0.29523245498277423, w0=-0.31466399999198913, gamma=2.7569676323934664\n",
      "Gradient Descent(833/4999): loss=0.2952311930815933, w0=-0.3146639999919894, gamma=0.03790232036776193\n",
      "Gradient Descent(834/4999): loss=0.29522849761213815, w0=-0.3146639999919894, gamma=0.035146420900383064\n",
      "Gradient Descent(835/4999): loss=0.29522845155153843, w0=-0.3146639999919894, gamma=0.0393242080013537\n",
      "Gradient Descent(836/4999): loss=0.29522841698804386, w0=-0.3146639999919894, gamma=0.08152793177151982\n",
      "Gradient Descent(837/4999): loss=0.2952283784214313, w0=-0.3146639999919894, gamma=0.08669472504194467\n",
      "Gradient Descent(838/4999): loss=0.29522829849468624, w0=-0.3146639999919894, gamma=4.540557609204346\n",
      "Gradient Descent(839/4999): loss=0.29522821351112805, w0=-0.31466399999198935, gamma=38.99914737890242\n",
      "Gradient Descent(840/4999): loss=0.29522376426817015, w0=-0.3146639999920004, gamma=0.04792744342932488\n",
      "Gradient Descent(841/4999): loss=0.2951867672577684, w0=-0.31466399999200007, gamma=0.036215636272691205\n",
      "Gradient Descent(842/4999): loss=0.29518585136436193, w0=-0.31466399999199973, gamma=0.0368379095009083\n",
      "Gradient Descent(843/4999): loss=0.2951856709598717, w0=-0.31466399999199945, gamma=0.11137541013044724\n",
      "Gradient Descent(844/4999): loss=0.2951856128952088, w0=-0.31466399999199857, gamma=0.12240423351526543\n",
      "Gradient Descent(845/4999): loss=0.2951854839007768, w0=-0.3146639999919977, gamma=0.08773094908778012\n",
      "Gradient Descent(846/4999): loss=0.29518536529930356, w0=-0.3146639999919971, gamma=0.04471987590718606\n",
      "Gradient Descent(847/4999): loss=0.29518528047780523, w0=-0.31466399999199685, gamma=0.036860550951751093\n",
      "Gradient Descent(848/4999): loss=0.2951852372224454, w0=-0.3146639999919966, gamma=0.03782452539003164\n",
      "Gradient Descent(849/4999): loss=0.2951852015890339, w0=-0.3146639999919964, gamma=0.3496958249360216\n",
      "Gradient Descent(850/4999): loss=0.2951851650259502, w0=-0.3146639999919947, gamma=4.192513361231374\n",
      "Gradient Descent(851/4999): loss=0.29518482700517795, w0=-0.31466399999198136, gamma=0.21432088288218523\n",
      "Gradient Descent(852/4999): loss=0.2951807765893672, w0=-0.3146639999919836, gamma=0.05096583826325514\n",
      "Gradient Descent(853/4999): loss=0.2951805733040988, w0=-0.314663999991984, gamma=0.04510952975499425\n",
      "Gradient Descent(854/4999): loss=0.2951805202786662, w0=-0.31466399999198436, gamma=0.03599040364892549\n",
      "Gradient Descent(855/4999): loss=0.29518047655548707, w0=-0.31466399999198463, gamma=0.03659930599333942\n",
      "Gradient Descent(856/4999): loss=0.2951804418017548, w0=-0.3146639999919849, gamma=0.09829492674877928\n",
      "Gradient Descent(857/4999): loss=0.29518040647693067, w0=-0.3146639999919856, gamma=0.17601744916407625\n",
      "Gradient Descent(858/4999): loss=0.29518031160875885, w0=-0.3146639999919867, gamma=1.1376715374536088\n",
      "Gradient Descent(859/4999): loss=0.29518014173471, w0=-0.31466399999199246, gamma=0.4205726029099198\n",
      "Gradient Descent(860/4999): loss=0.2951790439294082, w0=-0.3146639999919922, gamma=0.05369828831569727\n",
      "Gradient Descent(861/4999): loss=0.29517863877322975, w0=-0.3146639999919922, gamma=0.044702098677308864\n",
      "Gradient Descent(862/4999): loss=0.29517858644881445, w0=-0.3146639999919922, gamma=0.03627235128025608\n",
      "Gradient Descent(863/4999): loss=0.29517854328738685, w0=-0.3146639999919922, gamma=0.03866335284669814\n",
      "Gradient Descent(864/4999): loss=0.2951785082966277, w0=-0.3146639999919922, gamma=0.13657817811623957\n",
      "Gradient Descent(865/4999): loss=0.29517847100385014, w0=-0.3146639999919921, gamma=0.3286260244357027\n",
      "Gradient Descent(866/4999): loss=0.2951783392712974, w0=-0.31466399999199207, gamma=0.5254376520605245\n",
      "Gradient Descent(867/4999): loss=0.29517802232386964, w0=-0.314663999991992, gamma=0.10134365610813285\n",
      "Gradient Descent(868/4999): loss=0.2951775156479576, w0=-0.314663999991992, gamma=0.050250759130282506\n",
      "Gradient Descent(869/4999): loss=0.29517741791846924, w0=-0.314663999991992, gamma=0.03609298655301223\n",
      "Gradient Descent(870/4999): loss=0.2951773694512138, w0=-0.314663999991992, gamma=0.03647192513515141\n",
      "Gradient Descent(871/4999): loss=0.2951773346478702, w0=-0.314663999991992, gamma=0.4012382633336443\n",
      "Gradient Descent(872/4999): loss=0.29517729948266463, w0=-0.314663999991992, gamma=1.7699551913479208\n",
      "Gradient Descent(873/4999): loss=0.29517691263652907, w0=-0.314663999991992, gamma=0.1721108404136347\n",
      "Gradient Descent(874/4999): loss=0.2951752067396075, w0=-0.314663999991992, gamma=0.07271182014207299\n",
      "Gradient Descent(875/4999): loss=0.2951750410296913, w0=-0.314663999991992, gamma=0.057317687161388436\n",
      "Gradient Descent(876/4999): loss=0.2951749707120546, w0=-0.314663999991992, gamma=0.03622323070434973\n",
      "Gradient Descent(877/4999): loss=0.2951749154728191, w0=-0.314663999991992, gamma=0.03545173389950303\n",
      "Gradient Descent(878/4999): loss=0.2951748805661314, w0=-0.314663999991992, gamma=0.2908926747205546\n",
      "Gradient Descent(879/4999): loss=0.2951748464119827, w0=-0.314663999991992, gamma=3.5534169865029255\n",
      "Gradient Descent(880/4999): loss=0.2951745661755222, w0=-0.3146639999919925, gamma=0.5675350734600708\n",
      "Gradient Descent(881/4999): loss=0.2951711442701069, w0=-0.31466399999199246, gamma=0.0771715265511355\n",
      "Gradient Descent(882/4999): loss=0.295170603771673, w0=-0.31466399999199246, gamma=0.06711301367852399\n",
      "Gradient Descent(883/4999): loss=0.29517052407940486, w0=-0.31466399999199246, gamma=0.0351548854029602\n",
      "Gradient Descent(884/4999): loss=0.29517045946552983, w0=-0.31466399999199246, gamma=0.035050021330118405\n",
      "Gradient Descent(885/4999): loss=0.2951704254246154, w0=-0.31466399999199246, gamma=0.4711336555266942\n",
      "Gradient Descent(886/4999): loss=0.2951703917066363, w0=-0.3146639999919924, gamma=2.4533495868900017\n",
      "Gradient Descent(887/4999): loss=0.29516993849938505, w0=-0.3146639999919925, gamma=0.15461025917796897\n",
      "Gradient Descent(888/4999): loss=0.2951675795932692, w0=-0.3146639999919925, gamma=0.06897467436214791\n",
      "Gradient Descent(889/4999): loss=0.29516743114947613, w0=-0.3146639999919925, gamma=0.06303497864452329\n",
      "Gradient Descent(890/4999): loss=0.2951673643474261, w0=-0.3146639999919925, gamma=0.06141535692559658\n",
      "Gradient Descent(891/4999): loss=0.2951673037537144, w0=-0.3146639999919925, gamma=0.1360927979709185\n",
      "Gradient Descent(892/4999): loss=0.2951672447326269, w0=-0.3146639999919925, gamma=1.0663908777723714\n",
      "Gradient Descent(893/4999): loss=0.2951671139493432, w0=-0.3146639999919926, gamma=0.2120430590906684\n",
      "Gradient Descent(894/4999): loss=0.29516608929796245, w0=-0.3146639999919926, gamma=0.035592534816420475\n",
      "Gradient Descent(895/4999): loss=0.2951658859872076, w0=-0.3146639999919926, gamma=0.035112840626415065\n",
      "Gradient Descent(896/4999): loss=0.29516585140149315, w0=-0.3146639999919926, gamma=0.06430854066592159\n",
      "Gradient Descent(897/4999): loss=0.29516581767268524, w0=-0.3146639999919926, gamma=0.16266316437093453\n",
      "Gradient Descent(898/4999): loss=0.2951657559013364, w0=-0.3146639999919926, gamma=2.9912547402398375\n",
      "Gradient Descent(899/4999): loss=0.29516559966082795, w0=-0.3146639999919928, gamma=1.331651695143424\n",
      "Gradient Descent(900/4999): loss=0.2951627273313413, w0=-0.3146639999919929, gamma=0.05836776087159864\n",
      "Gradient Descent(901/4999): loss=0.2951614635426726, w0=-0.3146639999919929, gamma=0.05568888528683653\n",
      "Gradient Descent(902/4999): loss=0.29516139391740254, w0=-0.3146639999919929, gamma=0.05417321373841859\n",
      "Gradient Descent(903/4999): loss=0.29516134024066687, w0=-0.3146639999919929, gamma=0.04284016516802893\n",
      "Gradient Descent(904/4999): loss=0.2951612882602255, w0=-0.3146639999919929, gamma=0.03924461691182572\n",
      "Gradient Descent(905/4999): loss=0.2951612471678769, w0=-0.3146639999919929, gamma=0.067797913630968\n",
      "Gradient Descent(906/4999): loss=0.29516120952771635, w0=-0.3146639999919929, gamma=0.39776157178086996\n",
      "Gradient Descent(907/4999): loss=0.2951611445031481, w0=-0.31466399999199296, gamma=2.0803767385974448\n",
      "Gradient Descent(908/4999): loss=0.2951607630312168, w0=-0.31466399999199307, gamma=0.05706461811220318\n",
      "Gradient Descent(909/4999): loss=0.2951587688991416, w0=-0.31466399999199307, gamma=0.035200360897678085\n",
      "Gradient Descent(910/4999): loss=0.2951587138829739, w0=-0.31466399999199307, gamma=0.03508129519115196\n",
      "Gradient Descent(911/4999): loss=0.2951586799463672, w0=-0.31466399999199307, gamma=0.13863136313321467\n",
      "Gradient Descent(912/4999): loss=0.29515864632759037, w0=-0.31466399999199307, gamma=0.2985288395384878\n",
      "Gradient Descent(913/4999): loss=0.2951585134796938, w0=-0.3146639999919931, gamma=0.4845506896685322\n",
      "Gradient Descent(914/4999): loss=0.2951582274218491, w0=-0.3146639999919931, gamma=0.13326474293634388\n",
      "Gradient Descent(915/4999): loss=0.29515776318019543, w0=-0.3146639999919931, gamma=0.08169599611057753\n",
      "Gradient Descent(916/4999): loss=0.2951576355020504, w0=-0.3146639999919931, gamma=0.08380598836658205\n",
      "Gradient Descent(917/4999): loss=0.29515755723164006, w0=-0.3146639999919931, gamma=0.3303101824402404\n",
      "Gradient Descent(918/4999): loss=0.29515747695030875, w0=-0.3146639999919932, gamma=0.19318045120111602\n",
      "Gradient Descent(919/4999): loss=0.29515716054798047, w0=-0.3146639999919932, gamma=0.03725877742047537\n",
      "Gradient Descent(920/4999): loss=0.29515697554604253, w0=-0.3146639999919932, gamma=0.035141356983863024\n",
      "Gradient Descent(921/4999): loss=0.2951569398314369, w0=-0.3146639999919932, gamma=0.062061145637824125\n",
      "Gradient Descent(922/4999): loss=0.29515690617460333, w0=-0.3146639999919932, gamma=469.73132486820754\n",
      "Gradient Descent(923/4999): loss=0.2951568467361841, w0=-0.31466399999202327, gamma=2132.408901368649\n",
      "Gradient Descent(924/4999): loss=0.29472358364213, w0=-0.31466399999187755, gamma=0.03709618184067517\n",
      "Gradient Descent(925/4999): loss=0.8194607814531482, w0=-0.314663999991942, gamma=0.03562295918647138\n",
      "Gradient Descent(926/4999): loss=0.3072424672425482, w0=-0.31466399999195765, gamma=0.05087228124766288\n",
      "Gradient Descent(927/4999): loss=0.2965134498343933, w0=-0.31466399999198125, gamma=0.07291976626148147\n",
      "Gradient Descent(928/4999): loss=0.2936474070439089, w0=-0.31466399999201133, gamma=0.07997073153278435\n",
      "Gradient Descent(929/4999): loss=0.29325601726003153, w0=-0.31466399999204075, gamma=0.07134900104740308\n",
      "Gradient Descent(930/4999): loss=0.2932513143408306, w0=-0.3146639999920646, gamma=0.03748990061994152\n",
      "Gradient Descent(931/4999): loss=0.29325115439249677, w0=-0.31466399999207634, gamma=0.0353190687890017\n",
      "Gradient Descent(932/4999): loss=0.29325093425312393, w0=-0.31466399999208694, gamma=0.05060464691366611\n",
      "Gradient Descent(933/4999): loss=0.29325091466785685, w0=-0.31466399999210154, gamma=0.10960634966980524\n",
      "Gradient Descent(934/4999): loss=0.2932508916325938, w0=-0.3146639999921316, gamma=0.11565827590552576\n",
      "Gradient Descent(935/4999): loss=0.29325084631085385, w0=-0.3146639999921599, gamma=0.10813902630337638\n",
      "Gradient Descent(936/4999): loss=0.29325079977227797, w0=-0.31466399999218325, gamma=0.04087760255777185\n",
      "Gradient Descent(937/4999): loss=0.2932507562752517, w0=-0.31466399999219113, gamma=0.03635674086994691\n",
      "Gradient Descent(938/4999): loss=0.29325073983155076, w0=-0.31466399999219785, gamma=0.06272203237833583\n",
      "Gradient Descent(939/4999): loss=0.2932507252078048, w0=-0.314663999992209, gamma=2.8429425822289227\n",
      "Gradient Descent(940/4999): loss=0.29325069997950315, w0=-0.31466399999268374, gamma=10.148056704653442\n",
      "Gradient Descent(941/4999): loss=0.2932495566869061, w0=-0.31466399998957384, gamma=0.0427964697789588\n",
      "Gradient Descent(942/4999): loss=0.29324550103621627, w0=-0.31466399998969397, gamma=0.03573024364405844\n",
      "Gradient Descent(943/4999): loss=0.2932454646247434, w0=-0.31466399998979, gamma=0.03590175737234862\n",
      "Gradient Descent(944/4999): loss=0.29324544921717194, w0=-0.31466399998988304, gamma=0.08324467494535187\n",
      "Gradient Descent(945/4999): loss=0.29324543471460734, w0=-0.31466399999009104, gamma=0.1469094694820251\n",
      "Gradient Descent(946/4999): loss=0.2932454011693608, w0=-0.31466399999042755, gamma=0.25089179589622074\n",
      "Gradient Descent(947/4999): loss=0.2932453420775778, w0=-0.3146639999909179, gamma=0.1271454689073756\n",
      "Gradient Descent(948/4999): loss=0.2932452413148991, w0=-0.314663999991104, gamma=0.05551797027310025\n",
      "Gradient Descent(949/4999): loss=0.2932451903085557, w0=-0.31466399999117495, gamma=0.05218898642618628\n",
      "Gradient Descent(950/4999): loss=0.2932451680082992, w0=-0.31466399999123795, gamma=0.052586777856773716\n",
      "Gradient Descent(951/4999): loss=0.2932451470732423, w0=-0.3146639999912981, gamma=0.09542731309851392\n",
      "Gradient Descent(952/4999): loss=0.2932451259805163, w0=-0.31466399999140154, gamma=0.21313116801715656\n",
      "Gradient Descent(953/4999): loss=0.2932450877076165, w0=-0.31466399999161054, gamma=0.09185633492921269\n",
      "Gradient Descent(954/4999): loss=0.2932450022402643, w0=-0.31466399999168143, gamma=0.03644972293188985\n",
      "Gradient Descent(955/4999): loss=0.2932449654124649, w0=-0.31466399999170697, gamma=0.03549116745439659\n",
      "Gradient Descent(956/4999): loss=0.2932449507937891, w0=-0.31466399999173095, gamma=0.25571092926456185\n",
      "Gradient Descent(957/4999): loss=0.2932449365641438, w0=-0.31466399999189754, gamma=1.420598723368733\n",
      "Gradient Descent(958/4999): loss=0.29324483404489954, w0=-0.3146639999925864, gamma=2.34477825725359\n",
      "Gradient Descent(959/4999): loss=0.2932442646058022, w0=-0.3146639999921098, gamma=0.05248904854425939\n",
      "Gradient Descent(960/4999): loss=0.29324332542920933, w0=-0.3146639999921242, gamma=0.03504172857654973\n",
      "Gradient Descent(961/4999): loss=0.29324330414627914, w0=-0.3146639999921333, gamma=0.035066980031708425\n",
      "Gradient Descent(962/4999): loss=0.29324329002444366, w0=-0.3146639999921421, gamma=1.0393874037852346\n",
      "Gradient Descent(963/4999): loss=0.29324327597722233, w0=-0.31466399999239375, gamma=1.0816005451019615\n",
      "Gradient Descent(964/4999): loss=0.2932428596908885, w0=-0.314663999992384, gamma=2.1195875774223554\n",
      "Gradient Descent(965/4999): loss=0.29324242661205646, w0=-0.31466399999238653, gamma=0.47938685778887735\n",
      "Gradient Descent(966/4999): loss=0.2932415782048513, w0=-0.3146639999923863, gamma=0.059691055302401624\n",
      "Gradient Descent(967/4999): loss=0.29324138705782904, w0=-0.3146639999923863, gamma=0.05600093513389698\n",
      "Gradient Descent(968/4999): loss=0.29324136250446153, w0=-0.3146639999923863, gamma=0.053590018216555756\n",
      "Gradient Descent(969/4999): loss=0.29324134008337693, w0=-0.3146639999923863, gamma=0.04657060874198246\n",
      "Gradient Descent(970/4999): loss=0.2932413186410478, w0=-0.3146639999923863, gamma=0.04954119244575539\n",
      "Gradient Descent(971/4999): loss=0.29324130000879833, w0=-0.3146639999923863, gamma=0.104420120146335\n",
      "Gradient Descent(972/4999): loss=0.2932412801886422, w0=-0.3146639999923863, gamma=0.30646444832435826\n",
      "Gradient Descent(973/4999): loss=0.2932412384136838, w0=-0.31466399999238637, gamma=0.1342930562095113\n",
      "Gradient Descent(974/4999): loss=0.29324111581277656, w0=-0.3146639999923864, gamma=0.036683239765931394\n",
      "Gradient Descent(975/4999): loss=0.2932410620961522, w0=-0.3146639999923864, gamma=0.03523590836352035\n",
      "Gradient Descent(976/4999): loss=0.293241047417419, w0=-0.3146639999923864, gamma=0.12653085348294046\n",
      "Gradient Descent(977/4999): loss=0.29324103332256535, w0=-0.3146639999923865, gamma=112.19251781990174\n",
      "Gradient Descent(978/4999): loss=0.2932409827091735, w0=-0.31466399999241407, gamma=141.69006273581113\n",
      "Gradient Descent(979/4999): loss=0.29319639301530476, w0=-0.3146639999955675, gamma=0.04086164211527467\n",
      "Gradient Descent(980/4999): loss=0.29367445488698246, w0=-0.31466399999544187, gamma=0.03586258051546133\n",
      "Gradient Descent(981/4999): loss=0.2931971480632712, w0=-0.31466399999533734, gamma=0.04137564409185561\n",
      "Gradient Descent(982/4999): loss=0.29316812519073665, w0=-0.31466399999522077, gamma=0.08198299002811751\n",
      "Gradient Descent(983/4999): loss=0.29316068611216134, w0=-0.31466399999499917, gamma=0.09040602681045128\n",
      "Gradient Descent(984/4999): loss=0.2931551612408899, w0=-0.3146639999947745, gamma=0.5479404151786492\n",
      "Gradient Descent(985/4999): loss=0.29315175595196485, w0=-0.3146639999935349, gamma=0.4188221201541924\n",
      "Gradient Descent(986/4999): loss=0.29314187945044945, w0=-0.31466399999310146, gamma=0.03522222578734502\n",
      "Gradient Descent(987/4999): loss=0.2931656894868887, w0=-0.31466399999308053, gamma=0.03503486446434774\n",
      "Gradient Descent(988/4999): loss=0.29314115730031637, w0=-0.31466399999306016, gamma=0.055315390542534565\n",
      "Gradient Descent(989/4999): loss=0.29314113437926687, w0=-0.31466399999302913, gamma=0.10680072818017415\n",
      "Gradient Descent(990/4999): loss=0.2931411044126324, w0=-0.3146639999929725, gamma=0.27231007255874956\n",
      "Gradient Descent(991/4999): loss=0.2931410519287641, w0=-0.31466399999284356, gamma=0.5295409860664934\n",
      "Gradient Descent(992/4999): loss=0.29314093021988463, w0=-0.314663999992661, gamma=0.17084761023848924\n",
      "Gradient Descent(993/4999): loss=0.29314072300494354, w0=-0.3146639999926332, gamma=0.0789681940183707\n",
      "Gradient Descent(994/4999): loss=0.2931406593262332, w0=-0.3146639999926226, gamma=0.07732271170130788\n",
      "Gradient Descent(995/4999): loss=0.29314062538192714, w0=-0.314663999992613, gamma=0.05581905574585908\n",
      "Gradient Descent(996/4999): loss=0.2931405961209513, w0=-0.31466399999260664, gamma=0.045937865239129724\n",
      "Gradient Descent(997/4999): loss=0.2931405750158182, w0=-0.3146639999926017, gamma=0.053485959878298545\n",
      "Gradient Descent(998/4999): loss=0.2931405576533362, w0=-0.3146639999925962, gamma=0.17274314314335731\n",
      "Gradient Descent(999/4999): loss=0.2931405374422612, w0=-0.3146639999925794, gamma=0.3382198507265766\n",
      "Gradient Descent(1000/4999): loss=0.29314047218380446, w0=-0.3146639999925522, gamma=0.054931102617552074\n",
      "Gradient Descent(1001/4999): loss=0.2931403444946384, w0=-0.31466399999254924, gamma=0.03512880683909573\n",
      "Gradient Descent(1002/4999): loss=0.293140323742489, w0=-0.31466399999254746, gamma=0.035303182594390345\n",
      "Gradient Descent(1003/4999): loss=0.29314031047443184, w0=-0.31466399999254574, gamma=1.1585765628975226\n",
      "Gradient Descent(1004/4999): loss=0.293140297148681, w0=-0.3146639999924917, gamma=1.2551882550070494\n",
      "Gradient Descent(1005/4999): loss=0.2931398598922748, w0=-0.31466399999250155, gamma=1.8380101612025053\n",
      "Gradient Descent(1006/4999): loss=0.2931393863015575, w0=-0.3146639999924988, gamma=0.24366592194314513\n",
      "Gradient Descent(1007/4999): loss=0.2931386930515812, w0=-0.3146639999924993, gamma=0.037126193832825685\n",
      "Gradient Descent(1008/4999): loss=0.29313860159003213, w0=-0.31466399999249933, gamma=0.03521058357231857\n",
      "Gradient Descent(1009/4999): loss=0.2931385871543788, w0=-0.3146639999924994, gamma=0.046138393117024216\n",
      "Gradient Descent(1010/4999): loss=0.29313857387218517, w0=-0.31466399999249944, gamma=0.09813358453997123\n",
      "Gradient Descent(1011/4999): loss=0.29313855647347586, w0=-0.3146639999924996, gamma=0.13680696096648381\n",
      "Gradient Descent(1012/4999): loss=0.2931385194712769, w0=-0.31466399999249983, gamma=0.766887757515996\n",
      "Gradient Descent(1013/4999): loss=0.29313846788948383, w0=-0.31466399999250083, gamma=0.7670650267445098\n",
      "Gradient Descent(1014/4999): loss=0.2931381787644439, w0=-0.3146639999925013, gamma=0.05930335871672536\n",
      "Gradient Descent(1015/4999): loss=0.29313788974564214, w0=-0.3146639999925013, gamma=0.04730792905904839\n",
      "Gradient Descent(1016/4999): loss=0.29313786728073804, w0=-0.3146639999925013, gamma=0.03559408684843075\n",
      "Gradient Descent(1017/4999): loss=0.2931378494409813, w0=-0.3146639999925013, gamma=0.03631158104821596\n",
      "Gradient Descent(1018/4999): loss=0.29313783602459714, w0=-0.3146639999925013, gamma=0.2529664363700511\n",
      "Gradient Descent(1019/4999): loss=0.2931378223392079, w0=-0.3146639999925014, gamma=0.7346439944144637\n",
      "Gradient Descent(1020/4999): loss=0.29313772700148233, w0=-0.3146639999925018, gamma=0.21090241788265326\n",
      "Gradient Descent(1021/4999): loss=0.293137450157235, w0=-0.3146639999925019, gamma=0.07806622250040723\n",
      "Gradient Descent(1022/4999): loss=0.29313737069996176, w0=-0.31466399999250194, gamma=0.07201597224344768\n",
      "Gradient Descent(1023/4999): loss=0.2931373412705929, w0=-0.314663999992502, gamma=0.04017139512022098\n",
      "Gradient Descent(1024/4999): loss=0.2931373141365269, w0=-0.314663999992502, gamma=0.038342427034832545\n",
      "Gradient Descent(1025/4999): loss=0.2931372990007909, w0=-0.314663999992502, gamma=0.1840214145658686\n",
      "Gradient Descent(1026/4999): loss=0.2931372845545503, w0=-0.31466399999250205, gamma=13.124317780974327\n",
      "Gradient Descent(1027/4999): loss=0.2931372152221398, w0=-0.31466399999250844, gamma=0.9296204228526037\n",
      "Gradient Descent(1028/4999): loss=0.2931322743163557, w0=-0.314663999992508, gamma=0.035399658702858744\n",
      "Gradient Descent(1029/4999): loss=0.2931320136490047, w0=-0.31466399999250805, gamma=0.0351527652723114\n",
      "Gradient Descent(1030/4999): loss=0.29313191201595, w0=-0.31466399999250805, gamma=0.055814741208344655\n",
      "Gradient Descent(1031/4999): loss=0.29313189852560684, w0=-0.31466399999250805, gamma=0.06039744338804766\n",
      "Gradient Descent(1032/4999): loss=0.29313187750328623, w0=-0.31466399999250805, gamma=0.10773673260722962\n",
      "Gradient Descent(1033/4999): loss=0.2931318548161549, w0=-0.3146639999925081, gamma=0.14550738108422898\n",
      "Gradient Descent(1034/4999): loss=0.29313181435222574, w0=-0.31466399999250816, gamma=2.7257895433952752\n",
      "Gradient Descent(1035/4999): loss=0.29313175970469546, w0=-0.3146639999925094, gamma=0.9273864250998296\n",
      "Gradient Descent(1036/4999): loss=0.293130736176204, w0=-0.3146639999925097, gamma=0.03605324331630309\n",
      "Gradient Descent(1037/4999): loss=0.2931303917093747, w0=-0.3146639999925097, gamma=0.03524631962855356\n",
      "Gradient Descent(1038/4999): loss=0.29313037461978836, w0=-0.3146639999925097, gamma=0.054898261838341025\n",
      "Gradient Descent(1039/4999): loss=0.29313036136374393, w0=-0.3146639999925097, gamma=0.07279081244567359\n",
      "Gradient Descent(1040/4999): loss=0.2931303407542943, w0=-0.31466399999250977, gamma=0.09956645631557688\n",
      "Gradient Descent(1041/4999): loss=0.29313031343893314, w0=-0.3146639999925098, gamma=0.4962842551483376\n",
      "Gradient Descent(1042/4999): loss=0.29313027607775693, w0=-0.31466399999251005, gamma=3.8328311065165774\n",
      "Gradient Descent(1043/4999): loss=0.29313008986122285, w0=-0.31466399999251154, gamma=0.0714349900794739\n",
      "Gradient Descent(1044/4999): loss=0.2931286524700421, w0=-0.3146639999925116, gamma=0.036034550947705586\n",
      "Gradient Descent(1045/4999): loss=0.29312862564874737, w0=-0.3146639999925116, gamma=0.03509047574469291\n",
      "Gradient Descent(1046/4999): loss=0.2931286118327029, w0=-0.3146639999925116, gamma=0.04933085037618439\n",
      "Gradient Descent(1047/4999): loss=0.2931285986773097, w0=-0.3146639999925116, gamma=0.12721544483730643\n",
      "Gradient Descent(1048/4999): loss=0.2931285801845034, w0=-0.31466399999251166, gamma=0.2862675349249683\n",
      "Gradient Descent(1049/4999): loss=0.29312853249650356, w0=-0.3146639999925118, gamma=2.162381303215627\n",
      "Gradient Descent(1050/4999): loss=0.29312842519065424, w0=-0.3146639999925128, gamma=0.2083534825944545\n",
      "Gradient Descent(1051/4999): loss=0.2931276148032767, w0=-0.31466399999251293, gamma=0.05831554611750098\n",
      "Gradient Descent(1052/4999): loss=0.2931275369404652, w0=-0.31466399999251293, gamma=0.056483595393046655\n",
      "Gradient Descent(1053/4999): loss=0.29312751486397737, w0=-0.31466399999251293, gamma=0.03720428054343388\n",
      "Gradient Descent(1054/4999): loss=0.29312749370197866, w0=-0.31466399999251293, gamma=0.036829620423200475\n",
      "Gradient Descent(1055/4999): loss=0.2931274797637442, w0=-0.31466399999251293, gamma=0.2785107055359871\n",
      "Gradient Descent(1056/4999): loss=0.2931274659666239, w0=-0.31466399999251304, gamma=1.9857700341236897\n",
      "Gradient Descent(1057/4999): loss=0.29312736163324904, w0=-0.3146639999925139, gamma=0.3710313665171317\n",
      "Gradient Descent(1058/4999): loss=0.2931266178741293, w0=-0.31466399999251404, gamma=0.06327870281366611\n",
      "Gradient Descent(1059/4999): loss=0.2931264792552, w0=-0.3146639999925141, gamma=0.043453580344907004\n",
      "Gradient Descent(1060/4999): loss=0.2931264553069201, w0=-0.3146639999925141, gamma=0.03574493987574719\n",
      "Gradient Descent(1061/4999): loss=0.2931264389768207, w0=-0.3146639999925141, gamma=0.03779220365460575\n",
      "Gradient Descent(1062/4999): loss=0.29312642558934154, w0=-0.3146639999925141, gamma=0.09597440331270124\n",
      "Gradient Descent(1063/4999): loss=0.29312641143980195, w0=-0.31466399999251415, gamma=0.13568517751286\n",
      "Gradient Descent(1064/4999): loss=0.29312637550818127, w0=-0.3146639999925142, gamma=1.4345846973151053\n",
      "Gradient Descent(1065/4999): loss=0.29312632471083194, w0=-0.3146639999925149, gamma=1.7918238211124236\n",
      "Gradient Descent(1066/4999): loss=0.29312578769135184, w0=-0.31466399999251554, gamma=0.046355025778359614\n",
      "Gradient Descent(1067/4999): loss=0.29312511847006195, w0=-0.31466399999251554, gamma=0.03707117445726339\n",
      "Gradient Descent(1068/4999): loss=0.2931250999918984, w0=-0.31466399999251554, gamma=0.038349402454762774\n",
      "Gradient Descent(1069/4999): loss=0.29312508597783116, w0=-0.31466399999251554, gamma=0.0743272598733271\n",
      "Gradient Descent(1070/4999): loss=0.2931250716139432, w0=-0.3146639999925156, gamma=0.08115065126547835\n",
      "Gradient Descent(1071/4999): loss=0.2931250438023152, w0=-0.31466399999251565, gamma=0.20364692541017476\n",
      "Gradient Descent(1072/4999): loss=0.2931250134448948, w0=-0.31466399999251576, gamma=0.18657561252372584\n",
      "Gradient Descent(1073/4999): loss=0.2931249372652496, w0=-0.3146639999925158, gamma=0.04158553198465384\n",
      "Gradient Descent(1074/4999): loss=0.29312486747688576, w0=-0.3146639999925158, gamma=0.035382258470445446\n",
      "Gradient Descent(1075/4999): loss=0.2931248519194326, w0=-0.3146639999925158, gamma=0.04503214141415602\n",
      "Gradient Descent(1076/4999): loss=0.29312483868475697, w0=-0.3146639999925158, gamma=85.89663355168098\n",
      "Gradient Descent(1077/4999): loss=0.2931248218408095, w0=-0.3146639999925672, gamma=744.0627976452771\n",
      "Gradient Descent(1078/4999): loss=0.29309284368952876, w0=-0.3146639999814301, gamma=0.03853782910244663\n",
      "Gradient Descent(1079/4999): loss=0.2950785640606011, w0=-0.31466399998187233, gamma=0.03550595987255359\n",
      "Gradient Descent(1080/4999): loss=0.29296023007085636, w0=-0.31466399998226163, gamma=0.04763143927160408\n",
      "Gradient Descent(1081/4999): loss=0.29287617613000183, w0=-0.31466399998276595, gamma=0.09755063046630259\n",
      "Gradient Descent(1082/4999): loss=0.2928435326974236, w0=-0.31466399998375, gamma=0.10132152499390105\n",
      "Gradient Descent(1083/4999): loss=0.29283078890969066, w0=-0.3146639999846729, gamma=0.08283618166767255\n",
      "Gradient Descent(1084/4999): loss=0.2928306088511229, w0=-0.314663999985351, gamma=0.03994753933809882\n",
      "Gradient Descent(1085/4999): loss=0.2928305797419937, w0=-0.3146639999856509, gamma=0.035349805191480704\n",
      "Gradient Descent(1086/4999): loss=0.29283055709133654, w0=-0.3146639999859057, gamma=0.037797841661838034\n",
      "Gradient Descent(1087/4999): loss=0.2928305457589342, w0=-0.31466399998616856, gamma=0.09581341259538154\n",
      "Gradient Descent(1088/4999): loss=0.2928305339261457, w0=-0.3146639999868096, gamma=0.1151384324610191\n",
      "Gradient Descent(1089/4999): loss=0.29283050402111516, w0=-0.31466399998750616, gamma=0.09171797081106763\n",
      "Gradient Descent(1090/4999): loss=0.2928304681327436, w0=-0.3146639999879971, gamma=0.06813748444258322\n",
      "Gradient Descent(1091/4999): loss=0.2928304395469542, w0=-0.3146639999883284, gamma=0.08012775014484042\n",
      "Gradient Descent(1092/4999): loss=0.29283041831101864, w0=-0.31466399998869143, gamma=0.20266656978090644\n",
      "Gradient Descent(1093/4999): loss=0.2928303933387175, w0=-0.3146639999895361, gamma=0.15418091989261973\n",
      "Gradient Descent(1094/4999): loss=0.2928303301781775, w0=-0.31466399999004846, gamma=0.04024091130282064\n",
      "Gradient Descent(1095/4999): loss=0.2928302821314218, w0=-0.3146639999901616, gamma=0.035498427605004844\n",
      "Gradient Descent(1096/4999): loss=0.29283026958969116, w0=-0.31466399999025735, gamma=0.05434998282427338\n",
      "Gradient Descent(1097/4999): loss=0.2928302585274685, w0=-0.3146639999903988, gamma=9.230332812152167\n",
      "Gradient Descent(1098/4999): loss=0.2928302415908104, w0=-0.3146640000131107, gamma=22.36760259588276\n",
      "Gradient Descent(1099/4999): loss=0.2928273665781927, w0=-0.31466399956021024, gamma=0.060875007512279214\n",
      "Gradient Descent(1100/4999): loss=0.29282050440547963, w0=-0.31466399958654834, gamma=0.035251890143248706\n",
      "Gradient Descent(1101/4999): loss=0.2928204539630691, w0=-0.31466399960087194, gamma=0.03511520850110983\n",
      "Gradient Descent(1102/4999): loss=0.2928204113547961, w0=-0.31466399961463704, gamma=0.49691316426419907\n",
      "Gradient Descent(1103/4999): loss=0.2928203980237367, w0=-0.3146639998025859, gamma=0.6759220427415908\n",
      "Gradient Descent(1104/4999): loss=0.29282022373271616, w0=-0.3146639999312035, gamma=0.13199816808124026\n",
      "Gradient Descent(1105/4999): loss=0.29282001370547955, w0=-0.31466399993934346, gamma=0.05817981616195835\n",
      "Gradient Descent(1106/4999): loss=0.2928199733164278, w0=-0.3146639999424577, gamma=0.05728420287141294\n",
      "Gradient Descent(1107/4999): loss=0.29281995389683657, w0=-0.3146639999453456, gamma=0.037620654438164786\n",
      "Gradient Descent(1108/4999): loss=0.29281993616005025, w0=-0.31466399994713357, gamma=0.03644514962634229\n",
      "Gradient Descent(1109/4999): loss=0.29281992451213545, w0=-0.31466399994880045, gamma=0.25630991517763674\n",
      "Gradient Descent(1110/4999): loss=0.29281991322875045, w0=-0.3146639999600962, gamma=4.520939613625596\n",
      "Gradient Descent(1111/4999): loss=0.2928198338769318, w0=-0.3146640001082698, gamma=1.7006717626394037\n",
      "Gradient Descent(1112/4999): loss=0.29281843460053814, w0=-0.3146639999120175, gamma=0.0421955618817174\n",
      "Gradient Descent(1113/4999): loss=0.2928179183059693, w0=-0.31466399991542926, gamma=0.036681653084361554\n",
      "Gradient Descent(1114/4999): loss=0.29281789645794043, w0=-0.31466399991827004, gamma=0.03983808140226131\n",
      "Gradient Descent(1115/4999): loss=0.29281788449161855, w0=-0.3146639999212421, gamma=0.10036326715678028\n",
      "Gradient Descent(1116/4999): loss=0.29281787201978776, w0=-0.31466399992843125, gamma=0.12120933554450154\n",
      "Gradient Descent(1117/4999): loss=0.29281784086614276, w0=-0.3146639999362422, gamma=0.08295244122751143\n",
      "Gradient Descent(1118/4999): loss=0.2928178033844642, w0=-0.3146639999409399, gamma=0.04472680777563946\n",
      "Gradient Descent(1119/4999): loss=0.29281777773646134, w0=-0.3146639999432627, gamma=0.038400855772309686\n",
      "Gradient Descent(1120/4999): loss=0.29281776390715725, w0=-0.3146639999451678, gamma=0.05489776187111529\n",
      "Gradient Descent(1121/4999): loss=0.29281775203440147, w0=-0.31466399994778677, gamma=1.36527693557993\n",
      "Gradient Descent(1122/4999): loss=0.2928177350613488, w0=-0.3146640000093428, gamma=4.154544791298225\n",
      "Gradient Descent(1123/4999): loss=0.2928173129845491, w0=-0.3146639999409226, gamma=0.03975814682807644\n",
      "Gradient Descent(1124/4999): loss=0.29281603121766914, w0=-0.31466399994298816, gamma=0.03504054092923677\n",
      "Gradient Descent(1125/4999): loss=0.2928160168938088, w0=-0.31466399994473626, gamma=0.03524646973088646\n",
      "Gradient Descent(1126/4999): loss=0.2928160060311016, w0=-0.314663999946433, gamma=0.19115823271408747\n",
      "Gradient Descent(1127/4999): loss=0.2928159951435353, w0=-0.31466399995531086, gamma=0.23184437072650582\n",
      "Gradient Descent(1128/4999): loss=0.2928159361015188, w0=-0.31466399996402006, gamma=0.30516106689948325\n",
      "Gradient Descent(1129/4999): loss=0.2928158645011719, w0=-0.3146639999728257, gamma=0.10386209782704564\n",
      "Gradient Descent(1130/4999): loss=0.2928157702667936, w0=-0.3146639999749081, gamma=0.05638856547163758\n",
      "Gradient Descent(1131/4999): loss=0.2928157381950216, w0=-0.3146639999759213, gamma=0.05585273449774061\n",
      "Gradient Descent(1132/4999): loss=0.2928157207820611, w0=-0.3146639999768683, gamma=0.782982839417642\n",
      "Gradient Descent(1133/4999): loss=0.2928157035359035, w0=-0.31466399998940203, gamma=13.121139491410906\n",
      "Gradient Descent(1134/4999): loss=0.2928154617788707, w0=-0.3146640000349856, gamma=0.5406383772658967\n",
      "Gradient Descent(1135/4999): loss=0.29281141382302367, w0=-0.3146640000122221, gamma=0.040234101445440484\n",
      "Gradient Descent(1136/4999): loss=0.2928112751983042, w0=-0.3146640000114439, gamma=0.036329402968991925\n",
      "Gradient Descent(1137/4999): loss=0.2928112362477386, w0=-0.3146640000107695, gamma=0.040953752661271624\n",
      "Gradient Descent(1138/4999): loss=0.2928112238794855, w0=-0.3146640000100369, gamma=0.0848870264417353\n",
      "Gradient Descent(1139/4999): loss=0.2928112110127534, w0=-0.3146640000085806, gamma=0.11125801934392567\n",
      "Gradient Descent(1140/4999): loss=0.29281118471748047, w0=-0.31466400000683387, gamma=0.10146530552489781\n",
      "Gradient Descent(1141/4999): loss=0.29281115044389594, w0=-0.31466400000541817, gamma=0.04806312326610837\n",
      "Gradient Descent(1142/4999): loss=0.2928111192039566, w0=-0.3146640000048156, gamma=0.039423096212537596\n",
      "Gradient Descent(1143/4999): loss=0.29281110440509284, w0=-0.3146640000043451, gamma=0.04282010765447761\n",
      "Gradient Descent(1144/4999): loss=0.2928110922674397, w0=-0.3146640000038542, gamma=0.44473162309262504\n",
      "Gradient Descent(1145/4999): loss=0.2928110790841146, w0=-0.31466399999897415, gamma=4.964782423029438\n",
      "Gradient Descent(1146/4999): loss=0.29281094216540676, w0=-0.31466399996872463, gamma=0.10329527237711193\n",
      "Gradient Descent(1147/4999): loss=0.29280941434549645, w0=-0.3146639999712201, gamma=0.0354050664833776\n",
      "Gradient Descent(1148/4999): loss=0.2928093830787137, w0=-0.3146639999719871, gamma=0.0350831431413725\n",
      "Gradient Descent(1149/4999): loss=0.2928093714798817, w0=-0.31466399997272015, gamma=0.05476342375178664\n",
      "Gradient Descent(1150/4999): loss=0.2928093606892151, w0=-0.3146639999738243, gamma=0.09364517794386504\n",
      "Gradient Descent(1151/4999): loss=0.29280934384687113, w0=-0.31466399997560907, gamma=0.39503617138394326\n",
      "Gradient Descent(1152/4999): loss=0.29280931504742075, w0=-0.31466399998243294, gamma=1.739721702077005\n",
      "Gradient Descent(1153/4999): loss=0.29280919356361496, w0=-0.3146640000006137, gamma=0.5876147513670072\n",
      "Gradient Descent(1154/4999): loss=0.29280865864029454, w0=-0.31466399999607153, gamma=0.07734419419998843\n",
      "Gradient Descent(1155/4999): loss=0.29280847821202555, w0=-0.314663999995825, gamma=0.05398949865234519\n",
      "Gradient Descent(1156/4999): loss=0.2928084542734307, w0=-0.31466399999566624, gamma=0.040950970895593186\n",
      "Gradient Descent(1157/4999): loss=0.2928084376433197, w0=-0.31466399999555233, gamma=0.03743529591735401\n",
      "Gradient Descent(1158/4999): loss=0.29280842504964083, w0=-0.31466399999545247, gamma=0.0708474457626526\n",
      "Gradient Descent(1159/4999): loss=0.29280841354195236, w0=-0.3146639999952705, gamma=0.1790229554964863\n",
      "Gradient Descent(1160/4999): loss=0.2928083917651366, w0=-0.3146639999948433, gamma=0.23240385787469597\n",
      "Gradient Descent(1161/4999): loss=0.2928083367418929, w0=-0.314663999994388, gamma=0.10301946260426816\n",
      "Gradient Descent(1162/4999): loss=0.29280826531561127, w0=-0.31466399999423306, gamma=0.03717413779353765\n",
      "Gradient Descent(1163/4999): loss=0.2928082336558679, w0=-0.31466399999418293, gamma=0.03559439284409141\n",
      "Gradient Descent(1164/4999): loss=0.29280822223033603, w0=-0.3146639999941367, gamma=0.1852344884354685\n",
      "Gradient Descent(1165/4999): loss=0.292808211291505, w0=-0.3146639999939047, gamma=230.4393617321154\n",
      "Gradient Descent(1166/4999): loss=0.2928081543662688, w0=-0.3146639997588058, gamma=30.4655032112155\n",
      "Gradient Descent(1167/4999): loss=0.2927381325339497, w0=-0.3146640068925751, gamma=0.03531939132003441\n",
      "Gradient Descent(1168/4999): loss=0.29340365890096803, w0=-0.314664006648884, gamma=0.035133176304380405\n",
      "Gradient Descent(1169/4999): loss=0.29273148530840126, w0=-0.31466400641504083, gamma=0.055307194130734164\n",
      "Gradient Descent(1170/4999): loss=0.29272971884421883, w0=-0.3146640060598544, gamma=0.06309205343565039\n",
      "Gradient Descent(1171/4999): loss=0.2927292655344964, w0=-0.3146640056770825, gamma=0.12632009712996164\n",
      "Gradient Descent(1172/4999): loss=0.292729169399156, w0=-0.3146640049590655, gamma=0.1331062182848051\n",
      "Gradient Descent(1173/4999): loss=0.29272910264739443, w0=-0.314664004298048, gamma=0.12298092218152877\n",
      "Gradient Descent(1174/4999): loss=0.2927290630776532, w0=-0.3146640037686061, gamma=0.05208194912178161\n",
      "Gradient Descent(1175/4999): loss=0.29272902704918335, w0=-0.31466400357196384, gamma=0.04011553425888561\n",
      "Gradient Descent(1176/4999): loss=0.2927290117500931, w0=-0.3146640034283908, gamma=0.038351162399795836\n",
      "Gradient Descent(1177/4999): loss=0.2927290000024036, w0=-0.3146640032966386, gamma=0.1404526494749931\n",
      "Gradient Descent(1178/4999): loss=0.2927289887802105, w0=-0.31466400283263024, gamma=0.5955108471099833\n",
      "Gradient Descent(1179/4999): loss=0.29272894769631885, w0=-0.3146640011415847, gamma=0.40089267492899033\n",
      "Gradient Descent(1180/4999): loss=0.2927287736727034, w0=-0.31466400068111555, gamma=0.03613480044556319\n",
      "Gradient Descent(1181/4999): loss=0.2927286569082957, w0=-0.3146640006562497, gamma=0.03521291733469882\n",
      "Gradient Descent(1182/4999): loss=0.2927286460438872, w0=-0.3146640006328939, gamma=0.05318111650067803\n",
      "Gradient Descent(1183/4999): loss=0.29272863575889985, w0=-0.31466400059886235, gamma=0.07705042186638457\n",
      "Gradient Descent(1184/4999): loss=0.2927286202286896, w0=-0.3146640005521785, gamma=0.20793797780045395\n",
      "Gradient Descent(1185/4999): loss=0.29272859772910526, w0=-0.314664000435899, gamma=9.357879103204418\n",
      "Gradient Descent(1186/4999): loss=0.29272853701029566, w0=-0.3146639962910736, gamma=2.8658649643274012\n",
      "Gradient Descent(1187/4999): loss=0.2927258057861891, w0=-0.3146640069002332, gamma=0.044553227347055274\n",
      "Gradient Descent(1188/4999): loss=0.29272503672744005, w0=-0.3146640065924928, gamma=0.03868332056129664\n",
      "Gradient Descent(1189/4999): loss=0.2927249630079646, w0=-0.3146640063372017, gamma=0.03918137247961552\n",
      "Gradient Descent(1190/4999): loss=0.292724946914191, w0=-0.3146640060886264, gamma=0.058050848994380365\n",
      "Gradient Descent(1191/4999): loss=0.29272493505379327, w0=-0.3146640057347689, gamma=0.07037906426648927\n",
      "Gradient Descent(1192/4999): loss=0.2927249180281575, w0=-0.31466400533066713, gamma=0.08581292565341384\n",
      "Gradient Descent(1193/4999): loss=0.29272489750123193, w0=-0.3146640048726245, gamma=0.08278083839489593\n",
      "Gradient Descent(1194/4999): loss=0.29272487248583046, w0=-0.3146640044686834, gamma=0.050841881489822355\n",
      "Gradient Descent(1195/4999): loss=0.29272484835997725, w0=-0.3146640042411302, gamma=0.041254474408168165\n",
      "Gradient Descent(1196/4999): loss=0.29272483354370293, w0=-0.314664004065875, gamma=0.06013227770820945\n",
      "Gradient Descent(1197/4999): loss=0.2927248215224524, w0=-0.31466400382096255, gamma=0.3836461950376348\n",
      "Gradient Descent(1198/4999): loss=0.29272480400144996, w0=-0.31466400235237146, gamma=0.48718912996190605\n",
      "Gradient Descent(1199/4999): loss=0.2927246922316782, w0=-0.31466400120290144, gamma=0.03942904038416778\n",
      "Gradient Descent(1200/4999): loss=0.29272455036208145, w0=-0.3146640011551954, gamma=0.03505135131099226\n",
      "Gradient Descent(1201/4999): loss=0.29272453884232835, w0=-0.31466400111445814, gamma=0.036185677807738285\n",
      "Gradient Descent(1202/4999): loss=0.29272452863317777, w0=-0.3146640010738767, gamma=2.1735618535874006\n",
      "Gradient Descent(1203/4999): loss=0.29272451809431577, w0=-0.3146639987244807, gamma=3.3269264207702833\n",
      "Gradient Descent(1204/4999): loss=0.29272388513071007, w0=-0.31466400294468694, gamma=0.18539919367899743\n",
      "Gradient Descent(1205/4999): loss=0.29272291768638253, w0=-0.3146640023974432, gamma=0.15874751015428376\n",
      "Gradient Descent(1206/4999): loss=0.2927228628308969, w0=-0.3146640020157409, gamma=0.1261986208546875\n",
      "Gradient Descent(1207/4999): loss=0.2927228166167792, w0=-0.3146640017604715, gamma=0.035955825362798365\n",
      "Gradient Descent(1208/4999): loss=0.29272277990936163, w0=-0.31466400169691994, gamma=0.03513828479323837\n",
      "Gradient Descent(1209/4999): loss=0.29272276943901143, w0=-0.3146640016370465, gamma=0.14808590161328591\n",
      "Gradient Descent(1210/4999): loss=0.29272275921646523, w0=-0.3146640013935837, gamma=1.3453432769789397\n",
      "Gradient Descent(1211/4999): loss=0.29272271613564727, w0=-0.3146639995092935, gamma=1.5804094586739208\n",
      "Gradient Descent(1212/4999): loss=0.29272232479536336, w0=-0.31466400027372, gamma=0.07903377899746657\n",
      "Gradient Descent(1213/4999): loss=0.2927218654818439, w0=-0.3146640002515323, gamma=0.06597499328644536\n",
      "Gradient Descent(1214/4999): loss=0.2927218422336291, w0=-0.3146640002344745, gamma=0.044903218998087385\n",
      "Gradient Descent(1215/4999): loss=0.2927218230407912, w0=-0.3146640002236307, gamma=0.03640383611695026\n",
      "Gradient Descent(1216/4999): loss=0.29272180998290737, w0=-0.3146640002152343, gamma=0.040905944798458334\n",
      "Gradient Descent(1217/4999): loss=0.2927217993985817, w0=-0.3146640002061429, gamma=0.6259429097765269\n",
      "Gradient Descent(1218/4999): loss=0.29272178750562705, w0=-0.314664000072717, gamma=1.1594688363150027\n",
      "Gradient Descent(1219/4999): loss=0.292721605529285, w0=-0.31466399998026817, gamma=0.18038035600154584\n",
      "Gradient Descent(1220/4999): loss=0.2927212685037746, w0=-0.3146639999825618, gamma=0.04253466410538168\n",
      "Gradient Descent(1221/4999): loss=0.2927212161295519, w0=-0.3146639999830051, gamma=0.037465656870048256\n",
      "Gradient Descent(1222/4999): loss=0.2927212037121937, w0=-0.31466399998337896, gamma=0.03860116148337744\n",
      "Gradient Descent(1223/4999): loss=0.2927211928209434, w0=-0.3146639999837497, gamma=0.07206941084904621\n",
      "Gradient Descent(1224/4999): loss=0.2927211816024984, w0=-0.3146639999844152, gamma=0.19064623512502615\n",
      "Gradient Descent(1225/4999): loss=0.29272116065787673, w0=-0.31466399998604877, gamma=1.4176867975632448\n",
      "Gradient Descent(1226/4999): loss=0.29272110525404593, w0=-0.3146639999958805, gamma=0.23171658961731725\n",
      "Gradient Descent(1227/4999): loss=0.29272069330427075, w0=-0.3146639999952094, gamma=0.039006153267018055\n",
      "Gradient Descent(1228/4999): loss=0.29272062610356353, w0=-0.31466399999512257, gamma=0.03651573521198366\n",
      "Gradient Descent(1229/4999): loss=0.29272061465126104, w0=-0.31466399999504446, gamma=0.041863091780694126\n",
      "Gradient Descent(1230/4999): loss=0.2927206040397844, w0=-0.3146639999949582, gamma=0.06242455669886659\n",
      "Gradient Descent(1231/4999): loss=0.292720591877979, w0=-0.31466399999483496, gamma=0.21328188917425328\n",
      "Gradient Descent(1232/4999): loss=0.29272057374328975, w0=-0.3146639999944403, gamma=4.422243326053612\n",
      "Gradient Descent(1233/4999): loss=0.2927205117849032, w0=-0.3146639999880023, gamma=0.3376101474962105\n",
      "Gradient Descent(1234/4999): loss=0.29271922746388007, w0=-0.3146639999896848, gamma=0.03831135539493327\n",
      "Gradient Descent(1235/4999): loss=0.2927191314110012, w0=-0.31466399998981126, gamma=0.03638516589860038\n",
      "Gradient Descent(1236/4999): loss=0.292719118367978, w0=-0.3146639999899268, gamma=0.043086866027029484\n",
      "Gradient Descent(1237/4999): loss=0.2927191077619253, w0=-0.3146639999900586, gamma=0.05425574393652925\n",
      "Gradient Descent(1238/4999): loss=0.292719095253165, w0=-0.31466399999021744, gamma=0.0669526745789623\n",
      "Gradient Descent(1239/4999): loss=0.2927190795065944, w0=-0.3146639999904028, gamma=1.6317820319733263\n",
      "Gradient Descent(1240/4999): loss=0.2927190600754224, w0=-0.31466399999461814, gamma=2.2155975086859456\n",
      "Gradient Descent(1241/4999): loss=0.29271858653670785, w0=-0.3146639999910033, gamma=0.03611778847615057\n",
      "Gradient Descent(1242/4999): loss=0.2927179459361181, w0=-0.31466399999107497, gamma=0.03504382016888764\n",
      "Gradient Descent(1243/4999): loss=0.29271793331691487, w0=-0.314663999991142, gamma=0.03685901829586659\n",
      "Gradient Descent(1244/4999): loss=0.2927179231512562, w0=-0.31466399999121, gamma=0.08475712296148095\n",
      "Gradient Descent(1245/4999): loss=0.2927179124617873, w0=-0.31466399999136063, gamma=0.23954103623086578\n",
      "Gradient Descent(1246/4999): loss=0.29271788788184666, w0=-0.3146639999917503, gamma=0.9948339347309534\n",
      "Gradient Descent(1247/4999): loss=0.29271781841557853, w0=-0.31466399999298106, gamma=0.1482751887982577\n",
      "Gradient Descent(1248/4999): loss=0.2927175299467243, w0=-0.31466399999298206, gamma=0.056587052251349954\n",
      "Gradient Descent(1249/4999): loss=0.2927174869691897, w0=-0.3146639999929824, gamma=0.05606434975721258\n",
      "Gradient Descent(1250/4999): loss=0.29271747054372405, w0=-0.3146639999929827, gamma=0.12572050281452196\n",
      "Gradient Descent(1251/4999): loss=0.2927174542894061, w0=-0.3146639999929834, gamma=5.771942757116573\n",
      "Gradient Descent(1252/4999): loss=0.2927174178407768, w0=-0.31466399999300965, gamma=1.64280336533331\n",
      "Gradient Descent(1253/4999): loss=0.2927157449423133, w0=-0.31466399999297706, gamma=0.03622705318442168\n",
      "Gradient Descent(1254/4999): loss=0.29271528853809065, w0=-0.31466399999297756, gamma=0.03540705442832979\n",
      "Gradient Descent(1255/4999): loss=0.2927152589727929, w0=-0.314663999992978, gamma=0.052015798037974\n",
      "Gradient Descent(1256/4999): loss=0.29271524851965935, w0=-0.31466399999297867, gamma=0.06462253835058625\n",
      "Gradient Descent(1257/4999): loss=0.2927152334172036, w0=-0.31466399999297945, gamma=0.07505977688643407\n",
      "Gradient Descent(1258/4999): loss=0.29271521470568745, w0=-0.3146639999929803, gamma=0.1917862531491319\n",
      "Gradient Descent(1259/4999): loss=0.292715192975721, w0=-0.3146639999929822, gamma=0.9113723087493579\n",
      "Gradient Descent(1260/4999): loss=0.2927151374545144, w0=-0.3146639999929897, gamma=0.09057642759706301\n",
      "Gradient Descent(1261/4999): loss=0.29271487364206417, w0=-0.31466399999298983, gamma=0.03523081764573955\n",
      "Gradient Descent(1262/4999): loss=0.2927148474357448, w0=-0.3146639999929899, gamma=0.035085388787725665\n",
      "Gradient Descent(1263/4999): loss=0.2927148372222717, w0=-0.31466399999298994, gamma=0.29602261012761566\n",
      "Gradient Descent(1264/4999): loss=0.29271482706747926, w0=-0.31466399999299016, gamma=2.3447351892486337\n",
      "Gradient Descent(1265/4999): loss=0.2927147413911173, w0=-0.31466399999299205, gamma=0.18334954091148536\n",
      "Gradient Descent(1266/4999): loss=0.29271406289812235, w0=-0.314663999992992, gamma=0.05412591142970373\n",
      "Gradient Descent(1267/4999): loss=0.292714009997562, w0=-0.314663999992992, gamma=0.05391814254861528\n",
      "Gradient Descent(1268/4999): loss=0.2927139941711169, w0=-0.314663999992992, gamma=0.08529211056793279\n",
      "Gradient Descent(1269/4999): loss=0.292713978573914, w0=-0.314663999992992, gamma=0.5284529817144578\n",
      "Gradient Descent(1270/4999): loss=0.2927139539014949, w0=-0.3146639999929919, gamma=0.6287693979886884\n",
      "Gradient Descent(1271/4999): loss=0.2927138010419144, w0=-0.31466399999299194, gamma=0.03830586265163549\n",
      "Gradient Descent(1272/4999): loss=0.2927136192380883, w0=-0.31466399999299194, gamma=0.035097681747098736\n",
      "Gradient Descent(1273/4999): loss=0.29271360810570457, w0=-0.31466399999299194, gamma=0.037271556202248125\n",
      "Gradient Descent(1274/4999): loss=0.29271359795485313, w0=-0.31466399999299194, gamma=0.22882314013095628\n",
      "Gradient Descent(1275/4999): loss=0.29271358717592627, w0=-0.314663999992992, gamma=0.8515588790592162\n",
      "Gradient Descent(1276/4999): loss=0.2927135210015265, w0=-0.31466399999299227, gamma=0.21485932970813548\n",
      "Gradient Descent(1277/4999): loss=0.29271327475675135, w0=-0.3146639999929923, gamma=0.06768846960494981\n",
      "Gradient Descent(1278/4999): loss=0.2927132126474409, w0=-0.3146639999929923, gamma=0.0642827550306008\n",
      "Gradient Descent(1279/4999): loss=0.2927131930589276, w0=-0.3146639999929923, gamma=0.06829300788707897\n",
      "Gradient Descent(1280/4999): loss=0.29271317447301, w0=-0.3146639999929923, gamma=0.269373917079444\n",
      "Gradient Descent(1281/4999): loss=0.292713154728143, w0=-0.31466399999299244, gamma=0.38616860565137845\n",
      "Gradient Descent(1282/4999): loss=0.2927130768485954, w0=-0.31466399999299255, gamma=0.04304682456676911\n",
      "Gradient Descent(1283/4999): loss=0.2927129652155677, w0=-0.31466399999299255, gamma=0.03514225699919514\n",
      "Gradient Descent(1284/4999): loss=0.29271295276456266, w0=-0.31466399999299255, gamma=0.03657979453215506\n",
      "Gradient Descent(1285/4999): loss=0.2927129426052907, w0=-0.31466399999299255, gamma=5.658722067804566\n",
      "Gradient Descent(1286/4999): loss=0.29271293203091203, w0=-0.314663999992995, gamma=41.83820065375174\n",
      "Gradient Descent(1287/4999): loss=0.2927112966647928, w0=-0.31466399999298744, gamma=0.06299196606753207\n",
      "Gradient Descent(1288/4999): loss=0.2926995372339165, w0=-0.3146639999929888, gamma=0.05793693403579439\n",
      "Gradient Descent(1289/4999): loss=0.292699223858435, w0=-0.3146639999929899, gamma=0.05386045804560198\n",
      "Gradient Descent(1290/4999): loss=0.2926992040319334, w0=-0.3146639999929909, gamma=0.037225895199562924\n",
      "Gradient Descent(1291/4999): loss=0.29269918850713456, w0=-0.31466399999299155, gamma=0.03583246761663652\n",
      "Gradient Descent(1292/4999): loss=0.29269917781671045, w0=-0.31466399999299216, gamma=0.06387115104890706\n",
      "Gradient Descent(1293/4999): loss=0.29269916754914416, w0=-0.3146639999929932, gamma=0.09984575740154988\n",
      "Gradient Descent(1294/4999): loss=0.29269914924879953, w0=-0.3146639999929947, gamma=0.4158971037932984\n",
      "Gradient Descent(1295/4999): loss=0.29269912064203074, w0=-0.31466399999300043, gamma=2.9860773231642526\n",
      "Gradient Descent(1296/4999): loss=0.29269900148764444, w0=-0.31466399999302486, gamma=0.08246706702797474\n",
      "Gradient Descent(1297/4999): loss=0.29269814625362295, w0=-0.3146639999930236, gamma=0.03601050549297603\n",
      "Gradient Descent(1298/4999): loss=0.2926981226894804, w0=-0.3146639999930231, gamma=0.0350404219574676\n",
      "Gradient Descent(1299/4999): loss=0.292698112222467, w0=-0.31466399999302264, gamma=0.04220018051688288\n",
      "Gradient Descent(1300/4999): loss=0.2926981021895634, w0=-0.3146639999930221, gamma=0.4004912192014795\n",
      "Gradient Descent(1301/4999): loss=0.2926980901069306, w0=-0.3146639999930171, gamma=0.8433228916382194\n",
      "Gradient Descent(1302/4999): loss=0.2926979754428142, w0=-0.3146639999930109, gamma=0.22986136057407522\n",
      "Gradient Descent(1303/4999): loss=0.2926977340211022, w0=-0.3146639999930107, gamma=0.09934016212867762\n",
      "Gradient Descent(1304/4999): loss=0.29269766822738935, w0=-0.31466399999301065, gamma=0.08130787548030516\n",
      "Gradient Descent(1305/4999): loss=0.2926976397839731, w0=-0.3146639999930106, gamma=0.08085396724397269\n",
      "Gradient Descent(1306/4999): loss=0.2926976165104699, w0=-0.31466399999301053, gamma=0.145545852593833\n",
      "Gradient Descent(1307/4999): loss=0.29269759336797185, w0=-0.3146639999930105, gamma=0.13446110557799557\n",
      "Gradient Descent(1308/4999): loss=0.29269755170982564, w0=-0.3146639999930104, gamma=0.045820384881691295\n",
      "Gradient Descent(1309/4999): loss=0.29269751322571297, w0=-0.3146639999930104, gamma=0.03644874354744978\n",
      "Gradient Descent(1310/4999): loss=0.2926975001112021, w0=-0.3146639999930104, gamma=0.04891893129390443\n",
      "Gradient Descent(1311/4999): loss=0.292697489679363, w0=-0.3146639999930104, gamma=7.724719897265853\n",
      "Gradient Descent(1312/4999): loss=0.2926974756786427, w0=-0.3146639999930095, gamma=53.02709744696912\n",
      "Gradient Descent(1313/4999): loss=0.2926952656505939, w0=-0.3146639999931882, gamma=0.03623140765915284\n",
      "Gradient Descent(1314/4999): loss=0.29268131829287436, w0=-0.3146639999931824, gamma=0.03506221450464965\n",
      "Gradient Descent(1315/4999): loss=0.29268014028858996, w0=-0.31466399999317707, gamma=0.038257751334157895\n",
      "Gradient Descent(1316/4999): loss=0.2926801281400255, w0=-0.31466399999317146, gamma=0.06110205685489847\n",
      "Gradient Descent(1317/4999): loss=0.2926801171517117, w0=-0.3146639999931628, gamma=0.06473204877095029\n",
      "Gradient Descent(1318/4999): loss=0.2926800998290826, w0=-0.3146639999931542, gamma=0.0827209006402455\n",
      "Gradient Descent(1319/4999): loss=0.2926800815095394, w0=-0.31466399999314393, gamma=0.23658706539140947\n",
      "Gradient Descent(1320/4999): loss=0.29268005810050396, w0=-0.31466399999311695, gamma=1.0701039943767054\n",
      "Gradient Descent(1321/4999): loss=0.29267999115104626, w0=-0.3146639999930238, gamma=0.26376562220180994\n",
      "Gradient Descent(1322/4999): loss=0.2926796883613755, w0=-0.3146639999930255, gamma=0.05227882948563044\n",
      "Gradient Descent(1323/4999): loss=0.2926796137876698, w0=-0.3146639999930258, gamma=0.04657454066708293\n",
      "Gradient Descent(1324/4999): loss=0.2926795989476591, w0=-0.314663999993026, gamma=0.03587745619212608\n",
      "Gradient Descent(1325/4999): loss=0.29267958576983116, w0=-0.3146639999930262, gamma=0.03893041585937483\n",
      "Gradient Descent(1326/4999): loss=0.2926795756199723, w0=-0.31466399999302636, gamma=0.45258912042630556\n",
      "Gradient Descent(1327/4999): loss=0.29267956460675376, w0=-0.3146639999930282, gamma=0.9850445033736253\n",
      "Gradient Descent(1328/4999): loss=0.2926794365754596, w0=-0.3146639999930306, gamma=0.20515039509956856\n",
      "Gradient Descent(1329/4999): loss=0.29267915796009114, w0=-0.31466399999303063, gamma=0.07896390629474052\n",
      "Gradient Descent(1330/4999): loss=0.2926790999443357, w0=-0.31466399999303063, gamma=0.043565673510245165\n",
      "Gradient Descent(1331/4999): loss=0.29267907760460926, w0=-0.31466399999303063, gamma=0.03555546184983106\n",
      "Gradient Descent(1332/4999): loss=0.2926790652781265, w0=-0.31466399999303063, gamma=0.03805923546261968\n",
      "Gradient Descent(1333/4999): loss=0.2926790552225661, w0=-0.31466399999303063, gamma=0.29354686218806847\n",
      "Gradient Descent(1334/4999): loss=0.2926790444594107, w0=-0.31466399999303074, gamma=0.5807242342265378\n",
      "Gradient Descent(1335/4999): loss=0.2926789614463747, w0=-0.31466399999303096, gamma=0.26892005384608875\n",
      "Gradient Descent(1336/4999): loss=0.29267879723409085, w0=-0.3146639999930311, gamma=0.0771998243965137\n",
      "Gradient Descent(1337/4999): loss=0.2926787212031908, w0=-0.3146639999930311, gamma=0.055352642138544926\n",
      "Gradient Descent(1338/4999): loss=0.29267869936976454, w0=-0.3146639999930311, gamma=0.041227604917832186\n",
      "Gradient Descent(1339/4999): loss=0.292678683718696, w0=-0.3146639999930311, gamma=0.03930823430218278\n",
      "Gradient Descent(1340/4999): loss=0.29267867206220094, w0=-0.3146639999930311, gamma=0.14119666992179467\n",
      "Gradient Descent(1341/4999): loss=0.29267866094863104, w0=-0.31466399999303113, gamma=1.4329861240427504\n",
      "Gradient Descent(1342/4999): loss=0.2926786210288077, w0=-0.31466399999303163, gamma=0.819608658994653\n",
      "Gradient Descent(1343/4999): loss=0.29267821592368454, w0=-0.31466399999303185, gamma=0.03562801130107534\n",
      "Gradient Descent(1344/4999): loss=0.2926779848361129, w0=-0.31466399999303185, gamma=0.03505092446895711\n",
      "Gradient Descent(1345/4999): loss=0.29267797420530994, w0=-0.31466399999303185, gamma=0.04753961079411765\n",
      "Gradient Descent(1346/4999): loss=0.29267796429929854, w0=-0.31466399999303185, gamma=0.12291565647313689\n",
      "Gradient Descent(1347/4999): loss=0.2926779508644936, w0=-0.3146639999930319, gamma=0.32977743870182435\n",
      "Gradient Descent(1348/4999): loss=0.29267791612908206, w0=-0.314663999993032, gamma=1.426124849149947\n",
      "Gradient Descent(1349/4999): loss=0.2926778229387347, w0=-0.3146639999930326, gamma=0.13803962419808483\n",
      "Gradient Descent(1350/4999): loss=0.2926774200006055, w0=-0.31466399999303263, gamma=0.060682089709215334\n",
      "Gradient Descent(1351/4999): loss=0.2926773810159036, w0=-0.31466399999303263, gamma=0.058515327733398216\n",
      "Gradient Descent(1352/4999): loss=0.29267736384322635, w0=-0.31466399999303263, gamma=0.062069564264881694\n",
      "Gradient Descent(1353/4999): loss=0.29267734731306855, w0=-0.31466399999303263, gamma=0.09525714972627393\n",
      "Gradient Descent(1354/4999): loss=0.2926773297793508, w0=-0.3146639999930327, gamma=0.14000596673906265\n",
      "Gradient Descent(1355/4999): loss=0.29267730287101235, w0=-0.31466399999303274, gamma=0.07063185306019451\n",
      "Gradient Descent(1356/4999): loss=0.2926772633228643, w0=-0.31466399999303274, gamma=0.03899769314445694\n",
      "Gradient Descent(1357/4999): loss=0.292677243371541, w0=-0.31466399999303274, gamma=0.03887125358758035\n",
      "Gradient Descent(1358/4999): loss=0.29267723235587473, w0=-0.31466399999303274, gamma=0.33459522685727\n",
      "Gradient Descent(1359/4999): loss=0.2926772213761431, w0=-0.31466399999303285, gamma=24.876452669798564\n",
      "Gradient Descent(1360/4999): loss=0.2926771268669949, w0=-0.31466399999304057, gamma=0.37606329931575455\n",
      "Gradient Descent(1361/4999): loss=0.2926701094711686, w0=-0.31466399999304073, gamma=0.03503307375623616\n",
      "Gradient Descent(1362/4999): loss=0.29267008073136164, w0=-0.31466399999304073, gamma=0.035032123702606874\n",
      "Gradient Descent(1363/4999): loss=0.29266999306955327, w0=-0.31466399999304073, gamma=0.059982156795675534\n",
      "Gradient Descent(1364/4999): loss=0.2926699832204467, w0=-0.31466399999304073, gamma=0.10597361178380596\n",
      "Gradient Descent(1365/4999): loss=0.2926699663573279, w0=-0.3146639999930408, gamma=3.7308334251835293\n",
      "Gradient Descent(1366/4999): loss=0.29266993656483076, w0=-0.3146639999930425, gamma=3.975903387679276\n",
      "Gradient Descent(1367/4999): loss=0.292668887907351, w0=-0.3146639999930421, gamma=0.05449976042143594\n",
      "Gradient Descent(1368/4999): loss=0.2926677838245365, w0=-0.3146639999930422, gamma=0.05364813407915058\n",
      "Gradient Descent(1369/4999): loss=0.29266775584119975, w0=-0.31466399999304223, gamma=0.057808891857327974\n",
      "Gradient Descent(1370/4999): loss=0.2926677407754489, w0=-0.31466399999304234, gamma=0.1087978212074581\n",
      "Gradient Descent(1371/4999): loss=0.2926677245469064, w0=-0.3146639999930425, gamma=0.8935753715487551\n",
      "Gradient Descent(1372/4999): loss=0.2926676940050134, w0=-0.3146639999930436, gamma=3.535897511809756\n",
      "Gradient Descent(1373/4999): loss=0.29266744317335674, w0=-0.3146639999930449, gamma=0.09981341689272535\n",
      "Gradient Descent(1374/4999): loss=0.292666451317977, w0=-0.31466399999304495, gamma=0.07213508813659257\n",
      "Gradient Descent(1375/4999): loss=0.292666422970845, w0=-0.31466399999304495, gamma=0.0633300106149034\n",
      "Gradient Descent(1376/4999): loss=0.2926664026808814, w0=-0.31466399999304495, gamma=0.03528939472216442\n",
      "Gradient Descent(1377/4999): loss=0.29266638491717645, w0=-0.31466399999304495, gamma=0.035396384498430584\n",
      "Gradient Descent(1378/4999): loss=0.2926663750177163, w0=-0.31466399999304495, gamma=3.134573667334917\n",
      "Gradient Descent(1379/4999): loss=0.29266636508987715, w0=-0.314663999993046, gamma=9.78602244394747\n",
      "Gradient Descent(1380/4999): loss=0.292665486049101, w0=-0.3146639999930481, gamma=0.17668603695968027\n",
      "Gradient Descent(1381/4999): loss=0.29266275121402746, w0=-0.3146639999930483, gamma=0.13493812315985662\n",
      "Gradient Descent(1382/4999): loss=0.2926626952107054, w0=-0.31466399999304845, gamma=0.04588615344107388\n",
      "Gradient Descent(1383/4999): loss=0.29266265794287727, w0=-0.3146639999930485, gamma=0.03675464471152849\n",
      "Gradient Descent(1384/4999): loss=0.2926626441698326, w0=-0.31466399999304856, gamma=0.036286677482924246\n",
      "Gradient Descent(1385/4999): loss=0.29266263379892005, w0=-0.3146639999930486, gamma=0.06327365243688408\n",
      "Gradient Descent(1386/4999): loss=0.2926626236422333, w0=-0.31466399999304867, gamma=0.07822215042180489\n",
      "Gradient Descent(1387/4999): loss=0.29266260593748666, w0=-0.3146639999930487, gamma=0.23577510048345154\n",
      "Gradient Descent(1388/4999): loss=0.29266258405176004, w0=-0.3146639999930489, gamma=0.9155688709577263\n",
      "Gradient Descent(1389/4999): loss=0.2926625180861573, w0=-0.3146639999930495, gamma=0.1067593238693368\n",
      "Gradient Descent(1390/4999): loss=0.2926622619510552, w0=-0.31466399999304956, gamma=0.036064923012641865\n",
      "Gradient Descent(1391/4999): loss=0.2926622321018871, w0=-0.31466399999304956, gamma=0.035252451266062815\n",
      "Gradient Descent(1392/4999): loss=0.29266222199432784, w0=-0.31466399999304956, gamma=0.06068037415045504\n",
      "Gradient Descent(1393/4999): loss=0.2926622121334793, w0=-0.31466399999304956, gamma=0.26757147202526005\n",
      "Gradient Descent(1394/4999): loss=0.2926621951601761, w0=-0.3146639999930496, gamma=7.537810220379673\n",
      "Gradient Descent(1395/4999): loss=0.2926621203176094, w0=-0.3146639999930527, gamma=0.8196350905382209\n",
      "Gradient Descent(1396/4999): loss=0.292660012765561, w0=-0.31466399999305245, gamma=0.05331879328214464\n",
      "Gradient Descent(1397/4999): loss=0.29265979387929375, w0=-0.31466399999305245, gamma=0.051637678474315746\n",
      "Gradient Descent(1398/4999): loss=0.2926597690307315, w0=-0.31466399999305245, gamma=0.036112664064337635\n",
      "Gradient Descent(1399/4999): loss=0.29265975450624293, w0=-0.31466399999305245, gamma=0.03600945840758959\n",
      "Gradient Descent(1400/4999): loss=0.29265974439410886, w0=-0.31466399999305245, gamma=0.11407693201330774\n",
      "Gradient Descent(1401/4999): loss=0.2926597343350516, w0=-0.3146639999930525, gamma=0.14614452742512368\n",
      "Gradient Descent(1402/4999): loss=0.292659702474095, w0=-0.3146639999930525, gamma=0.2029500299020319\n",
      "Gradient Descent(1403/4999): loss=0.29265966166180907, w0=-0.31466399999305256, gamma=0.19903762149264462\n",
      "Gradient Descent(1404/4999): loss=0.29265960498809707, w0=-0.3146639999930526, gamma=0.06706400925063616\n",
      "Gradient Descent(1405/4999): loss=0.29265954940947103, w0=-0.3146639999930526, gamma=0.03894944564985176\n",
      "Gradient Descent(1406/4999): loss=0.292659530682752, w0=-0.3146639999930526, gamma=0.03663366633112211\n",
      "Gradient Descent(1407/4999): loss=0.29265951980662486, w0=-0.3146639999930526, gamma=0.16065587842967888\n",
      "Gradient Descent(1408/4999): loss=0.2926595095775376, w0=-0.31466399999305267, gamma=28.794553180647192\n",
      "Gradient Descent(1409/4999): loss=0.29265946471874577, w0=-0.31466399999306227, gamma=4.20638222768661\n",
      "Gradient Descent(1410/4999): loss=0.2926514354279362, w0=-0.3146639999930608, gamma=0.03611967236855239\n",
      "Gradient Descent(1411/4999): loss=0.29265144456472886, w0=-0.3146639999930609, gamma=0.035292453229512126\n",
      "Gradient Descent(1412/4999): loss=0.29265027493169504, w0=-0.31466399999306094, gamma=0.05857289219414895\n",
      "Gradient Descent(1413/4999): loss=0.29265025173027065, w0=-0.31466399999306105, gamma=0.07211937818765227\n",
      "Gradient Descent(1414/4999): loss=0.2926502312845201, w0=-0.3146639999930612, gamma=0.07248461456624362\n",
      "Gradient Descent(1415/4999): loss=0.2926502111074664, w0=-0.3146639999930614, gamma=0.055363389463390214\n",
      "Gradient Descent(1416/4999): loss=0.2926501909904216, w0=-0.3146639999930615, gamma=0.05132829658814833\n",
      "Gradient Descent(1417/4999): loss=0.29265017562534085, w0=-0.3146639999930616, gamma=0.08213866777117423\n",
      "Gradient Descent(1418/4999): loss=0.29265016138028616, w0=-0.31466399999306177, gamma=0.2507630927757171\n",
      "Gradient Descent(1419/4999): loss=0.2926501385847437, w0=-0.3146639999930622, gamma=0.1548758399804882\n",
      "Gradient Descent(1420/4999): loss=0.2926500689934196, w0=-0.31466399999306244, gamma=0.03819355824779864\n",
      "Gradient Descent(1421/4999): loss=0.2926500260160471, w0=-0.3146639999930625, gamma=0.03530198593210278\n",
      "Gradient Descent(1422/4999): loss=0.29265001541530616, w0=-0.31466399999306255, gamma=0.06818541448516746\n",
      "Gradient Descent(1423/4999): loss=0.29265000561901483, w0=-0.3146639999930626, gamma=497.15380662181155\n",
      "Gradient Descent(1424/4999): loss=0.29264998669776143, w0=-0.31466399999356565, gamma=563.785672290476\n",
      "Gradient Descent(1425/4999): loss=0.29251513561102027, w0=-0.31466399979766213, gamma=0.03509079141332171\n",
      "Gradient Descent(1426/4999): loss=0.33770739996615795, w0=-0.31466399980451715, gamma=0.03503651112903833\n",
      "Gradient Descent(1427/4999): loss=0.2924276890288427, w0=-0.31466399981113385, gamma=0.11366611862351761\n",
      "Gradient Descent(1428/4999): loss=0.29240235892989, w0=-0.31466399983184806, gamma=0.12365070260910797\n",
      "Gradient Descent(1429/4999): loss=0.29237583492622493, w0=-0.3146639998518212, gamma=0.08942124856393938\n",
      "Gradient Descent(1430/4999): loss=0.2923755955509829, w0=-0.31466399986447935, gamma=0.053723908556507334\n",
      "Gradient Descent(1431/4999): loss=0.2923755579584675, w0=-0.31466399987140425, gamma=0.05365747193833931\n",
      "Gradient Descent(1432/4999): loss=0.29237553230117025, w0=-0.314663999877949, gamma=0.2927314281371168\n",
      "Gradient Descent(1433/4999): loss=0.29237551984259597, w0=-0.31466399991173855, gamma=0.405090516991519\n",
      "Gradient Descent(1434/4999): loss=0.2923754520864113, w0=-0.3146639999448097, gamma=0.05445338159552949\n",
      "Gradient Descent(1435/4999): loss=0.2923753587635758, w0=-0.31466399994745436, gamma=0.035373400083355606\n",
      "Gradient Descent(1436/4999): loss=0.2923753461542892, w0=-0.31466399994907884, gamma=0.03534969064837943\n",
      "Gradient Descent(1437/4999): loss=0.2923753379789136, w0=-0.3146639999506448, gamma=0.14409698406087887\n",
      "Gradient Descent(1438/4999): loss=0.2923753298351533, w0=-0.3146639999568025, gamma=0.19102916695082622\n",
      "Gradient Descent(1439/4999): loss=0.2923752966420361, w0=-0.3146639999637894, gamma=0.4307736413310586\n",
      "Gradient Descent(1440/4999): loss=0.292375252642634, w0=-0.31466399997653527, gamma=0.4341743839568279\n",
      "Gradient Descent(1441/4999): loss=0.29237515343344445, w0=-0.31466399998384786, gamma=0.10417128572930057\n",
      "Gradient Descent(1442/4999): loss=0.2923750534556847, w0=-0.3146639999848406, gamma=0.05055865047298289\n",
      "Gradient Descent(1443/4999): loss=0.2923750294658273, w0=-0.3146639999852723, gamma=0.035499856059799155\n",
      "Gradient Descent(1444/4999): loss=0.29237501782183, w0=-0.31466399998556005, gamma=0.035901540395659834\n",
      "Gradient Descent(1445/4999): loss=0.29237500964703583, w0=-0.3146639999858407, gamma=1.4401477276429855\n",
      "Gradient Descent(1446/4999): loss=0.2923750013802785, w0=-0.3146639999966956, gamma=7.2683885049556425\n",
      "Gradient Descent(1447/4999): loss=0.29237466978949184, w0=-0.31466399997258493, gamma=0.09693829319591277\n",
      "Gradient Descent(1448/4999): loss=0.2923729984471133, w0=-0.3146639999746008, gamma=0.06849199446674409\n",
      "Gradient Descent(1449/4999): loss=0.2923729749582441, w0=-0.31466399997588707, gamma=0.054814027531876544\n",
      "Gradient Descent(1450/4999): loss=0.2923729589925331, w0=-0.31466399997684597, gamma=0.03795735423433122\n",
      "Gradient Descent(1451/4999): loss=0.2923729463741238, w0=-0.3146639999774736, gamma=0.03541177163912036\n",
      "Gradient Descent(1452/4999): loss=0.29237293764250477, w0=-0.31466399997803685, gamma=0.06471499788483724\n",
      "Gradient Descent(1453/4999): loss=0.2923729294996447, w0=-0.3146639999790298, gamma=0.4295892224771739\n",
      "Gradient Descent(1454/4999): loss=0.29237291461881737, w0=-0.3146639999851946, gamma=1.3429013068182762\n",
      "Gradient Descent(1455/4999): loss=0.29237281584097147, w0=-0.3146639999961874, gamma=0.33639852778990126\n",
      "Gradient Descent(1456/4999): loss=0.2923725071018124, w0=-0.31466399999524325, gamma=0.05976623946650837\n",
      "Gradient Descent(1457/4999): loss=0.29237242981935796, w0=-0.31466399999513195, gamma=0.03800297345117317\n",
      "Gradient Descent(1458/4999): loss=0.2923724160466282, w0=-0.3146639999950654, gamma=0.035798906911339426\n",
      "Gradient Descent(1459/4999): loss=0.2923724072965469, w0=-0.3146639999950051, gamma=0.06514566957181901\n",
      "Gradient Descent(1460/4999): loss=0.292372399066893, w0=-0.3146639999948993, gamma=0.13111964242967852\n",
      "Gradient Descent(1461/4999): loss=0.29237238409210414, w0=-0.3146639999947002, gamma=0.21316922832283455\n",
      "Gradient Descent(1462/4999): loss=0.2923723539537048, w0=-0.31466399999441896, gamma=1.1951435682876956\n",
      "Gradient Descent(1463/4999): loss=0.29237230495734146, w0=-0.31466399999317846, gamma=0.09599697072247355\n",
      "Gradient Descent(1464/4999): loss=0.2923720302838106, w0=-0.31466399999319794, gamma=0.03536452792370544\n",
      "Gradient Descent(1465/4999): loss=0.2923720082373114, w0=-0.31466399999320444, gamma=0.03505456997618091\n",
      "Gradient Descent(1466/4999): loss=0.2923720000899365, w0=-0.31466399999321065, gamma=0.2014264046796098\n",
      "Gradient Descent(1467/4999): loss=0.29237199203460407, w0=-0.314663999993245, gamma=2.198984682216441\n",
      "Gradient Descent(1468/4999): loss=0.2923719457486019, w0=-0.31466399999354483, gamma=0.9558793505734273\n",
      "Gradient Descent(1469/4999): loss=0.2923714404999475, w0=-0.3146639999933892, gamma=0.07707444709396555\n",
      "Gradient Descent(1470/4999): loss=0.29237122139137156, w0=-0.3146639999933886, gamma=0.0732235814086677\n",
      "Gradient Descent(1471/4999): loss=0.2923712032377955, w0=-0.3146639999933881, gamma=0.05765761900682576\n",
      "Gradient Descent(1472/4999): loss=0.2923711864135623, w0=-0.3146639999933878, gamma=0.050581913025370295\n",
      "Gradient Descent(1473/4999): loss=0.29237117317091954, w0=-0.3146639999933875, gamma=0.047040029101083916\n",
      "Gradient Descent(1474/4999): loss=0.2923711615539747, w0=-0.31466399999338723, gamma=0.09399773168858434\n",
      "Gradient Descent(1475/4999): loss=0.2923711507505859, w0=-0.3146639999933868, gamma=0.5207530275179472\n",
      "Gradient Descent(1476/4999): loss=0.292371129162928, w0=-0.3146639999933844, gamma=0.21112420403608143\n",
      "Gradient Descent(1477/4999): loss=0.2923710095701598, w0=-0.31466399999338396, gamma=0.03595171734180892\n",
      "Gradient Descent(1478/4999): loss=0.2923709610980559, w0=-0.3146639999933839, gamma=0.035068263028148215\n",
      "Gradient Descent(1479/4999): loss=0.2923709528318136, w0=-0.31466399999338385, gamma=0.08764614381107336\n",
      "Gradient Descent(1480/4999): loss=0.2923709447790359, w0=-0.31466399999338374, gamma=194.1006511568107\n",
      "Gradient Descent(1481/4999): loss=0.29237092465288983, w0=-0.31466399999316563, gamma=1558.3927015078668\n",
      "Gradient Descent(1482/4999): loss=0.2923266895364247, w0=-0.31466400040789144, gamma=0.07423256454072494\n",
      "Gradient Descent(1483/4999): loss=0.29593114259076886, w0=-0.3146640003771616, gamma=0.039938843907506995\n",
      "Gradient Descent(1484/4999): loss=0.29383798032183545, w0=-0.31466400036184966, gamma=0.03572412249051708\n",
      "Gradient Descent(1485/4999): loss=0.29226257648921256, w0=-0.3146640003487028, gamma=0.049219350915186616\n",
      "Gradient Descent(1486/4999): loss=0.2921243734520144, w0=-0.314664000331236, gamma=0.1278439878632557\n",
      "Gradient Descent(1487/4999): loss=0.2920487289019725, w0=-0.3146640002880995, gamma=0.1290004128163806\n",
      "Gradient Descent(1488/4999): loss=0.2920011093072661, w0=-0.31466400025013636, gamma=0.07339947027620322\n",
      "Gradient Descent(1489/4999): loss=0.29200046803422036, w0=-0.31466400023132235, gamma=0.035840023383406874\n",
      "Gradient Descent(1490/4999): loss=0.2920003475598, w0=-0.31466400022281, gamma=0.035466885316907885\n",
      "Gradient Descent(1491/4999): loss=0.29200019172462577, w0=-0.3146640002146882, gamma=0.31972866444357495\n",
      "Gradient Descent(1492/4999): loss=0.2920001437747776, w0=-0.3146640001440681, gamma=0.5567938508569635\n",
      "Gradient Descent(1493/4999): loss=0.29199982804944236, w0=-0.3146640000604074, gamma=0.38058091978570896\n",
      "Gradient Descent(1494/4999): loss=0.29199963929230704, w0=-0.3146640000350635, gamma=0.0821815539419577\n",
      "Gradient Descent(1495/4999): loss=0.2919996032377847, w0=-0.3146640000316736, gamma=0.041008417713296774\n",
      "Gradient Descent(1496/4999): loss=0.29199957327648285, w0=-0.31466400003012107, gamma=0.03589162382001109\n",
      "Gradient Descent(1497/4999): loss=0.29199954061109756, w0=-0.314664000028818, gamma=0.04089633009183357\n",
      "Gradient Descent(1498/4999): loss=0.2919995325072761, w0=-0.3146640000273865, gamma=0.12955069923583068\n",
      "Gradient Descent(1499/4999): loss=0.2919995245984979, w0=-0.31466400002303724, gamma=0.15570755873972195\n",
      "Gradient Descent(1500/4999): loss=0.29199950089145554, w0=-0.3146640000184871, gamma=0.09975437851735935\n",
      "Gradient Descent(1501/4999): loss=0.2919994733212437, w0=-0.31466400001602596, gamma=0.05400653449735118\n",
      "Gradient Descent(1502/4999): loss=0.291999455693106, w0=-0.3146640000148264, gamma=0.04344281692213038\n",
      "Gradient Descent(1503/4999): loss=0.2919994461477549, w0=-0.31466400001391365, gamma=0.04388543387418201\n",
      "Gradient Descent(1504/4999): loss=0.2919994384767874, w0=-0.31466400001303163, gamma=0.14803356723884298\n",
      "Gradient Descent(1505/4999): loss=0.2919994307299441, w0=-0.3146640000101869, gamma=0.5294532849448192\n",
      "Gradient Descent(1506/4999): loss=0.29199940460534346, w0=-0.3146640000015188, gamma=0.13213906810473114\n",
      "Gradient Descent(1507/4999): loss=0.29199931123437695, w0=-0.3146640000005008, gamma=0.03509591450925761\n",
      "Gradient Descent(1508/4999): loss=0.29199928798695235, w0=-0.3146640000002662, gamma=0.035039491973688684\n",
      "Gradient Descent(1509/4999): loss=0.2919992817462423, w0=-0.31466400000004013, gamma=0.47290836064205066\n",
      "Gradient Descent(1510/4999): loss=0.29199927556943206, w0=-0.31466399999709627, gamma=1.4862069482170472\n",
      "Gradient Descent(1511/4999): loss=0.291999192206609, w0=-0.3146639999922201, gamma=0.12453140619376335\n",
      "Gradient Descent(1512/4999): loss=0.2919989302572394, w0=-0.3146639999924188, gamma=0.05779941850853635\n",
      "Gradient Descent(1513/4999): loss=0.29199890831338926, w0=-0.3146639999924995, gamma=0.05775721793017108\n",
      "Gradient Descent(1514/4999): loss=0.2919988981146255, w0=-0.3146639999925755, gamma=1.3446232286492352\n",
      "Gradient Descent(1515/4999): loss=0.2919988879358721, w0=-0.31466399999424305, gamma=16.070042141314957\n",
      "Gradient Descent(1516/4999): loss=0.29199865097902433, w0=-0.3146639999873783, gamma=0.09114166891027226\n",
      "Gradient Descent(1517/4999): loss=0.291995821737862, w0=-0.31466399998796535, gamma=0.036557787344671244\n",
      "Gradient Descent(1518/4999): loss=0.2919958066954659, w0=-0.31466399998817934, gamma=0.035232535606305286\n",
      "Gradient Descent(1519/4999): loss=0.2919957982484009, w0=-0.314663999988378, gamma=0.042518179344578785\n",
      "Gradient Descent(1520/4999): loss=0.2919957920387713, w0=-0.31466399998860933, gamma=0.06278062436474242\n",
      "Gradient Descent(1521/4999): loss=0.29199578455742203, w0=-0.3146639999889364, gamma=0.16143506204917937\n",
      "Gradient Descent(1522/4999): loss=0.2919957735127375, w0=-0.3146639999897246, gamma=0.6455039885200947\n",
      "Gradient Descent(1523/4999): loss=0.2919957451146972, w0=-0.3146639999923675, gamma=0.4143433458805024\n",
      "Gradient Descent(1524/4999): loss=0.29199563158451397, w0=-0.3146639999929689, gamma=0.05491336793396489\n",
      "Gradient Descent(1525/4999): loss=0.2919955587480723, w0=-0.3146639999930156, gamma=0.05071363520341024\n",
      "Gradient Descent(1526/4999): loss=0.2919955490641875, w0=-0.31466399999305633, gamma=0.03697879470029368\n",
      "Gradient Descent(1527/4999): loss=0.2919955401457718, w0=-0.31466399999308453, gamma=0.03803589968080707\n",
      "Gradient Descent(1528/4999): loss=0.2919955336430948, w0=-0.31466399999311245, gamma=0.2897599607018222\n",
      "Gradient Descent(1529/4999): loss=0.291995526954676, w0=-0.31466399999331723, gamma=1.2877321149902243\n",
      "Gradient Descent(1530/4999): loss=0.291995476002518, w0=-0.31466399999396355, gamma=0.23668202407645295\n",
      "Gradient Descent(1531/4999): loss=0.2919952495817925, w0=-0.3146639999939294, gamma=0.06402612392624019\n",
      "Gradient Descent(1532/4999): loss=0.2919952079851865, w0=-0.31466399999392236, gamma=0.04272221982560679\n",
      "Gradient Descent(1533/4999): loss=0.29199519671488755, w0=-0.314663999993918, gamma=0.03560836775657076\n",
      "Gradient Descent(1534/4999): loss=0.2919951892002297, w0=-0.3146639999939145, gamma=0.038440677651453745\n",
      "Gradient Descent(1535/4999): loss=0.29199518293984955, w0=-0.3146639999939108, gamma=0.15585845273765264\n",
      "Gradient Descent(1536/4999): loss=0.2919951761817886, w0=-0.3146639999938966, gamma=0.3691507113840951\n",
      "Gradient Descent(1537/4999): loss=0.29199514878141447, w0=-0.3146639999938682, gamma=0.39760332513851654\n",
      "Gradient Descent(1538/4999): loss=0.2919950838852994, w0=-0.31466399999384886, gamma=0.1004340218428857\n",
      "Gradient Descent(1539/4999): loss=0.29199501399180755, w0=-0.3146639999938459, gamma=0.06880324643055435\n",
      "Gradient Descent(1540/4999): loss=0.29199499633562875, w0=-0.3146639999938441, gamma=0.03849129857564144\n",
      "Gradient Descent(1541/4999): loss=0.29199498424108544, w0=-0.31466399999384315, gamma=0.036378780632310856\n",
      "Gradient Descent(1542/4999): loss=0.2919949774748626, w0=-0.3146639999938423, gamma=0.16511901216990127\n",
      "Gradient Descent(1543/4999): loss=0.29199497108017525, w0=-0.3146639999938386, gamma=12.892496759693751\n",
      "Gradient Descent(1544/4999): loss=0.291994942055679, w0=-0.31466399999359523, gamma=4.2785575698333975\n",
      "Gradient Descent(1545/4999): loss=0.29199267670597934, w0=-0.31466399999456574, gamma=0.039193505405054875\n",
      "Gradient Descent(1546/4999): loss=0.29199200787334934, w0=-0.3146639999945366, gamma=0.0359762243193578\n",
      "Gradient Descent(1547/4999): loss=0.29199192360052734, w0=-0.31466399999451095, gamma=0.04622372925255462\n",
      "Gradient Descent(1548/4999): loss=0.2919919137312323, w0=-0.31466399999447914, gamma=0.07797976538936797\n",
      "Gradient Descent(1549/4999): loss=0.2919919047260339, w0=-0.31466399999442796, gamma=0.0793104685433879\n",
      "Gradient Descent(1550/4999): loss=0.291991890858512, w0=-0.31466399999438, gamma=0.0419054470826574\n",
      "Gradient Descent(1551/4999): loss=0.29199187694538903, w0=-0.3146639999943567, gamma=0.036799178588449014\n",
      "Gradient Descent(1552/4999): loss=0.2919918695940056, w0=-0.31466399999433703, gamma=0.06851240420591483\n",
      "Gradient Descent(1553/4999): loss=0.29199186313858067, w0=-0.31466399999430184, gamma=5.702668282291898\n",
      "Gradient Descent(1554/4999): loss=0.2919918511199993, w0=-0.3146639999915709, gamma=12.901623258578073\n",
      "Gradient Descent(1555/4999): loss=0.29199085091710486, w0=-0.3146640000206402, gamma=0.036444863039319916\n",
      "Gradient Descent(1556/4999): loss=0.2919886462596642, w0=-0.314664000019663, gamma=0.035397297877072006\n",
      "Gradient Descent(1557/4999): loss=0.2919885841712676, w0=-0.31466400001874845, gamma=0.04569268414356707\n",
      "Gradient Descent(1558/4999): loss=0.29198857753280305, w0=-0.3146640000176097, gamma=0.05549622845160634\n",
      "Gradient Descent(1559/4999): loss=0.29198856947962637, w0=-0.3146640000162898, gamma=0.07576282123422062\n",
      "Gradient Descent(1560/4999): loss=0.29198855975751603, w0=-0.3146640000145879, gamma=0.16442841289221546\n",
      "Gradient Descent(1561/4999): loss=0.2919885464903436, w0=-0.3146640000111741, gamma=0.4339342046834891\n",
      "Gradient Descent(1562/4999): loss=0.29198851770100775, w0=-0.31466400000364636, gamma=0.187013650402252\n",
      "Gradient Descent(1563/4999): loss=0.29198844173874927, w0=-0.31466400000180994, gamma=0.03649323095233571\n",
      "Gradient Descent(1564/4999): loss=0.29198840901800865, w0=-0.3146640000015186, gamma=0.035182022685808306\n",
      "Gradient Descent(1565/4999): loss=0.2919884026172074, w0=-0.314664000001248, gamma=0.05192657914348777\n",
      "Gradient Descent(1566/4999): loss=0.2919883964593369, w0=-0.31466400000086264, gamma=0.22018295960441364\n",
      "Gradient Descent(1567/4999): loss=0.29198838737082927, w0=-0.31466399999931344, gamma=1.0010091366242475\n",
      "Gradient Descent(1568/4999): loss=0.29198834883365593, w0=-0.3146639999938211, gamma=0.8239322535507788\n",
      "Gradient Descent(1569/4999): loss=0.29198817364355756, w0=-0.3146639999938258, gamma=0.10066203719022006\n",
      "Gradient Descent(1570/4999): loss=0.2919880294889199, w0=-0.31466399999382594, gamma=0.06577800948121182\n",
      "Gradient Descent(1571/4999): loss=0.29198801185045054, w0=-0.314663999993826, gamma=0.03703210460091948\n",
      "Gradient Descent(1572/4999): loss=0.29198800033829, w0=-0.31466399999382605, gamma=0.03525154897759\n",
      "Gradient Descent(1573/4999): loss=0.29198799385554286, w0=-0.3146639999938261, gamma=0.05745758821822084\n",
      "Gradient Descent(1574/4999): loss=0.29198798768728024, w0=-0.31466399999382616, gamma=0.8849462617802891\n",
      "Gradient Descent(1575/4999): loss=0.29198797763351103, w0=-0.3146639999938269, gamma=60.66223942524199\n",
      "Gradient Descent(1576/4999): loss=0.2919878227927845, w0=-0.31466399999384387, gamma=0.48423303908346327\n",
      "Gradient Descent(1577/4999): loss=0.2919772305285177, w0=-0.31466399999384115, gamma=0.049415729490264806\n",
      "Gradient Descent(1578/4999): loss=0.29197729138023915, w0=-0.314663999993841, gamma=0.04234704105188983\n",
      "Gradient Descent(1579/4999): loss=0.29197714678072095, w0=-0.31466399999384087, gamma=0.03674598582947513\n",
      "Gradient Descent(1580/4999): loss=0.2919771294172697, w0=-0.31466399999384076, gamma=0.04463272625246198\n",
      "Gradient Descent(1581/4999): loss=0.29197712192788267, w0=-0.31466399999384065, gamma=0.09880707093329848\n",
      "Gradient Descent(1582/4999): loss=0.29197711380598096, w0=-0.3146639999938404, gamma=0.10195246838706891\n",
      "Gradient Descent(1583/4999): loss=0.29197709646912856, w0=-0.3146639999938402, gamma=0.04090684716189705\n",
      "Gradient Descent(1584/4999): loss=0.29197707875689805, w0=-0.31466399999384015, gamma=0.03563889095201018\n",
      "Gradient Descent(1585/4999): loss=0.2919770716494136, w0=-0.3146639999938401, gamma=0.04627854012888456\n",
      "Gradient Descent(1586/4999): loss=0.29197706545785357, w0=-0.31466399999384004, gamma=2.345499896468403\n",
      "Gradient Descent(1587/4999): loss=0.29197705741792596, w0=-0.3146639999938365, gamma=34.29163733974849\n",
      "Gradient Descent(1588/4999): loss=0.291976649965338, w0=-0.31466399999391514, gamma=0.08519201277539416\n",
      "Gradient Descent(1589/4999): loss=0.29197071099841326, w0=-0.3146639999939092, gamma=0.04439810870108423\n",
      "Gradient Descent(1590/4999): loss=0.29197069169747175, w0=-0.3146639999939064, gamma=0.03620426288755687\n",
      "Gradient Descent(1591/4999): loss=0.2919706780134394, w0=-0.31466399999390415, gamma=0.03564407108936256\n",
      "Gradient Descent(1592/4999): loss=0.29197067133205257, w0=-0.31466399999390204, gamma=0.059858040679403784\n",
      "Gradient Descent(1593/4999): loss=0.29197066514935854, w0=-0.31466399999389866, gamma=0.10720155833402203\n",
      "Gradient Descent(1594/4999): loss=0.2919706547777202, w0=-0.3146639999938929, gamma=0.309388081377162\n",
      "Gradient Descent(1595/4999): loss=0.2919706362117053, w0=-0.31466399999387806, gamma=0.531453114811109\n",
      "Gradient Descent(1596/4999): loss=0.2919705826549926, w0=-0.3146639999938606, gamma=0.07452146297051374\n",
      "Gradient Descent(1597/4999): loss=0.2919704907191305, w0=-0.3146639999938594, gamma=0.04425617088493147\n",
      "Gradient Descent(1598/4999): loss=0.2919704778189852, w0=-0.3146639999938588, gamma=0.036871093315313706\n",
      "Gradient Descent(1599/4999): loss=0.2919704701579135, w0=-0.3146639999938583, gamma=0.03682340026168867\n",
      "Gradient Descent(1600/4999): loss=0.29197046377949754, w0=-0.3146639999938578, gamma=0.15689542760625658\n",
      "Gradient Descent(1601/4999): loss=0.29197045740969757, w0=-0.3146639999938558, gamma=0.8814647470056087\n",
      "Gradient Descent(1602/4999): loss=0.2919704302698335, w0=-0.3146639999938465, gamma=0.47582858207453643\n",
      "Gradient Descent(1603/4999): loss=0.2919702778004963, w0=-0.314663999993846, gamma=0.05336036445873004\n",
      "Gradient Descent(1604/4999): loss=0.29197019552960574, w0=-0.314663999993846, gamma=0.04905161325911383\n",
      "Gradient Descent(1605/4999): loss=0.29197018627379717, w0=-0.314663999993846, gamma=0.035536446302726\n",
      "Gradient Descent(1606/4999): loss=0.2919701777897758, w0=-0.314663999993846, gamma=0.03735072818779028\n",
      "Gradient Descent(1607/4999): loss=0.29197017164368816, w0=-0.314663999993846, gamma=1.1603234648275802\n",
      "Gradient Descent(1608/4999): loss=0.2919701651839554, w0=-0.3146639999938455, gamma=2.88058358349623\n",
      "Gradient Descent(1609/4999): loss=0.2919699645158946, w0=-0.31466399999384626, gamma=0.13544419492170787\n",
      "Gradient Descent(1610/4999): loss=0.29196946656507855, w0=-0.31466399999384626, gamma=0.08989763539814473\n",
      "Gradient Descent(1611/4999): loss=0.2919694430441667, w0=-0.31466399999384626, gamma=0.0462031482534825\n",
      "Gradient Descent(1612/4999): loss=0.29196942749128313, w0=-0.31466399999384626, gamma=0.03518122787608569\n",
      "Gradient Descent(1613/4999): loss=0.2919694194925438, w0=-0.31466399999384626, gamma=0.03540017765336\n",
      "Gradient Descent(1614/4999): loss=0.2919694134096552, w0=-0.31466399999384626, gamma=0.19597146653842248\n",
      "Gradient Descent(1615/4999): loss=0.29196940729027987, w0=-0.3146639999938463, gamma=0.4328236595884692\n",
      "Gradient Descent(1616/4999): loss=0.2919693734144863, w0=-0.31466399999384637, gamma=0.26832563591730385\n",
      "Gradient Descent(1617/4999): loss=0.29196929859861404, w0=-0.3146639999938464, gamma=0.087300993112693\n",
      "Gradient Descent(1618/4999): loss=0.291969252220212, w0=-0.3146639999938464, gamma=0.07622910425650986\n",
      "Gradient Descent(1619/4999): loss=0.2919692371293503, w0=-0.3146639999938464, gamma=0.07304064920979682\n",
      "Gradient Descent(1620/4999): loss=0.29196922395369596, w0=-0.3146639999938464, gamma=0.12401570712142979\n",
      "Gradient Descent(1621/4999): loss=0.29196921132928794, w0=-0.3146639999938464, gamma=0.1907262875820822\n",
      "Gradient Descent(1622/4999): loss=0.291969189894531, w0=-0.3146639999938465, gamma=0.06292405889053346\n",
      "Gradient Descent(1623/4999): loss=0.29196915693023956, w0=-0.3146639999938465, gamma=0.03676712321685503\n",
      "Gradient Descent(1624/4999): loss=0.2919691460548066, w0=-0.3146639999938465, gamma=0.037312681480063295\n",
      "Gradient Descent(1625/4999): loss=0.2919691397001726, w0=-0.3146639999938465, gamma=0.9294196273072904\n",
      "Gradient Descent(1626/4999): loss=0.2919691332513675, w0=-0.31466399999384664, gamma=123.49794432083624\n",
      "Gradient Descent(1627/4999): loss=0.2919689726231439, w0=-0.31466399999386935, gamma=0.29623531018914767\n",
      "Gradient Descent(1628/4999): loss=0.2919477162666468, w0=-0.3146639999938692, gamma=0.037151788568922234\n",
      "Gradient Descent(1629/4999): loss=0.2919481603833187, w0=-0.31466399999386924, gamma=0.03587285722075136\n",
      "Gradient Descent(1630/4999): loss=0.29194765659954874, w0=-0.31466399999386924, gamma=0.04525506144806326\n",
      "Gradient Descent(1631/4999): loss=0.2919476427274197, w0=-0.31466399999386924, gamma=0.05355008426912346\n",
      "Gradient Descent(1632/4999): loss=0.2919476342724685, w0=-0.31466399999386924, gamma=0.05339749323904115\n",
      "Gradient Descent(1633/4999): loss=0.29194762512799927, w0=-0.31466399999386924, gamma=0.04465538500810666\n",
      "Gradient Descent(1634/4999): loss=0.29194761602817365, w0=-0.31466399999386924, gamma=0.06913670234901591\n",
      "Gradient Descent(1635/4999): loss=0.2919476084182015, w0=-0.31466399999386924, gamma=0.4812327790083164\n",
      "Gradient Descent(1636/4999): loss=0.2919475966363072, w0=-0.3146639999938692, gamma=0.5099291259910067\n",
      "Gradient Descent(1637/4999): loss=0.29194751462891483, w0=-0.31466399999386924, gamma=0.037965586813308805\n",
      "Gradient Descent(1638/4999): loss=0.2919474277486331, w0=-0.31466399999386924, gamma=0.035048323953327215\n",
      "Gradient Descent(1639/4999): loss=0.29194742126686535, w0=-0.31466399999386924, gamma=0.037327419632051835\n",
      "Gradient Descent(1640/4999): loss=0.2919474152947551, w0=-0.31466399999386924, gamma=14.663002157961655\n",
      "Gradient Descent(1641/4999): loss=0.29194740893441556, w0=-0.3146639999938712, gamma=37.07208570891413\n",
      "Gradient Descent(1642/4999): loss=0.29194491148269, w0=-0.3146639999939018, gamma=0.1018004620774454\n",
      "Gradient Descent(1643/4999): loss=0.29193897412201086, w0=-0.3146639999938995, gamma=0.09705882599539815\n",
      "Gradient Descent(1644/4999): loss=0.2919385997111961, w0=-0.31466399999389744, gamma=0.07648867087390207\n",
      "Gradient Descent(1645/4999): loss=0.2919385781641215, w0=-0.314663999993896, gamma=0.039572361865724215\n",
      "Gradient Descent(1646/4999): loss=0.2919385650033511, w0=-0.3146639999938953, gamma=0.035154651168651074\n",
      "Gradient Descent(1647/4999): loss=0.2919385580336599, w0=-0.3146639999938947, gamma=0.03850879210567186\n",
      "Gradient Descent(1648/4999): loss=0.29193855206911523, w0=-0.3146639999938941, gamma=0.1422077550742464\n",
      "Gradient Descent(1649/4999): loss=0.2919385455423424, w0=-0.3146639999938919, gamma=0.14932632850451957\n",
      "Gradient Descent(1650/4999): loss=0.29193852144650617, w0=-0.3146639999938899, gamma=0.4017283569858173\n",
      "Gradient Descent(1651/4999): loss=0.2919384961481606, w0=-0.31466399999388533, gamma=0.4056519761770853\n",
      "Gradient Descent(1652/4999): loss=0.29193842809168424, w0=-0.3146639999938826, gamma=0.06508100633580487\n",
      "Gradient Descent(1653/4999): loss=0.2919383593780876, w0=-0.31466399999388234, gamma=0.04727144010943207\n",
      "Gradient Descent(1654/4999): loss=0.2919383483504932, w0=-0.31466399999388217, gamma=0.036160583045787474\n",
      "Gradient Descent(1655/4999): loss=0.2919383403425902, w0=-0.31466399999388206, gamma=0.03893899949576831\n",
      "Gradient Descent(1656/4999): loss=0.2919383342172353, w0=-0.31466399999388195, gamma=0.9147515352146424\n",
      "Gradient Descent(1657/4999): loss=0.291938327621329, w0=-0.31466399999387906, gamma=4.455156562726037\n",
      "Gradient Descent(1658/4999): loss=0.29193817267540734, w0=-0.31466399999387834, gamma=0.11313428592008536\n",
      "Gradient Descent(1659/4999): loss=0.2919374183530551, w0=-0.3146639999938785, gamma=0.06106185197073897\n",
      "Gradient Descent(1660/4999): loss=0.2919373991130448, w0=-0.31466399999387856, gamma=0.03944287449817238\n",
      "Gradient Descent(1661/4999): loss=0.29193738871660957, w0=-0.3146639999938786, gamma=0.035291156633762255\n",
      "Gradient Descent(1662/4999): loss=0.29193738201812985, w0=-0.3146639999938787, gamma=0.039071216048809344\n",
      "Gradient Descent(1663/4999): loss=0.29193737604327114, w0=-0.31466399999387873, gamma=0.10354915135652437\n",
      "Gradient Descent(1664/4999): loss=0.2919373694290046, w0=-0.31466399999387884, gamma=0.17358067577805744\n",
      "Gradient Descent(1665/4999): loss=0.29193735189971454, w0=-0.314663999993879, gamma=1.0071808356556156\n",
      "Gradient Descent(1666/4999): loss=0.2919373225155984, w0=-0.31466399999387984, gamma=0.6097664527455671\n",
      "Gradient Descent(1667/4999): loss=0.2919371520255535, w0=-0.31466399999387995, gamma=0.0667527101825007\n",
      "Gradient Descent(1668/4999): loss=0.29193704884957167, w0=-0.31466399999387995, gamma=0.05249146249453634\n",
      "Gradient Descent(1669/4999): loss=0.29193703752255146, w0=-0.31466399999387995, gamma=0.03670618414583773\n",
      "Gradient Descent(1670/4999): loss=0.29193702863577753, w0=-0.31466399999387995, gamma=0.035724882497511774\n",
      "Gradient Descent(1671/4999): loss=0.291937022422714, w0=-0.31466399999387995, gamma=0.12887257883671285\n",
      "Gradient Descent(1672/4999): loss=0.2919370163763856, w0=-0.31466399999388, gamma=0.5432565509404013\n",
      "Gradient Descent(1673/4999): loss=0.29193699456527583, w0=-0.31466399999388006, gamma=1.230014448193622\n",
      "Gradient Descent(1674/4999): loss=0.29193690262395094, w0=-0.3146639999938803, gamma=0.10116708087869918\n",
      "Gradient Descent(1675/4999): loss=0.2919366944865463, w0=-0.3146639999938803, gamma=0.05917560882843263\n",
      "Gradient Descent(1676/4999): loss=0.2919366773576333, w0=-0.3146639999938803, gamma=0.03711820044185939\n",
      "Gradient Descent(1677/4999): loss=0.291936667341218, w0=-0.3146639999938803, gamma=0.035312375875328435\n",
      "Gradient Descent(1678/4999): loss=0.2919366610588946, w0=-0.3146639999938803, gamma=0.07216980670126369\n",
      "Gradient Descent(1679/4999): loss=0.29193665508377614, w0=-0.3146639999938803, gamma=0.48902686142512175\n",
      "Gradient Descent(1680/4999): loss=0.291936642872196, w0=-0.3146639999938804, gamma=37.863399623500534\n",
      "Gradient Descent(1681/4999): loss=0.2919365601274513, w0=-0.31466399999389066, gamma=1.0119498591973557\n",
      "Gradient Descent(1682/4999): loss=0.291930160871561, w0=-0.3146639999938872, gamma=0.04675633164894323\n",
      "Gradient Descent(1683/4999): loss=0.2919301090468227, w0=-0.3146639999938873, gamma=0.03843678275846253\n",
      "Gradient Descent(1684/4999): loss=0.2919299962862787, w0=-0.3146639999938873, gamma=0.03869519371395053\n",
      "Gradient Descent(1685/4999): loss=0.29192997753938377, w0=-0.3146639999938873, gamma=0.06614676273304113\n",
      "Gradient Descent(1686/4999): loss=0.2919299695628244, w0=-0.3146639999938873, gamma=0.07857039268508122\n",
      "Gradient Descent(1687/4999): loss=0.2919299579382592, w0=-0.3146639999938873, gamma=0.06592784537559904\n",
      "Gradient Descent(1688/4999): loss=0.29192994468625993, w0=-0.3146639999938873, gamma=0.03608519541446459\n",
      "Gradient Descent(1689/4999): loss=0.29192993357864416, w0=-0.3146639999938873, gamma=0.03579808928867572\n",
      "Gradient Descent(1690/4999): loss=0.29192992749854507, w0=-0.3146639999938873, gamma=0.27540915001867206\n",
      "Gradient Descent(1691/4999): loss=0.29192992146755953, w0=-0.31466399999388733, gamma=0.4779491241776396\n",
      "Gradient Descent(1692/4999): loss=0.29192987506975654, w0=-0.31466399999388744, gamma=1.8734120547833142\n",
      "Gradient Descent(1693/4999): loss=0.2919297945536534, w0=-0.3146639999938879, gamma=0.08451022253044731\n",
      "Gradient Descent(1694/4999): loss=0.2919294790056564, w0=-0.3146639999938879, gamma=0.042571673677604664\n",
      "Gradient Descent(1695/4999): loss=0.29192946476741855, w0=-0.3146639999938879, gamma=0.036733368010811854\n",
      "Gradient Descent(1696/4999): loss=0.29192945758169103, w0=-0.3146639999938879, gamma=0.037267910724617416\n",
      "Gradient Descent(1697/4999): loss=0.2919294513943263, w0=-0.3146639999938879, gamma=0.08967425959484646\n",
      "Gradient Descent(1698/4999): loss=0.2919294451176995, w0=-0.3146639999938879, gamma=0.29151827319697804\n",
      "Gradient Descent(1699/4999): loss=0.2919294300150184, w0=-0.31466399999388794, gamma=0.6943559912982377\n",
      "Gradient Descent(1700/4999): loss=0.2919293809192524, w0=-0.314663999993888, gamma=0.0917314856954335\n",
      "Gradient Descent(1701/4999): loss=0.2919292639874336, w0=-0.314663999993888, gamma=0.04867286392514731\n",
      "Gradient Descent(1702/4999): loss=0.29192924853938307, w0=-0.314663999993888, gamma=0.041082164759919396\n",
      "Gradient Descent(1703/4999): loss=0.2919292403414752, w0=-0.314663999993888, gamma=0.03751753060308439\n",
      "Gradient Descent(1704/4999): loss=0.29192923342335686, w0=-0.314663999993888, gamma=0.09938822432144173\n",
      "Gradient Descent(1705/4999): loss=0.2919292271056343, w0=-0.314663999993888, gamma=4.801273478536991\n",
      "Gradient Descent(1706/4999): loss=0.29192921036937275, w0=-0.314663999993889, gamma=3.2104629122233113\n",
      "Gradient Descent(1707/4999): loss=0.291928401983213, w0=-0.3146639999938894, gamma=0.036871587303586445\n",
      "Gradient Descent(1708/4999): loss=0.29192787041214885, w0=-0.3146639999938894, gamma=0.0356927373153441\n",
      "Gradient Descent(1709/4999): loss=0.29192785564022117, w0=-0.3146639999938894, gamma=0.04648933044920439\n",
      "Gradient Descent(1710/4999): loss=0.29192784951439826, w0=-0.3146639999938894, gamma=0.058471270491925714\n",
      "Gradient Descent(1711/4999): loss=0.29192784167466584, w0=-0.3146639999938894, gamma=0.1051981719273449\n",
      "Gradient Descent(1712/4999): loss=0.2919278318332553, w0=-0.31466399999388944, gamma=0.17070873017965257\n",
      "Gradient Descent(1713/4999): loss=0.2919278141321698, w0=-0.3146639999938895, gamma=0.1765024755189935\n",
      "Gradient Descent(1714/4999): loss=0.2919277854118383, w0=-0.31466399999388955, gamma=0.04573364557991062\n",
      "Gradient Descent(1715/4999): loss=0.29192775571820617, w0=-0.31466399999388955, gamma=0.036022238559697016\n",
      "Gradient Descent(1716/4999): loss=0.29192774802395827, w0=-0.31466399999388955, gamma=0.041092705770357944\n",
      "Gradient Descent(1717/4999): loss=0.2919277419638332, w0=-0.31466399999388955, gamma=3.978064937183844\n",
      "Gradient Descent(1718/4999): loss=0.2919277350507552, w0=-0.3146639999938905, gamma=48.00685397551154\n",
      "Gradient Descent(1719/4999): loss=0.2919270658907749, w0=-0.31466399999388733, gamma=0.060705692936563825\n",
      "Gradient Descent(1720/4999): loss=0.29191904755651465, w0=-0.31466399999388805, gamma=0.039139926127859485\n",
      "Gradient Descent(1721/4999): loss=0.291919007692663, w0=-0.3146639999938885, gamma=0.035497577872503544\n",
      "Gradient Descent(1722/4999): loss=0.2919189896615845, w0=-0.3146639999938889, gamma=0.04844318141202944\n",
      "Gradient Descent(1723/4999): loss=0.29191898281206424, w0=-0.3146639999938894, gamma=0.18564244659750084\n",
      "Gradient Descent(1724/4999): loss=0.291918974023923, w0=-0.31466399999389116, gamma=0.20769967453274393\n",
      "Gradient Descent(1725/4999): loss=0.291918941959363, w0=-0.3146639999938928, gamma=0.13989236891729312\n",
      "Gradient Descent(1726/4999): loss=0.2919189071698048, w0=-0.3146639999938936, gamma=0.04294695558614762\n",
      "Gradient Descent(1727/4999): loss=0.2919188837793133, w0=-0.31466399999389383, gamma=0.03608847818010819\n",
      "Gradient Descent(1728/4999): loss=0.2919188765789236, w0=-0.314663999993894, gamma=0.036305170792655994\n",
      "Gradient Descent(1729/4999): loss=0.29191887054157284, w0=-0.31466399999389416, gamma=0.10275166571660846\n",
      "Gradient Descent(1730/4999): loss=0.2919188644688514, w0=-0.31466399999389466, gamma=0.35100033639375366\n",
      "Gradient Descent(1731/4999): loss=0.2919188472818793, w0=-0.3146639999938962, gamma=0.5953343635200437\n",
      "Gradient Descent(1732/4999): loss=0.2919187885722797, w0=-0.31466399999389794, gamma=0.0733315048331948\n",
      "Gradient Descent(1733/4999): loss=0.2919186890024638, w0=-0.31466399999389805, gamma=0.05277076884813821\n",
      "Gradient Descent(1734/4999): loss=0.29191867673485045, w0=-0.3146639999938981, gamma=0.04738428775989695\n",
      "Gradient Descent(1735/4999): loss=0.29191866790858484, w0=-0.31466399999389816, gamma=0.044365863245750786\n",
      "Gradient Descent(1736/4999): loss=0.29191865998374616, w0=-0.3146639999938982, gamma=0.1092721584656251\n",
      "Gradient Descent(1737/4999): loss=0.2919186525637801, w0=-0.3146639999938983, gamma=1.0556314723114428\n",
      "Gradient Descent(1738/4999): loss=0.2919186342886917, w0=-0.31466399999389927, gamma=0.26720770842184133\n",
      "Gradient Descent(1739/4999): loss=0.29191845774820424, w0=-0.3146639999938993, gamma=0.035308529494946915\n",
      "Gradient Descent(1740/4999): loss=0.2919184130956492, w0=-0.3146639999938993, gamma=0.03503826483670498\n",
      "Gradient Descent(1741/4999): loss=0.2919184071602503, w0=-0.3146639999938993, gamma=0.13424531278257476\n",
      "Gradient Descent(1742/4999): loss=0.2919184013012334, w0=-0.3146639999938993, gamma=50.90356797200585\n",
      "Gradient Descent(1743/4999): loss=0.2919183788531985, w0=-0.3146639999939101, gamma=44.46998441330471\n",
      "Gradient Descent(1744/4999): loss=0.29190987874061824, w0=-0.3146639999938325, gamma=0.056672426248114936\n",
      "Gradient Descent(1745/4999): loss=0.29191145678671265, w0=-0.31466399999383726, gamma=0.055554069150351104\n",
      "Gradient Descent(1746/4999): loss=0.2919025812522948, w0=-0.31466399999384165, gamma=0.0664817496685747\n",
      "Gradient Descent(1747/4999): loss=0.291902482920618, w0=-0.3146639999938466, gamma=0.06881073896745246\n",
      "Gradient Descent(1748/4999): loss=0.29190246405464815, w0=-0.31466399999385136, gamma=0.04236386042659423\n",
      "Gradient Descent(1749/4999): loss=0.29190245193901465, w0=-0.3146639999938541, gamma=0.03711249148698326\n",
      "Gradient Descent(1750/4999): loss=0.2919024446004118, w0=-0.31466399999385636, gamma=0.06109828903909112\n",
      "Gradient Descent(1751/4999): loss=0.29190243835504315, w0=-0.31466399999386, gamma=0.21598018678403508\n",
      "Gradient Descent(1752/4999): loss=0.2919024281393047, w0=-0.3146639999938721, gamma=0.22317618945885065\n",
      "Gradient Descent(1753/4999): loss=0.29190239227306003, w0=-0.3146639999938819, gamma=0.03958540670968244\n",
      "Gradient Descent(1754/4999): loss=0.2919023553478057, w0=-0.3146639999938832, gamma=0.03513260750132962\n",
      "Gradient Descent(1755/4999): loss=0.29190234878553234, w0=-0.3146639999938844, gamma=0.03780590511164993\n",
      "Gradient Descent(1756/4999): loss=0.29190234297108125, w0=-0.3146639999938856, gamma=0.20008408511561862\n",
      "Gradient Descent(1757/4999): loss=0.29190233671447, w0=-0.3146639999938917, gamma=0.3334730911660053\n",
      "Gradient Descent(1758/4999): loss=0.29190230360242814, w0=-0.3146639999938999, gamma=0.36365874698536826\n",
      "Gradient Descent(1759/4999): loss=0.2919022484172307, w0=-0.3146639999939058, gamma=0.11921328804074871\n",
      "Gradient Descent(1760/4999): loss=0.2919021882395748, w0=-0.31466399999390704, gamma=0.08276210705163699\n",
      "Gradient Descent(1761/4999): loss=0.29190216851211054, w0=-0.3146639999939078, gamma=0.07122207615602627\n",
      "Gradient Descent(1762/4999): loss=0.29190215481703125, w0=-0.3146639999939084, gamma=0.04807058911853395\n",
      "Gradient Descent(1763/4999): loss=0.29190214303179013, w0=-0.3146639999939088, gamma=0.04705552143404585\n",
      "Gradient Descent(1764/4999): loss=0.2919021350775084, w0=-0.31466399999390915, gamma=0.12164668069096794\n",
      "Gradient Descent(1765/4999): loss=0.29190212729124176, w0=-0.31466399999391004, gamma=0.7703035421975096\n",
      "Gradient Descent(1766/4999): loss=0.29190210716254006, w0=-0.31466399999391487, gamma=0.1553309559956054\n",
      "Gradient Descent(1767/4999): loss=0.2919019797060531, w0=-0.3146639999939151, gamma=0.035305051074705454\n",
      "Gradient Descent(1768/4999): loss=0.29190195401360775, w0=-0.31466399999391514, gamma=0.035055115477234326\n",
      "Gradient Descent(1769/4999): loss=0.2919019481641933, w0=-0.3146639999939152, gamma=0.4152106660276624\n",
      "Gradient Descent(1770/4999): loss=0.29190194236432787, w0=-0.31466399999391576, gamma=1218.193124396575\n",
      "Gradient Descent(1771/4999): loss=0.29190187366872156, w0=-0.3146639999948953, gamma=33.40951475480818\n",
      "Gradient Descent(1772/4999): loss=0.291706877504821, w0=-0.314663999969154, gamma=0.053907006793298004\n",
      "Gradient Descent(1773/4999): loss=0.2957055472216624, w0=-0.3146639999704999, gamma=0.05337945142976041\n",
      "Gradient Descent(1774/4999): loss=0.2917214442973527, w0=-0.31466399997176087, gamma=0.03973938802529269\n",
      "Gradient Descent(1775/4999): loss=0.29170544015029465, w0=-0.3146639999726499, gamma=0.03699721141619968\n",
      "Gradient Descent(1776/4999): loss=0.29170232908870847, w0=-0.3146639999734445, gamma=0.05245709386800157\n",
      "Gradient Descent(1777/4999): loss=0.2917020662849184, w0=-0.3146639999745296, gamma=0.07893204443882063\n",
      "Gradient Descent(1778/4999): loss=0.2917019813294805, w0=-0.31466399997607664, gamma=0.07824699886982707\n",
      "Gradient Descent(1779/4999): loss=0.2917019595732578, w0=-0.31466399997748923, gamma=0.03645845610236671\n",
      "Gradient Descent(1780/4999): loss=0.2917019481780432, w0=-0.3146639999780959, gamma=0.03537871232965383\n",
      "Gradient Descent(1781/4999): loss=0.29170194279487444, w0=-0.3146639999786632, gamma=0.12093706201052645\n",
      "Gradient Descent(1782/4999): loss=0.2917019376469695, w0=-0.3146639999805336, gamma=0.24665377326777563\n",
      "Gradient Descent(1783/4999): loss=0.291701920061297, w0=-0.3146639999838871, gamma=0.3040131418711353\n",
      "Gradient Descent(1784/4999): loss=0.29170188422363325, w0=-0.31466399998700095, gamma=0.2877906199798481\n",
      "Gradient Descent(1785/4999): loss=0.2917018400647632, w0=-0.31466399998905253, gamma=0.03892587665387966\n",
      "Gradient Descent(1786/4999): loss=0.2917017982757592, w0=-0.31466399998925015, gamma=0.03511311217614346\n",
      "Gradient Descent(1787/4999): loss=0.29170179261468615, w0=-0.31466399998942146, gamma=0.039902589090683026\n",
      "Gradient Descent(1788/4999): loss=0.2917017875154253, w0=-0.3146639999896093, gamma=1.6538201235833376\n",
      "Gradient Descent(1789/4999): loss=0.29170178172080136, w0=-0.31466399999708516, gamma=2.8436755968935845\n",
      "Gradient Descent(1790/4999): loss=0.29170154158594086, w0=-0.3146639999886816, gamma=0.08212574390439621\n",
      "Gradient Descent(1791/4999): loss=0.2917011289405449, w0=-0.3146639999891291, gamma=0.05987665297403608\n",
      "Gradient Descent(1792/4999): loss=0.29170111691728573, w0=-0.31466399998942857, gamma=0.05732179552040369\n",
      "Gradient Descent(1793/4999): loss=0.29170110820902095, w0=-0.3146639999896981, gamma=0.04860642394833531\n",
      "Gradient Descent(1794/4999): loss=0.29170109988943116, w0=-0.3146639999899135, gamma=0.04871641455343008\n",
      "Gradient Descent(1795/4999): loss=0.2917010928352448, w0=-0.31466399999011896, gamma=0.07662396876057347\n",
      "Gradient Descent(1796/4999): loss=0.29170108576530057, w0=-0.3146639999904263, gamma=0.15062531460620648\n",
      "Gradient Descent(1797/4999): loss=0.29170107464545914, w0=-0.31466399999098427, gamma=0.12214503563905937\n",
      "Gradient Descent(1798/4999): loss=0.2917010527866997, w0=-0.31466399999136857, gamma=0.042513403917135706\n",
      "Gradient Descent(1799/4999): loss=0.29170103506133355, w0=-0.314663999991486, gamma=0.036229340475245175\n",
      "Gradient Descent(1800/4999): loss=0.29170102889181243, w0=-0.3146639999915818, gamma=0.06120593612509271\n",
      "Gradient Descent(1801/4999): loss=0.291701023634339, w0=-0.3146639999917378, gamma=14.561527818095218\n",
      "Gradient Descent(1802/4999): loss=0.2917010147523931, w0=-0.31466400002657907, gamma=22.174927747294003\n",
      "Gradient Descent(1803/4999): loss=0.29169890233430823, w0=-0.31466399930708033, gamma=0.03647458905024149\n",
      "Gradient Descent(1804/4999): loss=0.2916959348277965, w0=-0.3146639993321404, gamma=0.03503518514739936\n",
      "Gradient Descent(1805/4999): loss=0.29169569246210114, w0=-0.31466399935533357, gamma=0.03669390939447848\n",
      "Gradient Descent(1806/4999): loss=0.2916956862588521, w0=-0.3146639993787737, gamma=0.5935967344221761\n",
      "Gradient Descent(1807/4999): loss=0.29169568026324355, w0=-0.3146639997440508, gamma=0.6195153356349249\n",
      "Gradient Descent(1808/4999): loss=0.2916955887293231, w0=-0.3146639998989824, gamma=0.17134531819563098\n",
      "Gradient Descent(1809/4999): loss=0.2916954995242696, w0=-0.31466399991528654, gamma=0.15689945953795886\n",
      "Gradient Descent(1810/4999): loss=0.29169547415195596, w0=-0.314663999927658, gamma=0.12814059124953714\n",
      "Gradient Descent(1811/4999): loss=0.29169545144784426, w0=-0.3146639999361765, gamma=0.05050694770144657\n",
      "Gradient Descent(1812/4999): loss=0.2916954329133077, w0=-0.3146639999391039, gamma=0.03636108946140363\n",
      "Gradient Descent(1813/4999): loss=0.29169542560726835, w0=-0.3146639999411049, gamma=0.03728867952375526\n",
      "Gradient Descent(1814/4999): loss=0.2916954203478991, w0=-0.31466399994308236, gamma=0.4100679454161579\n",
      "Gradient Descent(1815/4999): loss=0.29169541495454226, w0=-0.3146639999640179, gamma=1.1383519827611244\n",
      "Gradient Descent(1816/4999): loss=0.29169535564444843, w0=-0.31466399999830313, gamma=0.1791629780555306\n",
      "Gradient Descent(1817/4999): loss=0.2916951910155226, w0=-0.3146639999975566, gamma=0.06855609372197041\n",
      "Gradient Descent(1818/4999): loss=0.2916951651104879, w0=-0.31466399999732214, gamma=0.04568319468552012\n",
      "Gradient Descent(1819/4999): loss=0.29169515519087735, w0=-0.3146639999971766, gamma=0.03547902078129509\n",
      "Gradient Descent(1820/4999): loss=0.291695148583346, w0=-0.31466399999706873, gamma=0.036653419007020314\n",
      "Gradient Descent(1821/4999): loss=0.291695143452709, w0=-0.31466399999696126, gamma=0.33793640561503163\n",
      "Gradient Descent(1822/4999): loss=0.29169513815240844, w0=-0.31466399999600664, gamma=0.9997333119156105\n",
      "Gradient Descent(1823/4999): loss=0.29169508928537596, w0=-0.3146639999941369, gamma=0.16639055530872543\n",
      "Gradient Descent(1824/4999): loss=0.2916949447287413, w0=-0.31466399999413686, gamma=0.07741781807354695\n",
      "Gradient Descent(1825/4999): loss=0.2916949206714946, w0=-0.31466399999413686, gamma=0.06993837672881535\n",
      "Gradient Descent(1826/4999): loss=0.2916949094748945, w0=-0.31466399999413686, gamma=0.04093780062901076\n",
      "Gradient Descent(1827/4999): loss=0.2916948993628303, w0=-0.31466399999413686, gamma=0.037773848625404845\n",
      "Gradient Descent(1828/4999): loss=0.2916948934438296, w0=-0.31466399999413686, gamma=0.12074355595748933\n",
      "Gradient Descent(1829/4999): loss=0.29169488798235843, w0=-0.31466399999413686, gamma=8.68715640625254\n",
      "Gradient Descent(1830/4999): loss=0.2916948705249403, w0=-0.314663999994135, gamma=2.144010645985235\n",
      "Gradient Descent(1831/4999): loss=0.29169361476562455, w0=-0.3146639999941413, gamma=0.03605243986112292\n",
      "Gradient Descent(1832/4999): loss=0.2916933185293481, w0=-0.3146639999941412, gamma=0.0354192391917186\n",
      "Gradient Descent(1833/4999): loss=0.29169329994951126, w0=-0.3146639999941411, gamma=0.050483971906344086\n",
      "Gradient Descent(1834/4999): loss=0.29169329471720706, w0=-0.3146639999941409, gamma=0.056431587526766684\n",
      "Gradient Descent(1835/4999): loss=0.29169328740833483, w0=-0.31466399999414074, gamma=0.06398531406760259\n",
      "Gradient Descent(1836/4999): loss=0.2916932792568045, w0=-0.3146639999941406, gamma=0.29275954357595135\n",
      "Gradient Descent(1837/4999): loss=0.29169327001449125, w0=-0.3146639999941399, gamma=0.7092037400544605\n",
      "Gradient Descent(1838/4999): loss=0.29169322772768114, w0=-0.31466399999413874, gamma=0.07415714652721818\n",
      "Gradient Descent(1839/4999): loss=0.29169312529381175, w0=-0.31466399999413874, gamma=0.035407915473569716\n",
      "Gradient Descent(1840/4999): loss=0.29169311458351227, w0=-0.31466399999413874, gamma=0.03512188632748907\n",
      "Gradient Descent(1841/4999): loss=0.29169310946795646, w0=-0.31466399999413874, gamma=0.6016606653653749\n",
      "Gradient Descent(1842/4999): loss=0.2916931043953407, w0=-0.3146639999941385, gamma=7.4514338033960055\n",
      "Gradient Descent(1843/4999): loss=0.29169301749950943, w0=-0.31466399999413835, gamma=0.30072830409693563\n",
      "Gradient Descent(1844/4999): loss=0.2916919415800361, w0=-0.3146639999941387, gamma=0.05843372688877982\n",
      "Gradient Descent(1845/4999): loss=0.2916918988155318, w0=-0.31466399999413874, gamma=0.055522610041908\n",
      "Gradient Descent(1846/4999): loss=0.2916918897053381, w0=-0.3146639999941388, gamma=0.05906323483690692\n",
      "Gradient Descent(1847/4999): loss=0.29169188168050153, w0=-0.31466399999413885, gamma=0.05293249916746602\n",
      "Gradient Descent(1848/4999): loss=0.29169187315533224, w0=-0.3146639999941389, gamma=0.04327375226747847\n",
      "Gradient Descent(1849/4999): loss=0.2916918655158243, w0=-0.31466399999413897, gamma=0.05005155110683023\n",
      "Gradient Descent(1850/4999): loss=0.2916918592704395, w0=-0.314663999994139, gamma=0.17720886149866186\n",
      "Gradient Descent(1851/4999): loss=0.291691852046932, w0=-0.31466399999413913, gamma=0.4923016180881262\n",
      "Gradient Descent(1852/4999): loss=0.2916918264721563, w0=-0.3146639999941394, gamma=0.08708949937869945\n",
      "Gradient Descent(1853/4999): loss=0.2916917554253072, w0=-0.3146639999941394, gamma=0.03541625860023751\n",
      "Gradient Descent(1854/4999): loss=0.2916917428577919, w0=-0.3146639999941394, gamma=0.035170812734783057\n",
      "Gradient Descent(1855/4999): loss=0.29169173774586593, w0=-0.3146639999941394, gamma=1.1006856423115043\n",
      "Gradient Descent(1856/4999): loss=0.2916917326703524, w0=-0.3146639999941398, gamma=28.317422796290355\n",
      "Gradient Descent(1857/4999): loss=0.29169157383414646, w0=-0.3146639999941424, gamma=0.6098929906929631\n",
      "Gradient Descent(1858/4999): loss=0.2916874908325643, w0=-0.31466399999414335, gamma=0.10024099865571706\n",
      "Gradient Descent(1859/4999): loss=0.291687414001583, w0=-0.3146639999941434, gamma=0.07152135418403777\n",
      "Gradient Descent(1860/4999): loss=0.2916873896578723, w0=-0.31466399999414346, gamma=0.03658075336563307\n",
      "Gradient Descent(1861/4999): loss=0.2916873789254039, w0=-0.31466399999414346, gamma=0.03513744113073715\n",
      "Gradient Descent(1862/4999): loss=0.2916873725014196, w0=-0.31466399999414346, gamma=0.05697609667119442\n",
      "Gradient Descent(1863/4999): loss=0.29168736742402396, w0=-0.3146639999941435, gamma=0.14214382595320144\n",
      "Gradient Descent(1864/4999): loss=0.29168735920768424, w0=-0.3146639999941436, gamma=0.1597508413189932\n",
      "Gradient Descent(1865/4999): loss=0.29168733873684927, w0=-0.31466399999414363, gamma=0.20371626453205402\n",
      "Gradient Descent(1866/4999): loss=0.29168731574240164, w0=-0.31466399999414374, gamma=0.14107980651184868\n",
      "Gradient Descent(1867/4999): loss=0.2916872864216853, w0=-0.3146639999941438, gamma=0.04610992311334787\n",
      "Gradient Descent(1868/4999): loss=0.2916872661167378, w0=-0.3146639999941438, gamma=0.03610338172121713\n",
      "Gradient Descent(1869/4999): loss=0.29168725948017543, w0=-0.3146639999941438, gamma=0.04238582482424334\n",
      "Gradient Descent(1870/4999): loss=0.29168725428399617, w0=-0.3146639999941438, gamma=1.2408987925662553\n",
      "Gradient Descent(1871/4999): loss=0.2916872481836614, w0=-0.3146639999941444, gamma=6.111048084682417\n",
      "Gradient Descent(1872/4999): loss=0.2916870695939626, w0=-0.3146639999941442, gamma=0.09655594223805582\n",
      "Gradient Descent(1873/4999): loss=0.291686190607773, w0=-0.3146639999941443, gamma=0.05584354320373544\n",
      "Gradient Descent(1874/4999): loss=0.2916861764952269, w0=-0.31466399999414435, gamma=0.03559910038022681\n",
      "Gradient Descent(1875/4999): loss=0.29168616839287753, w0=-0.3146639999941444, gamma=0.03518489133885205\n",
      "Gradient Descent(1876/4999): loss=0.2916861632401665, w0=-0.31466399999414446, gamma=0.07596575358434249\n",
      "Gradient Descent(1877/4999): loss=0.29168615817920873, w0=-0.3146639999941445, gamma=0.09899023297652483\n",
      "Gradient Descent(1878/4999): loss=0.291686147252902, w0=-0.31466399999414457, gamma=0.8502344105360281\n",
      "Gradient Descent(1879/4999): loss=0.2916861330152361, w0=-0.31466399999414524, gamma=2.3383041505268474\n",
      "Gradient Descent(1880/4999): loss=0.2916860107303149, w0=-0.3146639999941456, gamma=0.27322450983692603\n",
      "Gradient Descent(1881/4999): loss=0.29168567447167065, w0=-0.3146639999941457, gamma=0.041246815209198026\n",
      "Gradient Descent(1882/4999): loss=0.2916856352572026, w0=-0.3146639999941457, gamma=0.03536266381766286\n",
      "Gradient Descent(1883/4999): loss=0.29168562924600283, w0=-0.3146639999941457, gamma=0.03740965369579182\n",
      "Gradient Descent(1884/4999): loss=0.2916856241578265, w0=-0.3146639999941457, gamma=0.08541168463415892\n",
      "Gradient Descent(1885/4999): loss=0.2916856187785235, w0=-0.3146639999941457, gamma=0.09717150307832646\n",
      "Gradient Descent(1886/4999): loss=0.29168560649754083, w0=-0.3146639999941457, gamma=0.6784771773866197\n",
      "Gradient Descent(1887/4999): loss=0.2916855925259787, w0=-0.3146639999941458, gamma=2.2180788528381785\n",
      "Gradient Descent(1888/4999): loss=0.29168549497511037, w0=-0.31466399999414596, gamma=0.0909842633147834\n",
      "Gradient Descent(1889/4999): loss=0.29168517611003675, w0=-0.314663999994146, gamma=0.03860465015826755\n",
      "Gradient Descent(1890/4999): loss=0.29168516303778314, w0=-0.314663999994146, gamma=0.03567567107737844\n",
      "Gradient Descent(1891/4999): loss=0.2916851574641558, w0=-0.314663999994146, gamma=0.041620108551869685\n",
      "Gradient Descent(1892/4999): loss=0.29168515233530534, w0=-0.314663999994146, gamma=0.11625145383937552\n",
      "Gradient Descent(1893/4999): loss=0.29168514635243725, w0=-0.31466399999414607, gamma=0.25849762570272045\n",
      "Gradient Descent(1894/4999): loss=0.2916851296417955, w0=-0.3146639999941461, gamma=0.2926520942136541\n",
      "Gradient Descent(1895/4999): loss=0.29168509248505187, w0=-0.3146639999941462, gamma=0.0715646063090519\n",
      "Gradient Descent(1896/4999): loss=0.2916850504204253, w0=-0.3146639999941462, gamma=0.05072103645344812\n",
      "Gradient Descent(1897/4999): loss=0.2916850401337193, w0=-0.3146639999941462, gamma=0.043048989056533385\n",
      "Gradient Descent(1898/4999): loss=0.2916850328433162, w0=-0.3146639999941462, gamma=0.053968513677290456\n",
      "Gradient Descent(1899/4999): loss=0.2916850266557421, w0=-0.3146639999941462, gamma=0.3948947194743613\n",
      "Gradient Descent(1900/4999): loss=0.29168501889870707, w0=-0.3146639999941463, gamma=1.2713060659927335\n",
      "Gradient Descent(1901/4999): loss=0.2916849621401511, w0=-0.3146639999941465, gamma=0.04675637351783222\n",
      "Gradient Descent(1902/4999): loss=0.2916847794378873, w0=-0.3146639999941465, gamma=0.03504149549814164\n",
      "Gradient Descent(1903/4999): loss=0.2916847727059666, w0=-0.3146639999941465, gamma=0.03511625513080756\n",
      "Gradient Descent(1904/4999): loss=0.2916847676685462, w0=-0.3146639999941465, gamma=44.12474553710764\n",
      "Gradient Descent(1905/4999): loss=0.29168476262197324, w0=-0.3146639999941501, gamma=112.02073054179694\n",
      "Gradient Descent(1906/4999): loss=0.29167842745227424, w0=-0.31466399999447103, gamma=0.0947091207055419\n",
      "Gradient Descent(1907/4999): loss=0.29166877477930125, w0=-0.3146639999944424, gamma=0.08503672183687348\n",
      "Gradient Descent(1908/4999): loss=0.2916628293457906, w0=-0.3146639999944192, gamma=0.09088739528947201\n",
      "Gradient Descent(1909/4999): loss=0.29166247787930344, w0=-0.3146639999943964, gamma=0.12626507701191342\n",
      "Gradient Descent(1910/4999): loss=0.29166241367341283, w0=-0.31466399999436767, gamma=0.08869757475391046\n",
      "Gradient Descent(1911/4999): loss=0.2916623779526878, w0=-0.31466399999435, gamma=0.03991009699056511\n",
      "Gradient Descent(1912/4999): loss=0.29166236367694415, w0=-0.3146639999943428, gamma=0.0359677560582991\n",
      "Gradient Descent(1913/4999): loss=0.29166235465503393, w0=-0.31466399999433653, gamma=0.05001961027910746\n",
      "Gradient Descent(1914/4999): loss=0.2916623491521474, w0=-0.31466399999432815, gamma=0.20065744631164117\n",
      "Gradient Descent(1915/4999): loss=0.2916623417363896, w0=-0.31466399999429623, gamma=0.22753941079797627\n",
      "Gradient Descent(1916/4999): loss=0.2916623127883665, w0=-0.31466399999426725, gamma=0.06428087954202304\n",
      "Gradient Descent(1917/4999): loss=0.29166228052089765, w0=-0.3146639999942609, gamma=0.035865711584657435\n",
      "Gradient Descent(1918/4999): loss=0.2916622714039712, w0=-0.31466399999425765, gamma=0.03511672816030481\n",
      "Gradient Descent(1919/4999): loss=0.29166226631517866, w0=-0.31466399999425454, gamma=0.09049719301522917\n",
      "Gradient Descent(1920/4999): loss=0.2916622613357538, w0=-0.31466399999424677, gamma=1.3270732769593805\n",
      "Gradient Descent(1921/4999): loss=0.29166224850364364, w0=-0.3146639999941436, gamma=2.8375393824944704\n",
      "Gradient Descent(1922/4999): loss=0.2916620603368137, w0=-0.3146639999942163, gamma=0.05882735126626919\n",
      "Gradient Descent(1923/4999): loss=0.2916616583044422, w0=-0.3146639999942136, gamma=0.053693044070509656\n",
      "Gradient Descent(1924/4999): loss=0.2916616497210416, w0=-0.3146639999942112, gamma=0.051342366504587726\n",
      "Gradient Descent(1925/4999): loss=0.2916616421079426, w0=-0.3146639999942091, gamma=0.042123755140670335\n",
      "Gradient Descent(1926/4999): loss=0.29166163483040614, w0=-0.3146639999942074, gamma=0.06159826632533415\n",
      "Gradient Descent(1927/4999): loss=0.2916616288596171, w0=-0.3146639999942051, gamma=0.40523952366708826\n",
      "Gradient Descent(1928/4999): loss=0.29166162012848934, w0=-0.3146639999941907, gamma=0.6368239943114126\n",
      "Gradient Descent(1929/4999): loss=0.29166156268945814, w0=-0.31466399999417727, gamma=0.043135465235413555\n",
      "Gradient Descent(1930/4999): loss=0.2916614724339419, w0=-0.31466399999417694, gamma=0.03506690198061083\n",
      "Gradient Descent(1931/4999): loss=0.2916614663156176, w0=-0.31466399999417666, gamma=0.03560481439057258\n",
      "Gradient Descent(1932/4999): loss=0.2916614613453385, w0=-0.31466399999417644, gamma=1.3219152229930937\n",
      "Gradient Descent(1933/4999): loss=0.29166145629910695, w0=-0.3146639999941677, gamma=2.0379441588175466\n",
      "Gradient Descent(1934/4999): loss=0.29166126895171385, w0=-0.3146639999941723, gamma=0.18891542048573745\n",
      "Gradient Descent(1935/4999): loss=0.2916609802030585, w0=-0.31466399999417194, gamma=0.12173864754200128\n",
      "Gradient Descent(1936/4999): loss=0.29166095340483494, w0=-0.3146639999941717, gamma=0.09132532023235872\n",
      "Gradient Descent(1937/4999): loss=0.2916609361467815, w0=-0.3146639999941716, gamma=0.06946617034796605\n",
      "Gradient Descent(1938/4999): loss=0.29166092320614667, w0=-0.3146639999941715, gamma=0.03777580227893301\n",
      "Gradient Descent(1939/4999): loss=0.29166091336387334, w0=-0.31466399999417144, gamma=0.036637468541653784\n",
      "Gradient Descent(1940/4999): loss=0.29166090801159, w0=-0.3146639999941714, gamma=0.25393532610757297\n",
      "Gradient Descent(1941/4999): loss=0.2916609028206999, w0=-0.3146639999941711, gamma=2.6119784854466084\n",
      "Gradient Descent(1942/4999): loss=0.2916608668428, w0=-0.3146639999941691, gamma=2.0856427272143194\n",
      "Gradient Descent(1943/4999): loss=0.2916604968009082, w0=-0.31466399999417244, gamma=0.03812744689403809\n",
      "Gradient Descent(1944/4999): loss=0.2916602023700416, w0=-0.3146639999941724, gamma=0.035557408396309206\n",
      "Gradient Descent(1945/4999): loss=0.29166019602485765, w0=-0.3146639999941723, gamma=0.04747511338403062\n",
      "Gradient Descent(1946/4999): loss=0.29166019096067164, w0=-0.31466399999417227, gamma=0.0797071679831474\n",
      "Gradient Descent(1947/4999): loss=0.29166018422906576, w0=-0.31466399999417216, gamma=0.08316809522559394\n",
      "Gradient Descent(1948/4999): loss=0.2916601729390195, w0=-0.31466399999417205, gamma=0.20799131163293172\n",
      "Gradient Descent(1949/4999): loss=0.2916601611606106, w0=-0.31466399999417183, gamma=0.2253940114127128\n",
      "Gradient Descent(1950/4999): loss=0.29166013170499944, w0=-0.31466399999417166, gamma=0.04432537787545593\n",
      "Gradient Descent(1951/4999): loss=0.2916600997858347, w0=-0.31466399999417166, gamma=0.035434386865511786\n",
      "Gradient Descent(1952/4999): loss=0.29166009350830896, w0=-0.31466399999417166, gamma=0.03938384401516933\n",
      "Gradient Descent(1953/4999): loss=0.29166008849026737, w0=-0.31466399999417166, gamma=6.131718581831077\n",
      "Gradient Descent(1954/4999): loss=0.2916600829129738, w0=-0.31466399999416833, gamma=41.02624207369316\n",
      "Gradient Descent(1955/4999): loss=0.29165921469118694, w0=-0.314663999994321, gamma=0.07465557206397594\n",
      "Gradient Descent(1956/4999): loss=0.2916534667410246, w0=-0.3146639999943102, gamma=0.05439005611330026\n",
      "Gradient Descent(1957/4999): loss=0.29165341191902894, w0=-0.314663999994303, gamma=0.04045883232686591\n",
      "Gradient Descent(1958/4999): loss=0.29165339675803215, w0=-0.3146639999942979, gamma=0.037159703501858564\n",
      "Gradient Descent(1959/4999): loss=0.2916533894783696, w0=-0.3146639999942934, gamma=0.06478864577487396\n",
      "Gradient Descent(1960/4999): loss=0.2916533839391199, w0=-0.3146639999942859, gamma=0.13158488609442912\n",
      "Gradient Descent(1961/4999): loss=0.29165337458076795, w0=-0.31466399999427164, gamma=0.1312638020086376\n",
      "Gradient Descent(1962/4999): loss=0.29165335593824543, w0=-0.3146639999942593, gamma=0.03802125622890768\n",
      "Gradient Descent(1963/4999): loss=0.2916533374255793, w0=-0.3146639999942562, gamma=0.03507986246156532\n",
      "Gradient Descent(1964/4999): loss=0.29165333205910104, w0=-0.31466399999425343, gamma=0.04048035249709407\n",
      "Gradient Descent(1965/4999): loss=0.2916533271110753, w0=-0.3146639999942504, gamma=1.5762116719481087\n",
      "Gradient Descent(1966/4999): loss=0.2916533214013586, w0=-0.3146639999941359, gamma=3.1250281124663775\n",
      "Gradient Descent(1967/4999): loss=0.2916530990864598, w0=-0.31466399999426753, gamma=0.16691674246170152\n",
      "Gradient Descent(1968/4999): loss=0.29165265850363825, w0=-0.31466399999425265, gamma=0.08514673164636842\n",
      "Gradient Descent(1969/4999): loss=0.29165263490438625, w0=-0.3146639999942463, gamma=0.0572830262090734\n",
      "Gradient Descent(1970/4999): loss=0.2916526228638911, w0=-0.31466399999424244, gamma=0.05363557270694594\n",
      "Gradient Descent(1971/4999): loss=0.2916526147767905, w0=-0.314663999994239, gamma=0.03716657475189547\n",
      "Gradient Descent(1972/4999): loss=0.29165260721439623, w0=-0.3146639999942368, gamma=0.04000290800803272\n",
      "Gradient Descent(1973/4999): loss=0.29165260197418125, w0=-0.31466399999423444, gamma=0.3144530953065578\n",
      "Gradient Descent(1974/4999): loss=0.2916525963341686, w0=-0.31466399999421685, gamma=0.5148355131044733\n",
      "Gradient Descent(1975/4999): loss=0.29165255200013085, w0=-0.31466399999419714, gamma=0.36292327573802313\n",
      "Gradient Descent(1976/4999): loss=0.29165247941709554, w0=-0.3146639999941904, gamma=0.03940548735073418\n",
      "Gradient Descent(1977/4999): loss=0.29165242825767906, w0=-0.31466399999419, gamma=0.03511355744253132\n",
      "Gradient Descent(1978/4999): loss=0.2916524226979101, w0=-0.3146639999941896, gamma=0.03849577435221658\n",
      "Gradient Descent(1979/4999): loss=0.29165241774771106, w0=-0.31466399999418915, gamma=0.32757019908165913\n",
      "Gradient Descent(1980/4999): loss=0.2916524123208091, w0=-0.3146639999941857, gamma=0.4985116620798199\n",
      "Gradient Descent(1981/4999): loss=0.2916523661425593, w0=-0.3146639999941822, gamma=0.3317453503151728\n",
      "Gradient Descent(1982/4999): loss=0.29165229586870217, w0=-0.31466399999418104, gamma=0.14974368265243998\n",
      "Gradient Descent(1983/4999): loss=0.2916522491054131, w0=-0.3146639999941807, gamma=0.08891721958805276\n",
      "Gradient Descent(1984/4999): loss=0.291652227997262, w0=-0.31466399999418054, gamma=0.0478276216310549\n",
      "Gradient Descent(1985/4999): loss=0.29165221546364944, w0=-0.3146639999941805, gamma=0.038577713200254625\n",
      "Gradient Descent(1986/4999): loss=0.29165220872191816, w0=-0.31466399999418043, gamma=0.046506192697564126\n",
      "Gradient Descent(1987/4999): loss=0.2916522032841436, w0=-0.3146639999941804, gamma=0.8846047610853239\n",
      "Gradient Descent(1988/4999): loss=0.2916521967288359, w0=-0.31466399999417904, gamma=8.238323340484893\n",
      "Gradient Descent(1989/4999): loss=0.2916520720415239, w0=-0.31466399999417854, gamma=0.0530840600582852\n",
      "Gradient Descent(1990/4999): loss=0.2916509115246426, w0=-0.31466399999417866, gamma=0.03534957967140607\n",
      "Gradient Descent(1991/4999): loss=0.29165090372507346, w0=-0.3146639999941787, gamma=0.03511999545865359\n",
      "Gradient Descent(1992/4999): loss=0.2916508986377002, w0=-0.31466399999417877, gamma=0.1091229817781407\n",
      "Gradient Descent(1993/4999): loss=0.2916508936900499, w0=-0.31466399999417893, gamma=0.12757605023575835\n",
      "Gradient Descent(1994/4999): loss=0.29165087831932063, w0=-0.3146639999941791, gamma=0.22007739661747752\n",
      "Gradient Descent(1995/4999): loss=0.2916508603508108, w0=-0.3146639999941794, gamma=0.2187110312025252\n",
      "Gradient Descent(1996/4999): loss=0.29165082935434705, w0=-0.3146639999941796, gamma=0.07787143288521854\n",
      "Gradient Descent(1997/4999): loss=0.291650798551052, w0=-0.31466399999417966, gamma=0.05967971184916396\n",
      "Gradient Descent(1998/4999): loss=0.29165078758354057, w0=-0.3146639999941797, gamma=0.06806614278920292\n",
      "Gradient Descent(1999/4999): loss=0.29165077917834187, w0=-0.31466399999417977, gamma=0.16263617892177462\n",
      "Gradient Descent(2000/4999): loss=0.29165076959209074, w0=-0.3146639999941799, gamma=0.1793241076241572\n",
      "Gradient Descent(2001/4999): loss=0.29165074668703717, w0=-0.31466399999417993, gamma=0.04591005825028751\n",
      "Gradient Descent(2002/4999): loss=0.291650721432242, w0=-0.31466399999417993, gamma=0.03567344044101393\n",
      "Gradient Descent(2003/4999): loss=0.29165071496643385, w0=-0.31466399999417993, gamma=0.04168121791847642\n",
      "Gradient Descent(2004/4999): loss=0.291650709942451, w0=-0.31466399999417993, gamma=19.760043681643136\n",
      "Gradient Descent(2005/4999): loss=0.29165070407241733, w0=-0.31466399999419015, gamma=515.7760401114817\n",
      "Gradient Descent(2006/4999): loss=0.2916479223808119, w0=-0.31466399999058675, gamma=0.03681150645570358\n",
      "Gradient Descent(2007/4999): loss=0.29159191253523736, w0=-0.31466399999072203, gamma=0.03505228862411987\n",
      "Gradient Descent(2008/4999): loss=0.29157622107105635, w0=-0.3146639999908458, gamma=0.03602314726495164\n",
      "Gradient Descent(2009/4999): loss=0.2915761689119743, w0=-0.3146639999909686, gamma=0.05780132470771485\n",
      "Gradient Descent(2010/4999): loss=0.2915761629570616, w0=-0.3146639999911585, gamma=0.05798728361287789\n",
      "Gradient Descent(2011/4999): loss=0.29157615498251543, w0=-0.31466399999133804, gamma=0.10849120327731668\n",
      "Gradient Descent(2012/4999): loss=0.29157614716647856, w0=-0.31466399999165445, gamma=0.12153163363825929\n",
      "Gradient Descent(2013/4999): loss=0.2915761325448183, w0=-0.3146639999919704, gamma=1.069406012767127\n",
      "Gradient Descent(2014/4999): loss=0.29157611616613366, w0=-0.314663999994413, gamma=1.7271949308507413\n",
      "Gradient Descent(2015/4999): loss=0.29157597204754343, w0=-0.3146639999941396, gamma=0.0390999712973834\n",
      "Gradient Descent(2016/4999): loss=0.29157573942184745, w0=-0.3146639999941441, gamma=0.035397103908037214\n",
      "Gradient Descent(2017/4999): loss=0.29157573403942966, w0=-0.314663999994148, gamma=0.040519128191634085\n",
      "Gradient Descent(2018/4999): loss=0.29157572926697367, w0=-0.31466399999415234, gamma=0.078135519960988\n",
      "Gradient Descent(2019/4999): loss=0.2915757238069732, w0=-0.31466399999416034, gamma=0.08738593220477206\n",
      "Gradient Descent(2020/4999): loss=0.29157571327900267, w0=-0.3146639999941686, gamma=0.7074399792779246\n",
      "Gradient Descent(2021/4999): loss=0.2915757015048882, w0=-0.3146639999942297, gamma=2.2365516243469368\n",
      "Gradient Descent(2022/4999): loss=0.29157560618855743, w0=-0.31466399999428646, gamma=0.06371667256094815\n",
      "Gradient Descent(2023/4999): loss=0.2915753048976449, w0=-0.31466399999428446, gamma=0.03548949403472754\n",
      "Gradient Descent(2024/4999): loss=0.29157529630528084, w0=-0.3146639999942834, gamma=0.03514384750054379\n",
      "Gradient Descent(2025/4999): loss=0.2915752915113644, w0=-0.3146639999942824, gamma=0.16058741149763886\n",
      "Gradient Descent(2026/4999): loss=0.29157528677698513, w0=-0.3146639999942781, gamma=0.2424816267611416\n",
      "Gradient Descent(2027/4999): loss=0.291575265144472, w0=-0.3146639999942726, gamma=0.3187194672485037\n",
      "Gradient Descent(2028/4999): loss=0.2915752324816433, w0=-0.3146639999942671, gamma=0.14557630503669142\n",
      "Gradient Descent(2029/4999): loss=0.2915751895505358, w0=-0.31466399999426536, gamma=0.07583657126873258\n",
      "Gradient Descent(2030/4999): loss=0.29157516994194554, w0=-0.31466399999426464, gamma=0.0624017799370545\n",
      "Gradient Descent(2031/4999): loss=0.29157515972698417, w0=-0.3146639999942641, gamma=0.04863441209989229\n",
      "Gradient Descent(2032/4999): loss=0.29157515132181305, w0=-0.31466399999426364, gamma=0.05551504471595739\n",
      "Gradient Descent(2033/4999): loss=0.29157514477106167, w0=-0.3146639999942632, gamma=0.1707268308005611\n",
      "Gradient Descent(2034/4999): loss=0.2915751372935659, w0=-0.31466399999426187, gamma=0.43208678418693613\n",
      "Gradient Descent(2035/4999): loss=0.2915751142979875, w0=-0.3146639999942591, gamma=0.0615066887804629\n",
      "Gradient Descent(2036/4999): loss=0.2915750561008332, w0=-0.31466399999425887, gamma=0.03523809448082372\n",
      "Gradient Descent(2037/4999): loss=0.291575047816493, w0=-0.31466399999425876, gamma=0.03539276654374454\n",
      "Gradient Descent(2038/4999): loss=0.29157504307011617, w0=-0.31466399999425865, gamma=10.40376482471101\n",
      "Gradient Descent(2039/4999): loss=0.29157503830322823, w0=-0.3146639999942272, gamma=3716.8224086800137\n",
      "Gradient Descent(2040/4999): loss=0.2915736373510334, w0=-0.3146640001048708, gamma=0.4837327771422582\n",
      "Gradient Descent(2041/4999): loss=0.29111589002963634, w0=-0.31466400005159934, gamma=0.046447340174814965\n",
      "Gradient Descent(2042/4999): loss=0.2913729659746684, w0=-0.3146640000489572, gamma=0.0362351120571145\n",
      "Gradient Descent(2043/4999): loss=0.2911518592490727, w0=-0.31466400004699285, gamma=0.038137572580190744\n",
      "Gradient Descent(2044/4999): loss=0.2911187812764288, w0=-0.314664000045, gamma=0.09813004968489122\n",
      "Gradient Descent(2045/4999): loss=0.2911128578649424, w0=-0.314664000040068, gamma=0.10225504119249913\n",
      "Gradient Descent(2046/4999): loss=0.29110902174403125, w0=-0.31466400003543316, gamma=0.06479071425269119\n",
      "Gradient Descent(2047/4999): loss=0.2911089998172367, w0=-0.31466400003279676, gamma=0.036363551091130444\n",
      "Gradient Descent(2048/4999): loss=0.2911089917910934, w0=-0.3146640000314129, gamma=0.03509379663227674\n",
      "Gradient Descent(2049/4999): loss=0.29110898644617283, w0=-0.314664000030126, gamma=0.047391719831031284\n",
      "Gradient Descent(2050/4999): loss=0.2911089827954815, w0=-0.3146640000284491, gamma=0.12031804802587008\n",
      "Gradient Descent(2051/4999): loss=0.29110897787447837, w0=-0.3146640000243935, gamma=0.14796039796600072\n",
      "Gradient Descent(2052/4999): loss=0.2911089653910373, w0=-0.3146640000200062, gamma=0.17680052371240212\n",
      "Gradient Descent(2053/4999): loss=0.29110895004545984, w0=-0.31466400001553946, gamma=0.1029073293641574\n",
      "Gradient Descent(2054/4999): loss=0.29110893171001107, w0=-0.3146640000133992, gamma=0.06621121718822831\n",
      "Gradient Descent(2055/4999): loss=0.29110892103790953, w0=-0.3146640000121639, gamma=0.05458321100823985\n",
      "Gradient Descent(2056/4999): loss=0.2911089141714475, w0=-0.3146640000112129, gamma=0.05419212201097138\n",
      "Gradient Descent(2057/4999): loss=0.291108908510906, w0=-0.3146640000103203, gamma=0.09542890393152548\n",
      "Gradient Descent(2058/4999): loss=0.2911089028909385, w0=-0.31466400000883366, gamma=0.2337193503339061\n",
      "Gradient Descent(2059/4999): loss=0.29110889299456005, w0=-0.3146640000055401, gamma=0.10204394023499556\n",
      "Gradient Descent(2060/4999): loss=0.29110886875704073, w0=-0.3146640000044382, gamma=0.03710420755186503\n",
      "Gradient Descent(2061/4999): loss=0.2911088581748693, w0=-0.31466400000407846, gamma=0.035597808336168046\n",
      "Gradient Descent(2062/4999): loss=0.2911088543269917, w0=-0.3146640000037461, gamma=0.19660464899650243\n",
      "Gradient Descent(2063/4999): loss=0.2911088506354216, w0=-0.31466400000197586, gamma=189.96313440651187\n",
      "Gradient Descent(2064/4999): loss=0.2911088302471613, w0=-0.31466399862778727, gamma=26.612430095246637\n",
      "Gradient Descent(2065/4999): loss=0.2910891712844988, w0=-0.31466403500616513, gamma=0.03506626078793256\n",
      "Gradient Descent(2066/4999): loss=0.2911144447981184, w0=-0.3146640337784459, gamma=0.03503247741144229\n",
      "Gradient Descent(2067/4999): loss=0.29108645001774147, w0=-0.31466403259491976, gamma=0.0663695073711144\n",
      "Gradient Descent(2068/4999): loss=0.29108644520624316, w0=-0.3146640304312634, gamma=0.07977048138505714\n",
      "Gradient Descent(2069/4999): loss=0.29108643723817207, w0=-0.3146640280033291, gamma=0.4282166311564986\n",
      "Gradient Descent(2070/4999): loss=0.2910864280585654, w0=-0.31466401600959515, gamma=0.8139595448140777\n",
      "Gradient Descent(2071/4999): loss=0.2910863798468922, w0=-0.314664002974169, gamma=0.29154793563128273\n",
      "Gradient Descent(2072/4999): loss=0.29108629158388477, w0=-0.3146640021055289, gamma=0.05854383480549183\n",
      "Gradient Descent(2073/4999): loss=0.29108626259321263, w0=-0.3146640019819565, gamma=0.0558812512156407\n",
      "Gradient Descent(2074/4999): loss=0.29108625418686795, w0=-0.31466400187090954, gamma=0.06347476932741598\n",
      "Gradient Descent(2075/4999): loss=0.291086248241243, w0=-0.31466400175182146, gamma=0.24310391552400015\n",
      "Gradient Descent(2076/4999): loss=0.2910862415230728, w0=-0.31466400132467326, gamma=1.807955689729596\n",
      "Gradient Descent(2077/4999): loss=0.29108621584018707, w0=-0.3146639989202513, gamma=0.16917613558811714\n",
      "Gradient Descent(2078/4999): loss=0.29108602682772383, w0=-0.31466399910203263, gamma=0.03585766160948001\n",
      "Gradient Descent(2079/4999): loss=0.29108601294225683, w0=-0.31466399913404375, gamma=0.035169667942617874\n",
      "Gradient Descent(2080/4999): loss=0.2910860053438276, w0=-0.31466399916431487, gamma=0.053958431392063265\n",
      "Gradient Descent(2081/4999): loss=0.2910860016899143, w0=-0.3146639992091244, gamma=0.07321745392749168\n",
      "Gradient Descent(2082/4999): loss=0.29108599610946595, w0=-0.3146639992666467, gamma=0.16733434726967047\n",
      "Gradient Descent(2083/4999): loss=0.2910859885450503, w0=-0.3146639993884851, gamma=3.386792325593272\n",
      "Gradient Descent(2084/4999): loss=0.2910859712627934, w0=-0.31466400144181383, gamma=3.027918945297154\n",
      "Gradient Descent(2085/4999): loss=0.2910856224071675, w0=-0.3146639970602523, gamma=0.03588268386037524\n",
      "Gradient Descent(2086/4999): loss=0.2910853225945699, w0=-0.31466399716555027, gamma=0.03517780046305868\n",
      "Gradient Descent(2087/4999): loss=0.29108530783144027, w0=-0.31466399726507566, gamma=0.055255947159258426\n",
      "Gradient Descent(2088/4999): loss=0.2910853041467991, w0=-0.31466399741590695, gamma=0.08080752378457523\n",
      "Gradient Descent(2089/4999): loss=0.2910852984409428, w0=-0.3146639976242977, gamma=0.11979578701448548\n",
      "Gradient Descent(2090/4999): loss=0.2910852901294764, w0=-0.31466399790826916, gamma=0.18834285357041286\n",
      "Gradient Descent(2091/4999): loss=0.29108527781897564, w0=-0.3146639983012448, gamma=0.4843796670627862\n",
      "Gradient Descent(2092/4999): loss=0.29108525846942884, w0=-0.3146639991215493, gamma=0.10496221562857175\n",
      "Gradient Descent(2093/4999): loss=0.2910852087183143, w0=-0.3146639992132035, gamma=0.037143124487308894\n",
      "Gradient Descent(2094/4999): loss=0.29108519794180854, w0=-0.31466399924223293, gamma=0.03540343693152646\n",
      "Gradient Descent(2095/4999): loss=0.2910851941223024, w0=-0.314663999268875, gamma=0.04547567936395684\n",
      "Gradient Descent(2096/4999): loss=0.2910851904863148, w0=-0.31466399930188516, gamma=0.12045551302536323\n",
      "Gradient Descent(2097/4999): loss=0.29108518581596227, w0=-0.3146639993853458, gamma=0.7529722868019096\n",
      "Gradient Descent(2098/4999): loss=0.29108517344525003, w0=-0.314663999844218, gamma=0.8744133210585375\n",
      "Gradient Descent(2099/4999): loss=0.29108509611679007, w0=-0.3146639999758542, gamma=0.06334914468195241\n",
      "Gradient Descent(2100/4999): loss=0.29108500632642376, w0=-0.3146639999770519, gamma=0.05344908500200877\n",
      "Gradient Descent(2101/4999): loss=0.29108499981475666, w0=-0.31466399997799843, gamma=0.03961064564818553\n",
      "Gradient Descent(2102/4999): loss=0.29108499432574725, w0=-0.3146639999786624, gamma=0.03631129532034983\n",
      "Gradient Descent(2103/4999): loss=0.2910849902580661, w0=-0.31466399997924693, gamma=0.09683780267558056\n",
      "Gradient Descent(2104/4999): loss=0.29108498652924636, w0=-0.31466399998074923, gamma=3.6124067791831496\n",
      "Gradient Descent(2105/4999): loss=0.2910849765849626, w0=-0.31466400003136374, gamma=10.198291493502609\n",
      "Gradient Descent(2106/4999): loss=0.29108460564200267, w0=-0.3146639996580756, gamma=0.039802993236741434\n",
      "Gradient Descent(2107/4999): loss=0.29108356191667845, w0=-0.31466399967147674, gamma=0.03588975827976854\n",
      "Gradient Descent(2108/4999): loss=0.29108355473443315, w0=-0.31466399968307934, gamma=0.04265257925380475\n",
      "Gradient Descent(2109/4999): loss=0.29108355091607163, w0=-0.3146639996963734, gamma=0.07244017890634748\n",
      "Gradient Descent(2110/4999): loss=0.2910835465140627, w0=-0.31466399971798864, gamma=0.07380507922929815\n",
      "Gradient Descent(2111/4999): loss=0.29108353907393525, w0=-0.3146639997384159, gamma=0.16352835065664367\n",
      "Gradient Descent(2112/4999): loss=0.2910835314989801, w0=-0.3146639997803357, gamma=0.12355937169422766\n",
      "Gradient Descent(2113/4999): loss=0.29108351471590377, w0=-0.31466399980683, gamma=0.03790535285307554\n",
      "Gradient Descent(2114/4999): loss=0.2910835020353058, w0=-0.31466399981395365, gamma=0.035367692136396164\n",
      "Gradient Descent(2115/4999): loss=0.29108349814498297, w0=-0.31466399982034843, gamma=0.08086641926724912\n",
      "Gradient Descent(2116/4999): loss=0.29108349451527044, w0=-0.3146639998344526, gamma=2.5254356598069743\n",
      "Gradient Descent(2117/4999): loss=0.2910834862161402, w0=-0.3146640002393027, gamma=4.598972951043056\n",
      "Gradient Descent(2118/4999): loss=0.29108322704483114, w0=-0.31466399911466575, gamma=0.14281143356763415\n",
      "Gradient Descent(2119/4999): loss=0.29108275523156196, w0=-0.3146639992403536, gamma=0.0395562151894715\n",
      "Gradient Descent(2120/4999): loss=0.2910827406654109, w0=-0.31466399927019517, gamma=0.03569380928702167\n",
      "Gradient Descent(2121/4999): loss=0.29108273644271015, w0=-0.3146639992960577, gamma=0.03985159739738038\n",
      "Gradient Descent(2122/4999): loss=0.29108273277422003, w0=-0.3146639993239022, gamma=0.10701675416625366\n",
      "Gradient Descent(2123/4999): loss=0.2910827286832373, w0=-0.31466399939569545, gamma=0.20801738293446115\n",
      "Gradient Descent(2124/4999): loss=0.29108271770005917, w0=-0.31466399952031177, gamma=0.17892479616829943\n",
      "Gradient Descent(2125/4999): loss=0.2910826963562408, w0=-0.3146639996052027, gamma=0.0607795112100043\n",
      "Gradient Descent(2126/4999): loss=0.2910826779995879, w0=-0.3146639996288799, gamma=0.05313193483385181\n",
      "Gradient Descent(2127/4999): loss=0.29108267176340274, w0=-0.31466399964832, gamma=0.04114458834289306\n",
      "Gradient Descent(2128/4999): loss=0.2910826663123524, w0=-0.3146639996625742, gamma=0.05371533660820472\n",
      "Gradient Descent(2129/4999): loss=0.2910826620911624, w0=-0.3146639996804178, gamma=0.4692818728352305\n",
      "Gradient Descent(2130/4999): loss=0.29108265658031085, w0=-0.3146639998279339, gamma=0.9853647811710419\n",
      "Gradient Descent(2131/4999): loss=0.2910826084354797, w0=-0.31466399999232075, gamma=0.04386043159928683\n",
      "Gradient Descent(2132/4999): loss=0.2910825073514297, w0=-0.31466399999242783, gamma=0.03504396736206427\n",
      "Gradient Descent(2133/4999): loss=0.29108250284815823, w0=-0.31466399999250966, gamma=0.03521791973843494\n",
      "Gradient Descent(2134/4999): loss=0.29108249925287616, w0=-0.314663999992589, gamma=9.06593668736127\n",
      "Gradient Descent(2135/4999): loss=0.29108249564001154, w0=-0.3146640000122925, gamma=12.113099566819944\n",
      "Gradient Descent(2136/4999): loss=0.2910815656934728, w0=-0.31466399979995746, gamma=0.2559969175064823\n",
      "Gradient Descent(2137/4999): loss=0.2910803273396714, w0=-0.3146639998498273, gamma=0.14312257911643617\n",
      "Gradient Descent(2138/4999): loss=0.2910802981436296, w0=-0.314663999870571, gamma=0.05772382328803516\n",
      "Gradient Descent(2139/4999): loss=0.29108028420254434, w0=-0.31466399987773985, gamma=0.057538377273836046\n",
      "Gradient Descent(2140/4999): loss=0.2910802768958542, w0=-0.31466399988447324, gamma=0.037974857607649515\n",
      "Gradient Descent(2141/4999): loss=0.2910802709926443, w0=-0.3146639998886615, gamma=0.040619731805708055\n",
      "Gradient Descent(2142/4999): loss=0.2910802670971195, w0=-0.31466399989297134, gamma=0.36197450422642935\n",
      "Gradient Descent(2143/4999): loss=0.29108026293110884, w0=-0.3146639999298177, gamma=0.6432778643083112\n",
      "Gradient Descent(2144/4999): loss=0.2910802258151272, w0=-0.3146639999715963, gamma=0.09523603657376613\n",
      "Gradient Descent(2145/4999): loss=0.29108015988292013, w0=-0.3146639999738027, gamma=0.03521867475889125\n",
      "Gradient Descent(2146/4999): loss=0.29108015012632543, w0=-0.3146639999745409, gamma=0.03504605663388378\n",
      "Gradient Descent(2147/4999): loss=0.29108014651066777, w0=-0.3146639999752497, gamma=0.18122656601549586\n",
      "Gradient Descent(2148/4999): loss=0.2910801429188719, w0=-0.31466399997878625, gamma=0.6857539068371554\n",
      "Gradient Descent(2149/4999): loss=0.29108012434541947, w0=-0.3146639999897432, gamma=0.4296003859784853\n",
      "Gradient Descent(2150/4999): loss=0.29108005406525844, w0=-0.31466399999190026, gamma=0.07776019174153667\n",
      "Gradient Descent(2151/4999): loss=0.2910800100402203, w0=-0.31466399999212297, gamma=0.07279651567715291\n",
      "Gradient Descent(2152/4999): loss=0.29108000206924495, w0=-0.31466399999231526, gamma=0.10600001443874194\n",
      "Gradient Descent(2153/4999): loss=0.29107999460892764, w0=-0.3146639999925749, gamma=0.8799756555340166\n",
      "Gradient Descent(2154/4999): loss=0.2910799837459082, w0=-0.31466399999450156, gamma=0.3074939767932829\n",
      "Gradient Descent(2155/4999): loss=0.2910798935660884, w0=-0.3146639999945824, gamma=0.03628489543059355\n",
      "Gradient Descent(2156/4999): loss=0.2910798620606761, w0=-0.314663999994589, gamma=0.03532794976277302\n",
      "Gradient Descent(2157/4999): loss=0.29107985833667754, w0=-0.3146639999945952, gamma=0.04839738939660649\n",
      "Gradient Descent(2158/4999): loss=0.29107985471638614, w0=-0.3146639999946034, gamma=0.09799017024189317\n",
      "Gradient Descent(2159/4999): loss=0.29107984975683443, w0=-0.3146639999946192, gamma=4.654136157203701\n",
      "Gradient Descent(2160/4999): loss=0.2910798397152654, w0=-0.3146639999952954, gamma=6.625917624485384\n",
      "Gradient Descent(2161/4999): loss=0.2910793628061976, w0=-0.31466399999177985, gamma=0.05310402817643082\n",
      "Gradient Descent(2162/4999): loss=0.29107868681050414, w0=-0.3146639999919384, gamma=0.05064004209517856\n",
      "Gradient Descent(2163/4999): loss=0.2910786785936032, w0=-0.31466399999208156, gamma=0.03508410936664804\n",
      "Gradient Descent(2164/4999): loss=0.29107867337574334, w0=-0.3146639999921757, gamma=0.03506670654346662\n",
      "Gradient Descent(2165/4999): loss=0.29107866977457353, w0=-0.3146639999922665, gamma=0.25317472682407566\n",
      "Gradient Descent(2166/4999): loss=0.2910786661827645, w0=-0.3146639999928992, gamma=0.3149012487544091\n",
      "Gradient Descent(2167/4999): loss=0.2910786402509847, w0=-0.3146639999934869, gamma=0.2114739904935513\n",
      "Gradient Descent(2168/4999): loss=0.29107860799737273, w0=-0.31466399999375727, gamma=0.07915631142184622\n",
      "Gradient Descent(2169/4999): loss=0.2910785863375893, w0=-0.3146639999938371, gamma=0.06546732466687212\n",
      "Gradient Descent(2170/4999): loss=0.29107857823007, w0=-0.3146639999938979, gamma=0.07361301559413148\n",
      "Gradient Descent(2171/4999): loss=0.2910785715247468, w0=-0.3146639999939617, gamma=0.4807501555295845\n",
      "Gradient Descent(2172/4999): loss=0.2910785639851532, w0=-0.31466399999434813, gamma=2.950736453468208\n",
      "Gradient Descent(2173/4999): loss=0.2910785147461167, w0=-0.31466399999558, gamma=0.08992157095878066\n",
      "Gradient Descent(2174/4999): loss=0.2910782125529022, w0=-0.31466399999550676, gamma=0.0464109849271104\n",
      "Gradient Descent(2175/4999): loss=0.2910782033415892, w0=-0.31466399999547234, gamma=0.03920452589879194\n",
      "Gradient Descent(2176/4999): loss=0.29107819858089157, w0=-0.31466399999544464, gamma=0.036028491752967476\n",
      "Gradient Descent(2177/4999): loss=0.2910781945655887, w0=-0.31466399999542016, gamma=0.04482649719783258\n",
      "Gradient Descent(2178/4999): loss=0.29107819087603076, w0=-0.31466399999539085, gamma=0.3050301862080468\n",
      "Gradient Descent(2179/4999): loss=0.29107818628551924, w0=-0.31466399999520017, gamma=7.270512902154265\n",
      "Gradient Descent(2180/4999): loss=0.29107815504868323, w0=-0.31466399999204103, gamma=0.39771465699880043\n",
      "Gradient Descent(2181/4999): loss=0.2910774105783177, w0=-0.31466399999312494, gamma=0.04768715092261492\n",
      "Gradient Descent(2182/4999): loss=0.2910773702466841, w0=-0.3146639999932032, gamma=0.04302374979495046\n",
      "Gradient Descent(2183/4999): loss=0.2910773649911529, w0=-0.3146639999932705, gamma=0.03645623361613771\n",
      "Gradient Descent(2184/4999): loss=0.2910773605695243, w0=-0.314663999993325, gamma=0.03684680361381555\n",
      "Gradient Descent(2185/4999): loss=0.2910773568364471, w0=-0.31466399999337813, gamma=0.07861947024184307\n",
      "Gradient Descent(2186/4999): loss=0.2910773530643064, w0=-0.31466399999348726, gamma=0.12958404652122485\n",
      "Gradient Descent(2187/4999): loss=0.29107734501587734, w0=-0.314663999993653, gamma=0.2582378581436745\n",
      "Gradient Descent(2188/4999): loss=0.29107733175023626, w0=-0.31466399999394057, gamma=0.25458959842932427\n",
      "Gradient Descent(2189/4999): loss=0.29107730531438764, w0=-0.3146639999941508, gamma=0.05854363046692496\n",
      "Gradient Descent(2190/4999): loss=0.2910772792524615, w0=-0.3146639999941868, gamma=0.04292705683640971\n",
      "Gradient Descent(2191/4999): loss=0.29107727325928867, w0=-0.3146639999942117, gamma=0.036964905311580004\n",
      "Gradient Descent(2192/4999): loss=0.29107726886490665, w0=-0.31466399999423217, gamma=0.06962331950568415\n",
      "Gradient Descent(2193/4999): loss=0.29107726508088894, w0=-0.31466399999426936, gamma=9.097800926735014\n",
      "Gradient Descent(2194/4999): loss=0.2910772579537154, w0=-0.31466399999879124, gamma=11.449648412519789\n",
      "Gradient Descent(2195/4999): loss=0.29107632672394074, w0=-0.31466399995271715, gamma=0.03539125342428343\n",
      "Gradient Descent(2196/4999): loss=0.29107518347512407, w0=-0.31466399995420535, gamma=0.03510948860627221\n",
      "Gradient Descent(2197/4999): loss=0.2910751516707281, w0=-0.3146639999556295, gamma=0.050658391426541945\n",
      "Gradient Descent(2198/4999): loss=0.29107514803279044, w0=-0.3146639999576122, gamma=0.05540338017754595\n",
      "Gradient Descent(2199/4999): loss=0.29107514284458336, w0=-0.31466399995967076, gamma=0.1086693482535121\n",
      "Gradient Descent(2200/4999): loss=0.29107513717750283, w0=-0.31466399996348476, gamma=0.13541812405878115\n",
      "Gradient Descent(2201/4999): loss=0.29107512606270736, w0=-0.3146639999677211, gamma=0.6558178819093925\n",
      "Gradient Descent(2202/4999): loss=0.2910751122123292, w0=-0.31466399998545896, gamma=0.9749484689545834\n",
      "Gradient Descent(2203/4999): loss=0.29107504513695753, w0=-0.31466399999453487, gamma=0.041175167246502734\n",
      "Gradient Descent(2204/4999): loss=0.2910749454344467, w0=-0.3146639999945445, gamma=0.036140734936432405\n",
      "Gradient Descent(2205/4999): loss=0.2910749412140476, w0=-0.3146639999945526, gamma=0.0371268211121338\n",
      "Gradient Descent(2206/4999): loss=0.29107493751756375, w0=-0.3146639999945606, gamma=0.06866450395829037\n",
      "Gradient Descent(2207/4999): loss=0.2910749337205598, w0=-0.31466399999457484, gamma=0.20449994046184863\n",
      "Gradient Descent(2208/4999): loss=0.29107492669819224, w0=-0.3146639999946144, gamma=1.5112979969128353\n",
      "Gradient Descent(2209/4999): loss=0.2910749057839354, w0=-0.3146639999948469, gamma=0.29370822986351286\n",
      "Gradient Descent(2210/4999): loss=0.29107475122704213, w0=-0.31466399999482386, gamma=0.05023128714094753\n",
      "Gradient Descent(2211/4999): loss=0.2910747212018283, w0=-0.3146639999948211, gamma=0.045781292863530054\n",
      "Gradient Descent(2212/4999): loss=0.29107471605475105, w0=-0.3146639999948187, gamma=0.03578668967708575\n",
      "Gradient Descent(2213/4999): loss=0.2910747113727249, w0=-0.3146639999948169, gamma=0.03691551934067422\n",
      "Gradient Descent(2214/4999): loss=0.2910747077130895, w0=-0.31466399999481515, gamma=0.5259647592606886\n",
      "Gradient Descent(2215/4999): loss=0.2910747039380584, w0=-0.31466399999479056, gamma=1.4295098312431938\n",
      "Gradient Descent(2216/4999): loss=0.29107465015257883, w0=-0.3146639999947591, gamma=0.18195610945785523\n",
      "Gradient Descent(2217/4999): loss=0.2910745039766623, w0=-0.3146639999947608, gamma=0.0672254644073379\n",
      "Gradient Descent(2218/4999): loss=0.29107448537226316, w0=-0.3146639999947613, gamma=0.04996099969842271\n",
      "Gradient Descent(2219/4999): loss=0.2910744784955218, w0=-0.3146639999947617, gamma=0.039163418348899685\n",
      "Gradient Descent(2220/4999): loss=0.29107447338655146, w0=-0.31466399999476197, gamma=0.03777128406707955\n",
      "Gradient Descent(2221/4999): loss=0.2910744693819796, w0=-0.3146639999947622, gamma=0.12362824583171464\n",
      "Gradient Descent(2222/4999): loss=0.2910744655198053, w0=-0.31466399999476297, gamma=0.455781440771021\n",
      "Gradient Descent(2223/4999): loss=0.29107445287866673, w0=-0.3146639999947655, gamma=2.0697710612951132\n",
      "Gradient Descent(2224/4999): loss=0.29107440627488573, w0=-0.314663999994772, gamma=0.05119712355926633\n",
      "Gradient Descent(2225/4999): loss=0.29107419465810835, w0=-0.31466399999477185, gamma=0.035428202253710316\n",
      "Gradient Descent(2226/4999): loss=0.29107418941540003, w0=-0.31466399999477174, gamma=0.035062462601165946\n",
      "Gradient Descent(2227/4999): loss=0.2910741857911766, w0=-0.3146639999947716, gamma=0.16560786721423737\n",
      "Gradient Descent(2228/4999): loss=0.2910741822064021, w0=-0.3146639999947712, gamma=2.590350626517623\n",
      "Gradient Descent(2229/4999): loss=0.2910741652747598, w0=-0.3146639999947652, gamma=0.8959602532984257\n",
      "Gradient Descent(2230/4999): loss=0.29107390044792114, w0=-0.31466399999476863, gamma=0.05695406110785702\n",
      "Gradient Descent(2231/4999): loss=0.29107380895672597, w0=-0.31466399999476863, gamma=0.056051516589839805\n",
      "Gradient Descent(2232/4999): loss=0.2910738030346048, w0=-0.3146639999947687, gamma=0.06102506124900119\n",
      "Gradient Descent(2233/4999): loss=0.2910737973043924, w0=-0.3146639999947687, gamma=0.07887346287319087\n",
      "Gradient Descent(2234/4999): loss=0.291073791066171, w0=-0.31466399999476874, gamma=0.10236102381377245\n",
      "Gradient Descent(2235/4999): loss=0.29107378300346803, w0=-0.3146639999947688, gamma=0.07783115685220711\n",
      "Gradient Descent(2236/4999): loss=0.29107377253983424, w0=-0.3146639999947688, gamma=0.046475156689304145\n",
      "Gradient Descent(2237/4999): loss=0.2910737645837448, w0=-0.3146639999947688, gamma=0.04269327072180694\n",
      "Gradient Descent(2238/4999): loss=0.29107375983294903, w0=-0.3146639999947688, gamma=0.10682469475247405\n",
      "Gradient Descent(2239/4999): loss=0.2910737554687587, w0=-0.31466399999476885, gamma=1.5354569186461593\n",
      "Gradient Descent(2240/4999): loss=0.29107374454895585, w0=-0.3146639999947693, gamma=0.3924268407347036\n",
      "Gradient Descent(2241/4999): loss=0.2910735875951826, w0=-0.3146639999947693, gamma=0.035226751577520164\n",
      "Gradient Descent(2242/4999): loss=0.29107354750761694, w0=-0.3146639999947693, gamma=0.03503381554144658\n",
      "Gradient Descent(2243/4999): loss=0.2910735438826226, w0=-0.3146639999947693, gamma=0.09554463699750444\n",
      "Gradient Descent(2244/4999): loss=0.2910735403017103, w0=-0.3146639999947693, gamma=773.1388872375229\n",
      "Gradient Descent(2245/4999): loss=0.2910735305358262, w0=-0.31466399999478933, gamma=2636.5820578642492\n",
      "Gradient Descent(2246/4999): loss=0.2909951398837373, w0=-0.31466400010314105, gamma=0.05574082239608493\n",
      "Gradient Descent(2247/4999): loss=0.31478848545454474, w0=-0.31466400009712137, gamma=0.05398462521801144\n",
      "Gradient Descent(2248/4999): loss=0.2912712443984711, w0=-0.3146640000916138, gamma=0.07782584370117902\n",
      "Gradient Descent(2249/4999): loss=0.2909026765625273, w0=-0.3146640000841007, gamma=0.09439887821478751\n",
      "Gradient Descent(2250/4999): loss=0.29076746849335255, w0=-0.3146640000756941, gamma=0.04934248677375042\n",
      "Gradient Descent(2251/4999): loss=0.290750232468837, w0=-0.3146640000717145, gamma=0.035429892968080044\n",
      "Gradient Descent(2252/4999): loss=0.2907427246939584, w0=-0.3146640000689977, gamma=0.0359424152981172\n",
      "Gradient Descent(2253/4999): loss=0.29074112024664417, w0=-0.31466400006633927, gamma=0.1284252457041361\n",
      "Gradient Descent(2254/4999): loss=0.2907409520843236, w0=-0.31466400005718176, gamma=0.13152778238906873\n",
      "Gradient Descent(2255/4999): loss=0.29074075386012943, w0=-0.3146640000490072, gamma=0.1591616644820626\n",
      "Gradient Descent(2256/4999): loss=0.2907407396414826, w0=-0.31466400004041617, gamma=0.061892396259102124\n",
      "Gradient Descent(2257/4999): loss=0.2907407252167237, w0=-0.31466400003760714, gamma=0.035277238313474096\n",
      "Gradient Descent(2258/4999): loss=0.29074071962866277, w0=-0.3146640000361052, gamma=0.03522377911923809\n",
      "Gradient Descent(2259/4999): loss=0.29074071641957194, w0=-0.3146640000346584, gamma=0.21721295854418662\n",
      "Gradient Descent(2260/4999): loss=0.2907407132612567, w0=-0.31466400002605077, gamma=0.2319804693316484\n",
      "Gradient Descent(2261/4999): loss=0.29074069379425893, w0=-0.31466400001885475, gamma=0.1495225959092557\n",
      "Gradient Descent(2262/4999): loss=0.290740673010789, w0=-0.31466400001529254, gamma=0.06904410739322621\n",
      "Gradient Descent(2263/4999): loss=0.29074065961495676, w0=-0.3146640000138936, gamma=0.05859576516606811\n",
      "Gradient Descent(2264/4999): loss=0.2907406534292302, w0=-0.3146640000127883, gamma=0.0711902577241787\n",
      "Gradient Descent(2265/4999): loss=0.2907406481796184, w0=-0.31466400001152417, gamma=0.15216814233080217\n",
      "Gradient Descent(2266/4999): loss=0.2907406418016781, w0=-0.3146640000090144, gamma=0.14659361208196042\n",
      "Gradient Descent(2267/4999): loss=0.29074062816896523, w0=-0.3146640000069645, gamma=0.04503071355308115\n",
      "Gradient Descent(2268/4999): loss=0.290740615035762, w0=-0.3146640000064271, gamma=0.03602034212076621\n",
      "Gradient Descent(2269/4999): loss=0.2907406110014751, w0=-0.3146640000060166, gamma=0.04712812405450009\n",
      "Gradient Descent(2270/4999): loss=0.2907406077744492, w0=-0.3146640000054988, gamma=15.092559379738772\n",
      "Gradient Descent(2271/4999): loss=0.290740603552297, w0=-0.31466399984751114, gamma=125.62234158872639\n",
      "Gradient Descent(2272/4999): loss=0.29073925159188346, w0=-0.3146640183793963, gamma=0.03574094660703313\n",
      "Gradient Descent(2273/4999): loss=0.2907285854398951, w0=-0.31466401772232205, gamma=0.03503503481518881\n",
      "Gradient Descent(2274/4999): loss=0.29072801117269237, w0=-0.31466401710124614, gamma=0.03591982725626156\n",
      "Gradient Descent(2275/4999): loss=0.2907280077590098, w0=-0.3146640164867942, gamma=0.06859931302538876\n",
      "Gradient Descent(2276/4999): loss=0.2907280045313415, w0=-0.3146640153554712, gamma=0.15067764332205985\n",
      "Gradient Descent(2277/4999): loss=0.2907279983799667, w0=-0.31466401304099767, gamma=0.4037447975234761\n",
      "Gradient Descent(2278/4999): loss=0.29072798488939094, w0=-0.31466400777376025, gamma=0.19946465878940778\n",
      "Gradient Descent(2279/4999): loss=0.2907279488237757, w0=-0.3146640062221805, gamma=0.05434451206724847\n",
      "Gradient Descent(2280/4999): loss=0.29072793106210143, w0=-0.3146640058837696, gamma=0.053678088989442506\n",
      "Gradient Descent(2281/4999): loss=0.2907279261750702, w0=-0.3146640055676739, gamma=0.22620317384891875\n",
      "Gradient Descent(2282/4999): loss=0.29072792138693065, w0=-0.3146640043071265, gamma=0.735136335042147\n",
      "Gradient Descent(2283/4999): loss=0.29072790121031633, w0=-0.3146640011371552, gamma=0.6381969165385615\n",
      "Gradient Descent(2284/4999): loss=0.29072783564505195, w0=-0.314664000408261, gamma=0.03656470816153949\n",
      "Gradient Descent(2285/4999): loss=0.29072777873745564, w0=-0.3146640003931517, gamma=0.03510332506874394\n",
      "Gradient Descent(2286/4999): loss=0.2907277754677873, w0=-0.31466400037917663, gamma=0.04357521447191996\n",
      "Gradient Descent(2287/4999): loss=0.2907277723371772, w0=-0.3146640003624378, gamma=0.10862631774017246\n",
      "Gradient Descent(2288/4999): loss=0.29072776845106946, w0=-0.3146640003225288, gamma=0.2635282843725952\n",
      "Gradient Descent(2289/4999): loss=0.2907277587636383, w0=-0.3146640002362263, gamma=0.9238303510425451\n",
      "Gradient Descent(2290/4999): loss=0.29072773526197765, w0=-0.3146640000134114, gamma=0.18419906172919828\n",
      "Gradient Descent(2291/4999): loss=0.29072765287551006, w0=-0.3146640000100275, gamma=0.06851161167522363\n",
      "Gradient Descent(2292/4999): loss=0.29072763644968125, w0=-0.3146640000090007, gamma=0.06378597181260193\n",
      "Gradient Descent(2293/4999): loss=0.2907276303391492, w0=-0.31466400000811023, gamma=0.043390521360014624\n",
      "Gradient Descent(2294/4999): loss=0.29072762465090474, w0=-0.31466400000754313, gamma=0.04064542104181381\n",
      "Gradient Descent(2295/4999): loss=0.29072762078147796, w0=-0.3146640000070349, gamma=0.13099715245762308\n",
      "Gradient Descent(2296/4999): loss=0.29072761715685996, w0=-0.3146640000054637, gamma=3.6189183012540096\n",
      "Gradient Descent(2297/4999): loss=0.29072760547501175, w0=-0.31466399996774275, gamma=0.5327150011120187\n",
      "Gradient Descent(2298/4999): loss=0.29072728276397447, w0=-0.3146639999822848, gamma=0.03512161719675183\n",
      "Gradient Descent(2299/4999): loss=0.29072723539202816, w0=-0.3146639999827328, gamma=0.03504519765317192\n",
      "Gradient Descent(2300/4999): loss=0.29072723213199364, w0=-0.31466399998316413, gamma=0.060307798149780406\n",
      "Gradient Descent(2301/4999): loss=0.29072722900716724, w0=-0.3146639999838804, gamma=0.07733616684075752\n",
      "Gradient Descent(2302/4999): loss=0.29072722362986403, w0=-0.3146639999847435, gamma=1.4916936175568516\n",
      "Gradient Descent(2303/4999): loss=0.2907272167342652, w0=-0.31466400000010386, gamma=28.935657782471765\n",
      "Gradient Descent(2304/4999): loss=0.2907270837307746, w0=-0.3146639998536026, gamma=0.09989745367530947\n",
      "Gradient Descent(2305/4999): loss=0.2907245051604957, w0=-0.31466399986773214, gamma=0.054925368863157804\n",
      "Gradient Descent(2306/4999): loss=0.29072449601352246, w0=-0.3146639998747247, gamma=0.05436769440099341\n",
      "Gradient Descent(2307/4999): loss=0.2907244906379551, w0=-0.31466399988126614, gamma=0.0486579777812241\n",
      "Gradient Descent(2308/4999): loss=0.2907244857941572, w0=-0.31466399988680227, gamma=0.03927322545230651\n",
      "Gradient Descent(2309/4999): loss=0.2907244814597421, w0=-0.3146639998910532, gamma=0.04701195399924139\n",
      "Gradient Descent(2310/4999): loss=0.29072447796135853, w0=-0.31466399989594196, gamma=0.2562294502443927\n",
      "Gradient Descent(2311/4999): loss=0.29072447377364274, w0=-0.31466399992133454, gamma=0.599324494249794\n",
      "Gradient Descent(2312/4999): loss=0.29072445094943555, w0=-0.3146639999655097, gamma=0.11452124273814938\n",
      "Gradient Descent(2313/4999): loss=0.29072439756405627, w0=-0.3146639999688919, gamma=0.036394546681837905\n",
      "Gradient Descent(2314/4999): loss=0.29072438736347284, w0=-0.3146639999698436, gamma=0.0351367031571746\n",
      "Gradient Descent(2315/4999): loss=0.29072438412114315, w0=-0.314663999970729, gamma=0.07482467519493566\n",
      "Gradient Descent(2316/4999): loss=0.2907243809913599, w0=-0.3146639999725483, gamma=0.8422558928298328\n",
      "Gradient Descent(2317/4999): loss=0.2907243743264043, w0=-0.3146639999914944, gamma=66.94177526671352\n",
      "Gradient Descent(2318/4999): loss=0.29072429930368765, w0=-0.31466400022903956, gamma=1.6707892289713095\n",
      "Gradient Descent(2319/4999): loss=0.29071833997925917, w0=-0.3146639998380878, gamma=0.04931906677098241\n",
      "Gradient Descent(2320/4999): loss=0.29071827735773087, w0=-0.3146639998458289, gamma=0.03974135599957301\n",
      "Gradient Descent(2321/4999): loss=0.2907181984824391, w0=-0.314663999851759, gamma=0.03744961634687925\n",
      "Gradient Descent(2322/4999): loss=0.29071818547809547, w0=-0.3146639998571251, gamma=0.05889967625144128\n",
      "Gradient Descent(2323/4999): loss=0.29071818097746477, w0=-0.3146639998652487, gamma=0.10290117819167652\n",
      "Gradient Descent(2324/4999): loss=0.29071817511849873, w0=-0.3146639998786051, gamma=0.12051529367282206\n",
      "Gradient Descent(2325/4999): loss=0.29071816574207154, w0=-0.3146639998926382, gamma=0.058460304321650336\n",
      "Gradient Descent(2326/4999): loss=0.2907181550064508, w0=-0.3146639998986251, gamma=0.036177672890607186\n",
      "Gradient Descent(2327/4999): loss=0.29071814980222627, w0=-0.3146639999021134, gamma=0.03656684089539304\n",
      "Gradient Descent(2328/4999): loss=0.29071814658209905, w0=-0.3146639999055117, gamma=0.2763283622054287\n",
      "Gradient Descent(2329/4999): loss=0.29071814333026574, w0=-0.31466399993025296, gamma=0.34581295753543423\n",
      "Gradient Descent(2330/4999): loss=0.2907181187637739, w0=-0.31466399995265976, gamma=0.17481334271250915\n",
      "Gradient Descent(2331/4999): loss=0.2907180880287106, w0=-0.3146639999600697, gamma=0.0462044934200668\n",
      "Gradient Descent(2332/4999): loss=0.2907180724922882, w0=-0.31466399996168587, gamma=0.03663663219545737\n",
      "Gradient Descent(2333/4999): loss=0.29071806838545977, w0=-0.3146639999629081, gamma=0.03798664324239434\n",
      "Gradient Descent(2334/4999): loss=0.2907180651293108, w0=-0.31466399996412897, gamma=0.13106282850722775\n",
      "Gradient Descent(2335/4999): loss=0.29071806175322157, w0=-0.31466399996818123, gamma=0.37767371822960116\n",
      "Gradient Descent(2336/4999): loss=0.29071805010495444, w0=-0.31466399997832795, gamma=0.5491291928769713\n",
      "Gradient Descent(2337/4999): loss=0.2907180165392496, w0=-0.3146639999875091, gamma=0.08632249007678242\n",
      "Gradient Descent(2338/4999): loss=0.2907179677365534, w0=-0.31466399998815986, gamma=0.04988553949524916\n",
      "Gradient Descent(2339/4999): loss=0.29071796006455375, w0=-0.3146639999885035, gamma=0.03559243102382477\n",
      "Gradient Descent(2340/4999): loss=0.2907179556309766, w0=-0.3146639999887364, gamma=0.03633851260143492\n",
      "Gradient Descent(2341/4999): loss=0.2907179524677725, w0=-0.3146639999889657, gamma=0.7444771538182606\n",
      "Gradient Descent(2342/4999): loss=0.2907179492382938, w0=-0.31466399999349354, gamma=3.263921131022414\n",
      "Gradient Descent(2343/4999): loss=0.2907178830755292, w0=-0.31466399999856626, gamma=0.10999536603209885\n",
      "Gradient Descent(2344/4999): loss=0.29071759303246314, w0=-0.31466399999817923, gamma=0.07196298475593674\n",
      "Gradient Descent(2345/4999): loss=0.29071758324813973, w0=-0.3146639999979539, gamma=0.0623623577635892\n",
      "Gradient Descent(2346/4999): loss=0.290717576849884, w0=-0.31466399999777267, gamma=0.03522982433785311\n",
      "Gradient Descent(2347/4999): loss=0.2907175713082831, w0=-0.3146639999976767, gamma=0.03532159655728667\n",
      "Gradient Descent(2348/4999): loss=0.29071756817765443, w0=-0.3146639999975838, gamma=7.116796506894199\n",
      "Gradient Descent(2349/4999): loss=0.29071756503897844, w0=-0.3146639999795359, gamma=112.88224856400919\n",
      "Gradient Descent(2350/4999): loss=0.29071693267650073, w0=-0.31466400173060755, gamma=0.08991514948710976\n",
      "Gradient Descent(2351/4999): loss=0.29070696286649195, w0=-0.31466400157455515, gamma=0.07369535945656136\n",
      "Gradient Descent(2352/4999): loss=0.29070690772289365, w0=-0.3146640014581534, gamma=0.05095298656089492\n",
      "Gradient Descent(2353/4999): loss=0.29070689906119274, w0=-0.3146640013836042, gamma=0.03764167493063162\n",
      "Gradient Descent(2354/4999): loss=0.29070689415355266, w0=-0.3146640013313369, gamma=0.03514978328555345\n",
      "Gradient Descent(2355/4999): loss=0.2907068907607157, w0=-0.3146640012843669, gamma=0.04078144052598205\n",
      "Gradient Descent(2356/4999): loss=0.29070688764821345, w0=-0.31466400123178695, gamma=0.11158290095753108\n",
      "Gradient Descent(2357/4999): loss=0.29070688403771977, w0=-0.3146640010937889, gamma=0.13439074066670004\n",
      "Gradient Descent(2358/4999): loss=0.29070687415947766, w0=-0.31466400094612934, gamma=0.22551173655029258\n",
      "Gradient Descent(2359/4999): loss=0.2907068622624078, w0=-0.3146640007316511, gamma=0.18593793776013925\n",
      "Gradient Descent(2360/4999): loss=0.29070684229895183, w0=-0.3146640005946901, gamma=0.06880408122008241\n",
      "Gradient Descent(2361/4999): loss=0.29070682583895197, w0=-0.3146640005534328, gamma=0.05451641129256702\n",
      "Gradient Descent(2362/4999): loss=0.2907068197480943, w0=-0.3146640005229921, gamma=0.05250081702443076\n",
      "Gradient Descent(2363/4999): loss=0.2907068149220942, w0=-0.314664000495275, gamma=0.08829071931255272\n",
      "Gradient Descent(2364/4999): loss=0.2907068102745352, w0=-0.31466400045111026, gamma=0.24630618565496368\n",
      "Gradient Descent(2365/4999): loss=0.2907068024587443, w0=-0.3146640003387812, gamma=0.12632030637336353\n",
      "Gradient Descent(2366/4999): loss=0.29070678065499944, w0=-0.31466400029536173, gamma=0.03750170402728728\n",
      "Gradient Descent(2367/4999): loss=0.2907067694729042, w0=-0.31466400028409974, gamma=0.035392939777530975\n",
      "Gradient Descent(2368/4999): loss=0.2907067661530911, w0=-0.31466400027386965, gamma=0.10738563396153816\n",
      "Gradient Descent(2369/4999): loss=0.2907067630200394, w0=-0.31466400024392904, gamma=38.27187912348425\n",
      "Gradient Descent(2370/4999): loss=0.2907067535140727, w0=-0.31466399071908374, gamma=35.90622566347261\n",
      "Gradient Descent(2371/4999): loss=0.29070336668215246, w0=-0.31466432378432285, gamma=0.03749402793919129\n",
      "Gradient Descent(2372/4999): loss=0.2907004509673738, w0=-0.3146643116441592, gamma=0.03503579023306443\n",
      "Gradient Descent(2373/4999): loss=0.2907002051390783, w0=-0.3146643007252868, gamma=0.035358568739643305\n",
      "Gradient Descent(2374/4999): loss=0.29070020014280723, w0=-0.3146642900918964, gamma=0.4261262291649899\n",
      "Gradient Descent(2375/4999): loss=0.29070019635834843, w0=-0.3146641664740259, gamma=0.8729268181157631\n",
      "Gradient Descent(2376/4999): loss=0.29070015232326324, w0=-0.3146640211500727, gamma=0.09770171560772162\n",
      "Gradient Descent(2377/4999): loss=0.2907000728835778, w0=-0.31466401908319175, gamma=0.05883321171481441\n",
      "Gradient Descent(2378/4999): loss=0.29070006243907215, w0=-0.3146640179601757, gamma=0.05757769562428229\n",
      "Gradient Descent(2379/4999): loss=0.2907000559282759, w0=-0.31466401692578577, gamma=0.09600486182894477\n",
      "Gradient Descent(2380/4999): loss=0.29070005076949773, w0=-0.31466401530035387, gamma=0.7421301248074512\n",
      "Gradient Descent(2381/4999): loss=0.2907000421815682, w0=-0.3146640039418348, gamma=1.315419941909682\n",
      "Gradient Descent(2382/4999): loss=0.29069997604063447, w0=-0.3146639987501694, gamma=0.0697850702588258\n",
      "Gradient Descent(2383/4999): loss=0.2906998600159313, w0=-0.31466399883704427, gamma=0.04476832145018859\n",
      "Gradient Descent(2384/4999): loss=0.29069985353287225, w0=-0.31466399888888674, gamma=0.036328126712019015\n",
      "Gradient Descent(2385/4999): loss=0.2906998494713624, w0=-0.314663998929072, gamma=0.035828490799742825\n",
      "Gradient Descent(2386/4999): loss=0.29069984625357165, w0=-0.31466399896726477, gamma=0.08711505111652358\n",
      "Gradient Descent(2387/4999): loss=0.29069984308758984, w0=-0.31466399905680126, gamma=0.22450155754779735\n",
      "Gradient Descent(2388/4999): loss=0.2906998353904314, w0=-0.314663999267442, gamma=0.35291712465735103\n",
      "Gradient Descent(2389/4999): loss=0.29069981555674573, w0=-0.31466399952423113, gamma=0.08735661508240128\n",
      "Gradient Descent(2390/4999): loss=0.2906997843828993, w0=-0.31466399956536123, gamma=0.05257955699846143\n",
      "Gradient Descent(2391/4999): loss=0.29069977666635505, w0=-0.31466399958795466, gamma=0.04776845981270693\n",
      "Gradient Descent(2392/4999): loss=0.29069977202178554, w0=-0.3146639996074015, gamma=0.04655925393067142\n",
      "Gradient Descent(2393/4999): loss=0.29069976780257967, w0=-0.3146639996254506, gamma=0.11753167876423452\n",
      "Gradient Descent(2394/4999): loss=0.2906997636902232, w0=-0.3146639996688915, gamma=0.6482934784302911\n",
      "Gradient Descent(2395/4999): loss=0.2906997533093302, w0=-0.314663999880345, gamma=0.15765803825120026\n",
      "Gradient Descent(2396/4999): loss=0.29069969605194196, w0=-0.3146639998984309, gamma=0.03531121264749134\n",
      "Gradient Descent(2397/4999): loss=0.2906996821324205, w0=-0.314663999901843, gamma=0.03505320216471622\n",
      "Gradient Descent(2398/4999): loss=0.2906996790096451, w0=-0.3146639999051106, gamma=0.3423808284119939\n",
      "Gradient Descent(2399/4999): loss=0.29069967591398904, w0=-0.3146639999359079, gamma=4.849095593657906\n",
      "Gradient Descent(2400/4999): loss=0.2906996456776095, w0=-0.31466400022274704, gamma=4.835242590572212\n",
      "Gradient Descent(2401/4999): loss=0.290699217496334, w0=-0.3146639991218318, gamma=0.05852821569706621\n",
      "Gradient Descent(2402/4999): loss=0.2906987911397767, w0=-0.3146639991729404, gamma=0.053957557980181724\n",
      "Gradient Descent(2403/4999): loss=0.2906987854684522, w0=-0.3146639992173001, gamma=0.04688854068291982\n",
      "Gradient Descent(2404/4999): loss=0.290698780697906, w0=-0.3146639992537682, gamma=0.03788488134392621\n",
      "Gradient Descent(2405/4999): loss=0.29069877655802423, w0=-0.314663999281852, gamma=0.04102553143741843\n",
      "Gradient Descent(2406/4999): loss=0.2906987732134716, w0=-0.31466399931111183, gamma=0.14955488363007516\n",
      "Gradient Descent(2407/4999): loss=0.2906987695917411, w0=-0.31466399941339984, gamma=0.4401059198422465\n",
      "Gradient Descent(2408/4999): loss=0.2906987563891817, w0=-0.3146639996693927, gamma=0.3612939885654636\n",
      "Gradient Descent(2409/4999): loss=0.2906987175379918, w0=-0.31466399978705495, gamma=0.05013576878506649\n",
      "Gradient Descent(2410/4999): loss=0.2906986856466462, w0=-0.31466399979748355, gamma=0.037140843471171046\n",
      "Gradient Descent(2411/4999): loss=0.290698681219742, w0=-0.3146639998048218, gamma=0.036773644809891134\n",
      "Gradient Descent(2412/4999): loss=0.29069867794100135, w0=-0.3146639998118176, gamma=0.08680027000029694\n",
      "Gradient Descent(2413/4999): loss=0.29069867469490346, w0=-0.31466399982772325, gamma=0.17019428611689358\n",
      "Gradient Descent(2414/4999): loss=0.2906986670328859, w0=-0.3146639998562033, gamma=2.5081952597015422\n",
      "Gradient Descent(2415/4999): loss=0.2906986520096206, w0=-0.3146640002044875, gamma=0.6643452741151776\n",
      "Gradient Descent(2416/4999): loss=0.29069843061623685, w0=-0.3146640000653566, gamma=0.03634379979575719\n",
      "Gradient Descent(2417/4999): loss=0.29069837207114696, w0=-0.3146640000628018, gamma=0.035255340922824265\n",
      "Gradient Descent(2418/4999): loss=0.29069836877297706, w0=-0.3146640000604136, gamma=0.05653850717644541\n",
      "Gradient Descent(2419/4999): loss=0.2906983656602318, w0=-0.31466400005671874, gamma=0.07766102075381809\n",
      "Gradient Descent(2420/4999): loss=0.290698360669737, w0=-0.3146640000519304, gamma=0.08908313346063251\n",
      "Gradient Descent(2421/4999): loss=0.29069835381531084, w0=-0.3146640000468644, gamma=3.8735438186485895\n",
      "Gradient Descent(2422/4999): loss=0.29069834595281285, w0=-0.31466399984620547, gamma=4.231108298629274\n",
      "Gradient Descent(2423/4999): loss=0.29069800408424096, w0=-0.31466400047603466, gamma=0.0356701472504748\n",
      "Gradient Descent(2424/4999): loss=0.29069763185499303, w0=-0.3146640004588783, gamma=0.0350905389451836\n",
      "Gradient Descent(2425/4999): loss=0.29069762756079714, w0=-0.3146640004426028, gamma=0.05586226229384237\n",
      "Gradient Descent(2426/4999): loss=0.29069762446108144, w0=-0.3146640004176022, gamma=0.07335132330853228\n",
      "Gradient Descent(2427/4999): loss=0.29069761953086265, w0=-0.31466400038660836, gamma=0.07807719315859765\n",
      "Gradient Descent(2428/4999): loss=0.2906976130584561, w0=-0.31466400035603753, gamma=3.7174011154738373\n",
      "Gradient Descent(2429/4999): loss=0.29069760616913093, w0=-0.31466399901414865, gamma=15.070157964269487\n",
      "Gradient Descent(2430/4999): loss=0.2906972781661238, w0=-0.3146640137966806, gamma=0.04131521576000474\n",
      "Gradient Descent(2431/4999): loss=0.290695950223617, w0=-0.3146640132264638, gamma=0.035196405457430885\n",
      "Gradient Descent(2432/4999): loss=0.29069594519667713, w0=-0.3146640127607661, gamma=0.036127461789982274\n",
      "Gradient Descent(2433/4999): loss=0.2906959420318034, w0=-0.3146640122995737, gamma=0.14947404697025807\n",
      "Gradient Descent(2434/4999): loss=0.2906959388346715, w0=-0.3146640104603688, gamma=0.19182215107914197\n",
      "Gradient Descent(2435/4999): loss=0.2906959256289035, w0=-0.3146640084528915, gamma=0.13217966878257528\n",
      "Gradient Descent(2436/4999): loss=0.29069590870617584, w0=-0.31466400733493866, gamma=0.06031471298992778\n",
      "Gradient Descent(2437/4999): loss=0.2906958970485878, w0=-0.3146640068922362, gamma=0.055478245757374936\n",
      "Gradient Descent(2438/4999): loss=0.29069589172828486, w0=-0.3146640065095932, gamma=0.09059670357546007\n",
      "Gradient Descent(2439/4999): loss=0.2906958868355003, w0=-0.3146640059193983, gamma=0.379394757877143\n",
      "Gradient Descent(2440/4999): loss=0.29069587884565024, w0=-0.31466400367173686, gamma=0.2080890593669052\n",
      "Gradient Descent(2441/4999): loss=0.2906958453872589, w0=-0.3146640029066614, gamma=0.03628382167344569\n",
      "Gradient Descent(2442/4999): loss=0.29069582703838465, w0=-0.31466400280101753, gamma=0.035080798466636996\n",
      "Gradient Descent(2443/4999): loss=0.2906958238370733, w0=-0.31466400270258243, gamma=0.06500550549389893\n",
      "Gradient Descent(2444/4999): loss=0.2906958207434998, w0=-0.31466400252657883, gamma=1.7001229413672392\n",
      "Gradient Descent(2445/4999): loss=0.29069581501105984, w0=-0.31466399822269203, gamma=2.6058598098893073\n",
      "Gradient Descent(2446/4999): loss=0.2906956650920918, w0=-0.31466400284124485, gamma=0.145403471608062\n",
      "Gradient Descent(2447/4999): loss=0.29069543532318576, w0=-0.3146640024274003, gamma=0.053891160013359425\n",
      "Gradient Descent(2448/4999): loss=0.29069542250826325, w0=-0.31466400229631886, gamma=0.051300117403199647\n",
      "Gradient Descent(2449/4999): loss=0.2906954177483741, w0=-0.31466400217826423, gamma=0.03638628354281204\n",
      "Gradient Descent(2450/4999): loss=0.2906954132251595, w0=-0.3146640020988257, gamma=0.037150255402038576\n",
      "Gradient Descent(2451/4999): loss=0.2906954100169785, w0=-0.31466400202067046, gamma=0.3605896048914029\n",
      "Gradient Descent(2452/4999): loss=0.29069540674146704, w0=-0.3146640012902579, gamma=1.1448959925775473\n",
      "Gradient Descent(2453/4999): loss=0.2906953749486916, w0=-0.3146639998073958, gamma=0.19155135102963955\n",
      "Gradient Descent(2454/4999): loss=0.29069527400689826, w0=-0.3146639998433439, gamma=0.06867727314196283\n",
      "Gradient Descent(2455/4999): loss=0.2906952571197174, w0=-0.31466399985376364, gamma=0.049576453907993756\n",
      "Gradient Descent(2456/4999): loss=0.2906952510636357, w0=-0.3146639998607688, gamma=0.03538516438208178\n",
      "Gradient Descent(2457/4999): loss=0.2906952466926003, w0=-0.3146639998655209, gamma=0.03605277250105341\n",
      "Gradient Descent(2458/4999): loss=0.29069524357286847, w0=-0.3146639998701913, gamma=0.7937763579707361\n",
      "Gradient Descent(2459/4999): loss=0.2906952403943159, w0=-0.31466399996931255, gamma=2.7368732356656835\n",
      "Gradient Descent(2460/4999): loss=0.29069517041241083, w0=-0.314664000039792, gamma=0.11102638094013878\n",
      "Gradient Descent(2461/4999): loss=0.2906949291419041, w0=-0.3146640000348261, gamma=0.07785861262203746\n",
      "Gradient Descent(2462/4999): loss=0.29069491934493474, w0=-0.3146640000317303, gamma=0.07029390175661024\n",
      "Gradient Descent(2463/4999): loss=0.2906949124793744, w0=-0.3146640000291529, gamma=0.035860737436177986\n",
      "Gradient Descent(2464/4999): loss=0.29069490628270767, w0=-0.3146640000279305, gamma=0.03556059050096173\n",
      "Gradient Descent(2465/4999): loss=0.2906949031214013, w0=-0.31466400002676176, gamma=0.8383200342038203\n",
      "Gradient Descent(2466/4999): loss=0.2906948999866156, w0=-0.31466400000018935, gamma=8.995152669502659\n",
      "Gradient Descent(2467/4999): loss=0.29069482608644326, w0=-0.31466399995409206, gamma=0.8022913768944739\n",
      "Gradient Descent(2468/4999): loss=0.2906940332199016, w0=-0.3146639999869644, gamma=0.056286956701317545\n",
      "Gradient Descent(2469/4999): loss=0.2906939630713763, w0=-0.3146639999874204, gamma=0.04435624610800583\n",
      "Gradient Descent(2470/4999): loss=0.2906939576214879, w0=-0.3146639999877595, gamma=0.036246376409239986\n",
      "Gradient Descent(2471/4999): loss=0.2906939536554326, w0=-0.3146639999880243, gamma=0.03968842220467286\n",
      "Gradient Descent(2472/4999): loss=0.29069395045328733, w0=-0.31466399998830374, gamma=0.12480114747082323\n",
      "Gradient Descent(2473/4999): loss=0.29069394695278705, w0=-0.3146639999891476, gamma=0.16373730371745343\n",
      "Gradient Descent(2474/4999): loss=0.29069393595031834, w0=-0.31466399999011657, gamma=0.1659947001688986\n",
      "Gradient Descent(2475/4999): loss=0.29069392151985807, w0=-0.314663999990938, gamma=0.06692945825507367\n",
      "Gradient Descent(2476/4999): loss=0.29069390689178937, w0=-0.31466399999121425, gamma=0.03862957398368188\n",
      "Gradient Descent(2477/4999): loss=0.290693900993696, w0=-0.314663999991363, gamma=0.03572118179478344\n",
      "Gradient Descent(2478/4999): loss=0.2906938975894902, w0=-0.31466399999149525, gamma=0.08443204359697964\n",
      "Gradient Descent(2479/4999): loss=0.29069389444165367, w0=-0.3146639999917967, gamma=0.9876755120605123\n",
      "Gradient Descent(2480/4999): loss=0.2906938870013152, w0=-0.31466399999502503, gamma=2.249004954066067\n",
      "Gradient Descent(2481/4999): loss=0.29069379996612865, w0=-0.31466399999511596, gamma=0.12530625691865477\n",
      "Gradient Descent(2482/4999): loss=0.2906936017932669, w0=-0.31466399999510963, gamma=0.036974536085437416\n",
      "Gradient Descent(2483/4999): loss=0.29069359076029233, w0=-0.314663999995108, gamma=0.03540939504843575\n",
      "Gradient Descent(2484/4999): loss=0.29069358749046, w0=-0.3146639999951065, gamma=0.04707970599326257\n",
      "Gradient Descent(2485/4999): loss=0.2906935843702211, w0=-0.3146639999951046, gamma=0.11752102806790297\n",
      "Gradient Descent(2486/4999): loss=0.29069358022177744, w0=-0.3146639999951, gamma=0.30684675455758686\n",
      "Gradient Descent(2487/4999): loss=0.29069356986653977, w0=-0.31466399999508937, gamma=0.348629801199208\n",
      "Gradient Descent(2488/4999): loss=0.29069354282959187, w0=-0.31466399999508105, gamma=0.07529225483887654\n",
      "Gradient Descent(2489/4999): loss=0.29069351211157984, w0=-0.3146639999950799, gamma=0.04966349904355102\n",
      "Gradient Descent(2490/4999): loss=0.2906935054774525, w0=-0.31466399999507916, gamma=0.04090480763525336\n",
      "Gradient Descent(2491/4999): loss=0.2906935011015629, w0=-0.3146639999950786, gamma=0.04527203877871553\n",
      "Gradient Descent(2492/4999): loss=0.2906934974974376, w0=-0.314663999995078, gamma=0.38879923363924096\n",
      "Gradient Descent(2493/4999): loss=0.2906934935085238, w0=-0.31466399999507316, gamma=4.204028035419189\n",
      "Gradient Descent(2494/4999): loss=0.2906934592516105, w0=-0.31466399999504147, gamma=0.07640956186208996\n",
      "Gradient Descent(2495/4999): loss=0.29069308886213, w0=-0.31466399999504335, gamma=0.035054441656587075\n",
      "Gradient Descent(2496/4999): loss=0.2906930821345795, w0=-0.31466399999504413, gamma=0.035036348576014974\n",
      "Gradient Descent(2497/4999): loss=0.2906930790324226, w0=-0.3146639999950449, gamma=0.19157605901053304\n",
      "Gradient Descent(2498/4999): loss=0.2906930759458263, w0=-0.3146639999950489, gamma=1.1911648549454046\n",
      "Gradient Descent(2499/4999): loss=0.29069305906859805, w0=-0.31466399999506917, gamma=0.23194631166893415\n",
      "Gradient Descent(2500/4999): loss=0.29069295413254137, w0=-0.3146639999950684, gamma=0.054143567236781216\n",
      "Gradient Descent(2501/4999): loss=0.2906929337023568, w0=-0.3146639999950683, gamma=0.053756045025141634\n",
      "Gradient Descent(2502/4999): loss=0.2906929289297995, w0=-0.31466399999506817, gamma=0.2513645793366045\n",
      "Gradient Descent(2503/4999): loss=0.29069292419430504, w0=-0.3146639999950676, gamma=23.688378992968367\n",
      "Gradient Descent(2504/4999): loss=0.29069290205109105, w0=-0.31466399999502903, gamma=3.5014133850962934\n",
      "Gradient Descent(2505/4999): loss=0.2906908156953105, w0=-0.3146639999951637, gamma=0.06326482854797161\n",
      "Gradient Descent(2506/4999): loss=0.2906905273607812, w0=-0.31466399999515765, gamma=0.05679380021172901\n",
      "Gradient Descent(2507/4999): loss=0.2906905028670755, w0=-0.31466399999515254, gamma=0.03748018288955637\n",
      "Gradient Descent(2508/4999): loss=0.290690497157215, w0=-0.3146639999951494, gamma=0.03538058103165023\n",
      "Gradient Descent(2509/4999): loss=0.29069049361753635, w0=-0.3146639999951465, gamma=0.048483989843842286\n",
      "Gradient Descent(2510/4999): loss=0.29069049049842605, w0=-0.31466399999514266, gamma=0.07859408519266493\n",
      "Gradient Descent(2511/4999): loss=0.29069048622957133, w0=-0.3146639999951368, gamma=0.08005054798346685\n",
      "Gradient Descent(2512/4999): loss=0.29069047931171854, w0=-0.3146639999951313, gamma=0.6564395685648239\n",
      "Gradient Descent(2513/4999): loss=0.2906904722659377, w0=-0.31466399999508965, gamma=0.7319799692672827\n",
      "Gradient Descent(2514/4999): loss=0.2906904144887451, w0=-0.31466399999507366, gamma=0.037334996560476306\n",
      "Gradient Descent(2515/4999): loss=0.29069035006926186, w0=-0.31466399999507344, gamma=0.035048698109053436\n",
      "Gradient Descent(2516/4999): loss=0.2906903467781738, w0=-0.3146639999950733, gamma=0.03685118005156435\n",
      "Gradient Descent(2517/4999): loss=0.29069034369342284, w0=-0.31466399999507305, gamma=0.42614328147194563\n",
      "Gradient Descent(2518/4999): loss=0.2906903404500582, w0=-0.31466399999507083, gamma=1.6063283589359936\n",
      "Gradient Descent(2519/4999): loss=0.29069030294428705, w0=-0.3146639999950661, gamma=0.13345399353186405\n",
      "Gradient Descent(2520/4999): loss=0.29069016157284655, w0=-0.3146639999950664, gamma=0.06932011009547394\n",
      "Gradient Descent(2521/4999): loss=0.2906901498276902, w0=-0.3146639999950665, gamma=0.06636746063027046\n",
      "Gradient Descent(2522/4999): loss=0.2906901437256012, w0=-0.3146639999950666, gamma=0.06515279732653538\n",
      "Gradient Descent(2523/4999): loss=0.29069013788482795, w0=-0.3146639999950667, gamma=0.2883061498320707\n",
      "Gradient Descent(2524/4999): loss=0.29069013215098244, w0=-0.3146639999950671, gamma=1.653743787833872\n",
      "Gradient Descent(2525/4999): loss=0.2906901067783725, w0=-0.3146639999950686, gamma=0.0705113563367434\n",
      "Gradient Descent(2526/4999): loss=0.29068996124402574, w0=-0.31466399999506856, gamma=0.03545832298219533\n",
      "Gradient Descent(2527/4999): loss=0.2906899550388936, w0=-0.31466399999506856, gamma=0.03507863205395157\n",
      "Gradient Descent(2528/4999): loss=0.2906899519167728, w0=-0.31466399999506856, gamma=0.09519900892568021\n",
      "Gradient Descent(2529/4999): loss=0.2906899488298377, w0=-0.3146639999950685, gamma=0.7179823715371342\n",
      "Gradient Descent(2530/4999): loss=0.29068994045229923, w0=-0.3146639999950683, gamma=1.3068705158690097\n",
      "Gradient Descent(2531/4999): loss=0.2906898772701597, w0=-0.3146639999950681, gamma=0.06334545517087366\n",
      "Gradient Descent(2532/4999): loss=0.2906897622759568, w0=-0.3146639999950681, gamma=0.05467614790202037\n",
      "Gradient Descent(2533/4999): loss=0.2906897566952715, w0=-0.3146639999950681, gamma=0.05442348902291889\n",
      "Gradient Descent(2534/4999): loss=0.2906897518839018, w0=-0.3146639999950681, gamma=0.21063112077426369\n",
      "Gradient Descent(2535/4999): loss=0.290689747094949, w0=-0.31466399999506817, gamma=0.7105550956552344\n",
      "Gradient Descent(2536/4999): loss=0.29068972856068176, w0=-0.3146639999950683, gamma=0.06195116006036893\n",
      "Gradient Descent(2537/4999): loss=0.29068966603716734, w0=-0.3146639999950683, gamma=0.03510654661395656\n",
      "Gradient Descent(2538/4999): loss=0.2906896605858224, w0=-0.3146639999950683, gamma=0.0351545783864194\n",
      "Gradient Descent(2539/4999): loss=0.2906896574965096, w0=-0.3146639999950683, gamma=25.837568946244094\n",
      "Gradient Descent(2540/4999): loss=0.2906896544032116, w0=-0.31466399999507483, gamma=2747.1515505935695\n",
      "Gradient Descent(2541/4999): loss=0.29068738137591604, w0=-0.3146639999819104, gamma=0.08224085996631009\n",
      "Gradient Descent(2542/4999): loss=0.29046705274310064, w0=-0.3146639999830072, gamma=0.05806024726308566\n",
      "Gradient Descent(2543/4999): loss=0.2904538657938469, w0=-0.3146639999837177, gamma=0.054002806924909004\n",
      "Gradient Descent(2544/4999): loss=0.2904510709233081, w0=-0.3146639999843402, gamma=0.03852301970281896\n",
      "Gradient Descent(2545/4999): loss=0.29045099418393056, w0=-0.3146639999847603, gamma=0.03847516344502995\n",
      "Gradient Descent(2546/4999): loss=0.29045096706113704, w0=-0.31466399998516376, gamma=0.11963529624147078\n",
      "Gradient Descent(2547/4999): loss=0.29045095483985434, w0=-0.31466399998636996, gamma=0.1949476965603828\n",
      "Gradient Descent(2548/4999): loss=0.2904509300155425, w0=-0.3146639999881005, gamma=0.19042909390772184\n",
      "Gradient Descent(2549/4999): loss=0.29045090903001025, w0=-0.3146639999894615, gamma=0.03696380500992601\n",
      "Gradient Descent(2550/4999): loss=0.290450895511388, w0=-0.31466399998967537, gamma=0.03505588745241338\n",
      "Gradient Descent(2551/4999): loss=0.29045089018413095, w0=-0.3146639999898707, gamma=0.03836697117559834\n",
      "Gradient Descent(2552/4999): loss=0.29045088733293395, w0=-0.314663999990077, gamma=0.13799534549950518\n",
      "Gradient Descent(2553/4999): loss=0.29045088422343834, w0=-0.31466399999079053, gamma=0.16733790438135832\n",
      "Gradient Descent(2554/4999): loss=0.2904508730490583, w0=-0.3146639999915364, gamma=0.16013775451228265\n",
      "Gradient Descent(2555/4999): loss=0.29045085950696664, w0=-0.3146639999921307, gamma=0.09148850811036272\n",
      "Gradient Descent(2556/4999): loss=0.29045084654930303, w0=-0.31466399999241584, gamma=0.07325566539804426\n",
      "Gradient Descent(2557/4999): loss=0.29045083914648323, w0=-0.3146639999926233, gamma=0.07583446603103183\n",
      "Gradient Descent(2558/4999): loss=0.290450833219078, w0=-0.3146639999928223, gamma=0.13739674506009572\n",
      "Gradient Descent(2559/4999): loss=0.2904508270830394, w0=-0.31466399999315553, gamma=0.1493237857038061\n",
      "Gradient Descent(2560/4999): loss=0.2904508159658109, w0=-0.31466399999346795, gamma=0.049259290288521496\n",
      "Gradient Descent(2561/4999): loss=0.29045080388358957, w0=-0.3146639999935556, gamma=0.03648001204740123\n",
      "Gradient Descent(2562/4999): loss=0.2904507998978697, w0=-0.3146639999936173, gamma=0.042848716834730634\n",
      "Gradient Descent(2563/4999): loss=0.2904507969461725, w0=-0.3146639999936872, gamma=4.212231970149736\n",
      "Gradient Descent(2564/4999): loss=0.29045079347917296, w0=-0.31466400000026074, gamma=46.48900538371468\n",
      "Gradient Descent(2565/4999): loss=0.2904504526665173, w0=-0.31466399976722703, gamma=0.044685437996607395\n",
      "Gradient Descent(2566/4999): loss=0.29044669976919163, w0=-0.3146639997774164, gamma=0.035169357295504806\n",
      "Gradient Descent(2567/4999): loss=0.29044669003808743, w0=-0.3146639997850775, gamma=0.035554927962817465\n",
      "Gradient Descent(2568/4999): loss=0.29044668660342005, w0=-0.31466399979255016, gamma=0.238509249643135\n",
      "Gradient Descent(2569/4999): loss=0.2904466836447011, w0=-0.31466399984089605, gamma=0.26630494957051226\n",
      "Gradient Descent(2570/4999): loss=0.2904466641112575, w0=-0.31466399988200144, gamma=0.06864444686024361\n",
      "Gradient Descent(2571/4999): loss=0.29044664259674174, w0=-0.3146639998897754, gamma=0.05572795454695303\n",
      "Gradient Descent(2572/4999): loss=0.2904466370402311, w0=-0.3146639998956533, gamma=0.0550161395217054\n",
      "Gradient Descent(2573/4999): loss=0.2904466325366825, w0=-0.31466399990113275, gamma=0.07930709270492918\n",
      "Gradient Descent(2574/4999): loss=0.2904466280912321, w0=-0.314663999908597, gamma=0.15430133439904195\n",
      "Gradient Descent(2575/4999): loss=0.290446621683029, w0=-0.3146639999219678, gamma=0.10621289254825195\n",
      "Gradient Descent(2576/4999): loss=0.2904466092151629, w0=-0.3146639999297514, gamma=0.04043485778690631\n",
      "Gradient Descent(2577/4999): loss=0.29044660063299343, w0=-0.31466399993239985, gamma=0.03628191501159458\n",
      "Gradient Descent(2578/4999): loss=0.29044659736577716, w0=-0.3146639999346802, gamma=0.0867614301756012\n",
      "Gradient Descent(2579/4999): loss=0.29044659443414633, w0=-0.3146639999399354, gamma=8.56633313490522\n",
      "Gradient Descent(2580/4999): loss=0.29044658742370894, w0=-0.31466400041378545, gamma=9.249544995790885\n",
      "Gradient Descent(2581/4999): loss=0.2904458952940496, w0=-0.3146639965425361, gamma=0.03808012832808576\n",
      "Gradient Descent(2582/4999): loss=0.29044515378406305, w0=-0.31466399667401596, gamma=0.035058691431962614\n",
      "Gradient Descent(2583/4999): loss=0.29044514543732913, w0=-0.31466399679045415, gamma=0.03771939837213574\n",
      "Gradient Descent(2584/4999): loss=0.29044514250259207, w0=-0.3146639969113372, gamma=0.33691947846717807\n",
      "Gradient Descent(2585/4999): loss=0.29044513940114747, w0=-0.3146639979503681, gamma=0.3404873638297764\n",
      "Gradient Descent(2586/4999): loss=0.2904451119769747, w0=-0.3146639986466252, gamma=0.17895535086217282\n",
      "Gradient Descent(2587/4999): loss=0.29044508447319933, w0=-0.31466399888796914, gamma=0.05592846112757976\n",
      "Gradient Descent(2588/4999): loss=0.29044507002280534, w0=-0.31466399894989777, gamma=0.054847461622193144\n",
      "Gradient Descent(2589/4999): loss=0.29044506550257815, w0=-0.3146639990072328, gamma=0.04883486235327413\n",
      "Gradient Descent(2590/4999): loss=0.2904450610729939, w0=-0.3146639990554826, gamma=0.06761329176853245\n",
      "Gradient Descent(2591/4999): loss=0.2904450571290178, w0=-0.3146639991190235, gamma=0.1980057083599779\n",
      "Gradient Descent(2592/4999): loss=0.29044505166848605, w0=-0.3146639992925217, gamma=0.20452427235232015\n",
      "Gradient Descent(2593/4999): loss=0.2904450356773869, w0=-0.31466399943624707, gamma=0.04181420144031241\n",
      "Gradient Descent(2594/4999): loss=0.29044501916006543, w0=-0.3146639994596214, gamma=0.03528328068019552\n",
      "Gradient Descent(2595/4999): loss=0.2904450157830642, w0=-0.3146639994785202, gamma=0.041637517261792455\n",
      "Gradient Descent(2596/4999): loss=0.29044501293359454, w0=-0.3146639995000356, gamma=6.838945997748361\n",
      "Gradient Descent(2597/4999): loss=0.290445009570966, w0=-0.31466400288679347, gamma=8.442127124173888\n",
      "Gradient Descent(2598/4999): loss=0.29044445728760315, w0=-0.31466397847603567, gamma=0.3644141940342156\n",
      "Gradient Descent(2599/4999): loss=0.2904437760082883, w0=-0.31466398631794384, gamma=0.03860746621563282\n",
      "Gradient Descent(2600/4999): loss=0.2904437476056857, w0=-0.31466398684599006, gamma=0.035058226883703004\n",
      "Gradient Descent(2601/4999): loss=0.2904437431728472, w0=-0.3146639873069799, gamma=0.036054719931916265\n",
      "Gradient Descent(2602/4999): loss=0.29044374032362574, w0=-0.31466398776445204, gamma=0.13690939321600418\n",
      "Gradient Descent(2603/4999): loss=0.2904437374092999, w0=-0.31466398943896356, gamma=0.17187195105857397\n",
      "Gradient Descent(2604/4999): loss=0.2904437263482461, w0=-0.3146639912532936, gamma=0.2846808526941931\n",
      "Gradient Descent(2605/4999): loss=0.29044371246830075, w0=-0.314663993741962, gamma=0.17770274506853753\n",
      "Gradient Descent(2606/4999): loss=0.2904436894841385, w0=-0.31466399485318897, gamma=0.06307153634379464\n",
      "Gradient Descent(2607/4999): loss=0.2904436751393542, w0=-0.3146639951775068, gamma=0.05724064971107913\n",
      "Gradient Descent(2608/4999): loss=0.29044367004683974, w0=-0.3146639954532777, gamma=0.07170957810731121\n",
      "Gradient Descent(2609/4999): loss=0.2904436654260532, w0=-0.31466399577898085, gamma=0.22641820241665372\n",
      "Gradient Descent(2610/4999): loss=0.29044365963730623, w0=-0.31466399673362155, gamma=0.2267805604346431\n",
      "Gradient Descent(2611/4999): loss=0.2904436413598645, w0=-0.31466399747329615, gamma=0.04041648571493307\n",
      "Gradient Descent(2612/4999): loss=0.29044362305353755, w0=-0.3146639975752248, gamma=0.035192302741503596\n",
      "Gradient Descent(2613/4999): loss=0.29044361979082334, w0=-0.31466399766039116, gamma=0.04106043722092359\n",
      "Gradient Descent(2614/4999): loss=0.29044361694999676, w0=-0.3146639977562616, gamma=3.42031378845321\n",
      "Gradient Descent(2615/4999): loss=0.2904436136354857, w0=-0.3146640054143148, gamma=5.487567997783321\n",
      "Gradient Descent(2616/4999): loss=0.2904433375454133, w0=-0.31466397567684545, gamma=0.15664132032572406\n",
      "Gradient Descent(2617/4999): loss=0.29044289472527357, w0=-0.31466397948611297, gamma=0.07431311741490387\n",
      "Gradient Descent(2618/4999): loss=0.29044288205373714, w0=-0.31466398101021115, gamma=0.046815615558469144\n",
      "Gradient Descent(2619/4999): loss=0.2904428760178459, w0=-0.31466398189900757, gamma=0.037488447794169696\n",
      "Gradient Descent(2620/4999): loss=0.2904428722248765, w0=-0.31466398257740774, gamma=0.037597832315411944\n",
      "Gradient Descent(2621/4999): loss=0.29044286919675705, w0=-0.314663983232281, gamma=0.15089613408853036\n",
      "Gradient Descent(2622/4999): loss=0.29044286616119136, w0=-0.31466398576174875, gamma=0.3145494649062627\n",
      "Gradient Descent(2623/4999): loss=0.2904428539796562, w0=-0.31466399023889, gamma=0.29807707544928524\n",
      "Gradient Descent(2624/4999): loss=0.29044282859157666, w0=-0.3146639931470383, gamma=0.04925589800286221\n",
      "Gradient Descent(2625/4999): loss=0.29044280453767984, w0=-0.3146639934843533, gamma=0.0387434153616002\n",
      "Gradient Descent(2626/4999): loss=0.2904428005607971, w0=-0.3146639937366078, gamma=0.03614405657383823\n",
      "Gradient Descent(2627/4999): loss=0.29044279743393536, w0=-0.31466399396282063, gamma=0.06602908092238613\n",
      "Gradient Descent(2628/4999): loss=0.2904427945170463, w0=-0.3146639943611366, gamma=0.1913492708141503\n",
      "Gradient Descent(2629/4999): loss=0.29044278918841004, w0=-0.3146639954392208, gamma=0.6033673832366215\n",
      "Gradient Descent(2630/4999): loss=0.29044277374634164, w0=-0.31466399818818236, gamma=0.4267539903880022\n",
      "Gradient Descent(2631/4999): loss=0.2904427250545115, w0=-0.31466399895935726, gamma=0.04364151245197343\n",
      "Gradient Descent(2632/4999): loss=0.2904426906171641, w0=-0.3146639990045654, gamma=0.03591767733174488\n",
      "Gradient Descent(2633/4999): loss=0.29044268709421395, w0=-0.3146639990401486, gamma=0.038090207553643986\n",
      "Gradient Descent(2634/4999): loss=0.29044268419562513, w0=-0.31466399907652876, gamma=0.11693893815431718\n",
      "Gradient Descent(2635/4999): loss=0.2904426811218029, w0=-0.3146639991839635, gamma=0.15844604260680584\n",
      "Gradient Descent(2636/4999): loss=0.29044267168505966, w0=-0.3146639993125092, gamma=1.2555353479906972\n",
      "Gradient Descent(2637/4999): loss=0.2904426588988306, w0=-0.314664000169719, gamma=0.42348003239369975\n",
      "Gradient Descent(2638/4999): loss=0.29044255758129, w0=-0.31466400009583645, gamma=0.044114132186823654\n",
      "Gradient Descent(2639/4999): loss=0.2904425234150692, w0=-0.31466400009139933, gamma=0.03940812426704638\n",
      "Gradient Descent(2640/4999): loss=0.29044251984925684, w0=-0.3146640000876104, gamma=0.03771122435082774\n",
      "Gradient Descent(2641/4999): loss=0.2904425166689555, w0=-0.31466400008412754, gamma=0.050121577518424706\n",
      "Gradient Descent(2642/4999): loss=0.2904425136258835, w0=-0.31466400007967305, gamma=0.14010207149690193\n",
      "Gradient Descent(2643/4999): loss=0.2904425095813912, w0=-0.31466400006784573, gamma=0.6729052950773518\n",
      "Gradient Descent(2644/4999): loss=0.29044249827607493, w0=-0.3146640000189982, gamma=0.6467515937035966\n",
      "Gradient Descent(2645/4999): loss=0.29044244397745095, w0=-0.3146640000036415, gamma=0.03858304682031911\n",
      "Gradient Descent(2646/4999): loss=0.2904423917935036, w0=-0.31466400000331785, gamma=0.035334382180237676\n",
      "Gradient Descent(2647/4999): loss=0.29044238867693295, w0=-0.3146640000030329, gamma=0.04003819230831186\n",
      "Gradient Descent(2648/4999): loss=0.2904423858257273, w0=-0.31466400000272143, gamma=0.10855642838769375\n",
      "Gradient Descent(2649/4999): loss=0.29044238259502525, w0=-0.31466400000191075, gamma=0.1816191493560179\n",
      "Gradient Descent(2650/4999): loss=0.29044237383559107, w0=-0.3146640000007017, gamma=0.9855239305889585\n",
      "Gradient Descent(2651/4999): loss=0.2904423591807751, w0=-0.31466399999533257, gamma=0.22418128672670318\n",
      "Gradient Descent(2652/4999): loss=0.29044227965997527, w0=-0.3146639999953149, gamma=0.053417915733305524\n",
      "Gradient Descent(2653/4999): loss=0.2904422615727267, w0=-0.31466399999531164, gamma=0.05074272912215602\n",
      "Gradient Descent(2654/4999): loss=0.2904422572611401, w0=-0.3146639999953087, gamma=0.03629656039190339\n",
      "Gradient Descent(2655/4999): loss=0.2904422531668518, w0=-0.3146639999953067, gamma=0.04016324045862294\n",
      "Gradient Descent(2656/4999): loss=0.29044225023819703, w0=-0.3146639999953046, gamma=3.0239256782869126\n",
      "Gradient Descent(2657/4999): loss=0.29044224699756027, w0=-0.3146639999951512, gamma=34.90158400419004\n",
      "Gradient Descent(2658/4999): loss=0.2904420030122419, w0=-0.31466399999873895, gamma=0.07130239990170395\n",
      "Gradient Descent(2659/4999): loss=0.2904391900559898, w0=-0.3146639999984906, gamma=0.0392284711042971\n",
      "Gradient Descent(2660/4999): loss=0.29043918312975625, w0=-0.3146639999983637, gamma=0.03535100474600045\n",
      "Gradient Descent(2661/4999): loss=0.290439178938395, w0=-0.31466399999825384, gamma=0.04177502230869798\n",
      "Gradient Descent(2662/4999): loss=0.2904391760527292, w0=-0.3146639999981286, gamma=0.10022483424497966\n",
      "Gradient Descent(2663/4999): loss=0.2904391726748636, w0=-0.3146639999978407, gamma=0.10214176112936849\n",
      "Gradient Descent(2664/4999): loss=0.2904391645905424, w0=-0.3146639999975767, gamma=0.07434909155915657\n",
      "Gradient Descent(2665/4999): loss=0.2904391563573708, w0=-0.3146639999974042, gamma=0.06573730598818575\n",
      "Gradient Descent(2666/4999): loss=0.29043915036445356, w0=-0.31466399999726297, gamma=0.0672976264158326\n",
      "Gradient Descent(2667/4999): loss=0.29043914506570295, w0=-0.3146639999971279, gamma=0.07815600722030244\n",
      "Gradient Descent(2668/4999): loss=0.2904391396411926, w0=-0.31466399999698164, gamma=0.0859228441585435\n",
      "Gradient Descent(2669/4999): loss=0.2904391333414567, w0=-0.31466399999683337, gamma=0.0686169552348129\n",
      "Gradient Descent(2670/4999): loss=0.290439126415693, w0=-0.3146639999967251, gamma=0.05094642854515917\n",
      "Gradient Descent(2671/4999): loss=0.290439120884872, w0=-0.3146639999966503, gamma=0.052347159230798836\n",
      "Gradient Descent(2672/4999): loss=0.2904391167783774, w0=-0.3146639999965773, gamma=0.11893318569662648\n",
      "Gradient Descent(2673/4999): loss=0.2904391125589846, w0=-0.31466399999642014, gamma=0.3784598979705374\n",
      "Gradient Descent(2674/4999): loss=0.29043910297250686, w0=-0.3146639999959796, gamma=0.09490450922375904\n",
      "Gradient Descent(2675/4999): loss=0.2904390724673486, w0=-0.31466399999591094, gamma=0.03565492346125122\n",
      "Gradient Descent(2676/4999): loss=0.29043906481784476, w0=-0.31466399999588757, gamma=0.035245219623179544\n",
      "Gradient Descent(2677/4999): loss=0.2904390619438553, w0=-0.3146639999958653, gamma=0.7094242400525065\n",
      "Gradient Descent(2678/4999): loss=0.2904390591029986, w0=-0.3146639999954332, gamma=11219.964807204804\n",
      "Gradient Descent(2679/4999): loss=0.29043900192186184, w0=-0.3146639980095587, gamma=49.048236347495184\n",
      "Gradient Descent(2680/4999): loss=0.28960148526960344, w0=-0.31466409543639867, gamma=0.03503194667624542\n",
      "Gradient Descent(2681/4999): loss=0.38286862871464983, w0=-0.31466409209295015, gamma=0.0350319436186824\n",
      "Gradient Descent(2682/4999): loss=0.28959843234527527, w0=-0.3146640888666118, gamma=0.05772837193114723\n",
      "Gradient Descent(2683/4999): loss=0.28959842731572477, w0=-0.31466408373625077, gamma=0.057964849693823775\n",
      "Gradient Descent(2684/4999): loss=0.28959842314211554, w0=-0.3146640788822544, gamma=0.9262855135687391\n",
      "Gradient Descent(2685/4999): loss=0.28959841947439563, w0=-0.31466400581097487, gamma=1.0837452963928909\n",
      "Gradient Descent(2686/4999): loss=0.28959836150247903, w0=-0.31466399950892415, gamma=0.5430668332160172\n",
      "Gradient Descent(2687/4999): loss=0.2895982947440219, w0=-0.31466399977338927, gamma=0.058497636253553274\n",
      "Gradient Descent(2688/4999): loss=0.28959826208501915, w0=-0.31466399978640613, gamma=0.05467218003465254\n",
      "Gradient Descent(2689/4999): loss=0.2895982578002418, w0=-0.3146639997978601, gamma=0.061367141496407304\n",
      "Gradient Descent(2690/4999): loss=0.28959825443587567, w0=-0.31466399981001375, gamma=0.09220241589592695\n",
      "Gradient Descent(2691/4999): loss=0.2895982506713563, w0=-0.3146639998271537, gamma=0.17983058287685907\n",
      "Gradient Descent(2692/4999): loss=0.2895982450166013, w0=-0.31466399985750104, gamma=1.1935805721050488\n",
      "Gradient Descent(2693/4999): loss=0.28959823398856177, w0=-0.31466400002270173, gamma=1.066452046396203\n",
      "Gradient Descent(2694/4999): loss=0.2895981608087678, w0=-0.31466399999412836, gamma=0.07172195063612366\n",
      "Gradient Descent(2695/4999): loss=0.28959809549752297, w0=-0.3146639999942561, gamma=0.0635527394001643\n",
      "Gradient Descent(2696/4999): loss=0.2895980910479275, w0=-0.3146639999943611, gamma=0.05610092547659504\n",
      "Gradient Descent(2697/4999): loss=0.2895980871499387, w0=-0.31466399999444794, gamma=0.06262844774622861\n",
      "Gradient Descent(2698/4999): loss=0.28959808371128654, w0=-0.3146639999945394, gamma=0.13253191055862173\n",
      "Gradient Descent(2699/4999): loss=0.28959807987269626, w0=-0.3146639999947209, gamma=0.2208113233777657\n",
      "Gradient Descent(2700/4999): loss=0.2895980717496815, w0=-0.31466399999498323, gamma=0.22734876536215143\n",
      "Gradient Descent(2701/4999): loss=0.28959805821600637, w0=-0.3146639999951937, gamma=0.04229846891042711\n",
      "Gradient Descent(2702/4999): loss=0.2895980442817951, w0=-0.31466399999522393, gamma=0.035334185800671956\n",
      "Gradient Descent(2703/4999): loss=0.28959804168925063, w0=-0.31466399999524813, gamma=0.04009891310672879\n",
      "Gradient Descent(2704/4999): loss=0.2895980395236151, w0=-0.3146639999952746, gamma=2.626934746679579\n",
      "Gradient Descent(2705/4999): loss=0.2895980370659546, w0=-0.31466399999694084, gamma=5.842948980325008\n",
      "Gradient Descent(2706/4999): loss=0.2895978760637295, w0=-0.3146639999909116, gamma=0.0928719852062654\n",
      "Gradient Descent(2707/4999): loss=0.28959751802430334, w0=-0.31466399999137573, gamma=0.06532556780347883\n",
      "Gradient Descent(2708/4999): loss=0.2895975122934872, w0=-0.3146639999916719, gamma=0.043159225067477984\n",
      "Gradient Descent(2709/4999): loss=0.28959750828459857, w0=-0.31466399999185474, gamma=0.03591057675222457\n",
      "Gradient Descent(2710/4999): loss=0.2895975056381154, w0=-0.31466399999200034, gamma=0.03667362698723702\n",
      "Gradient Descent(2711/4999): loss=0.2895975034373717, w0=-0.3146639999921437, gamma=0.13798675521512865\n",
      "Gradient Descent(2712/4999): loss=0.28959750118996297, w0=-0.31466399999266326, gamma=0.2873141173539681\n",
      "Gradient Descent(2713/4999): loss=0.28959749273399915, w0=-0.3146639999935959, gamma=0.2783262060800383\n",
      "Gradient Descent(2714/4999): loss=0.28959747512726974, w0=-0.3146639999942398, gamma=0.06615394296552102\n",
      "Gradient Descent(2715/4999): loss=0.28959745807154347, w0=-0.31466399999435024, gamma=0.053208585772169976\n",
      "Gradient Descent(2716/4999): loss=0.28959745401757353, w0=-0.3146639999944332, gamma=0.04656904356592433\n",
      "Gradient Descent(2717/4999): loss=0.289597450756969, w0=-0.3146639999945019, gamma=0.059657659270791225\n",
      "Gradient Descent(2718/4999): loss=0.28959744790324055, w0=-0.3146639999945859, gamma=0.2540021472893508\n",
      "Gradient Descent(2719/4999): loss=0.28959744424745204, w0=-0.31466399999492206, gamma=0.4750923613281195\n",
      "Gradient Descent(2720/4999): loss=0.2895974286823724, w0=-0.3146639999953911, gamma=0.0463653244168181\n",
      "Gradient Descent(2721/4999): loss=0.28959739956935693, w0=-0.3146639999954151, gamma=0.03510425777641263\n",
      "Gradient Descent(2722/4999): loss=0.2895973967280087, w0=-0.3146639999954325, gamma=0.03571047614229803\n",
      "Gradient Descent(2723/4999): loss=0.2895973945768539, w0=-0.31466399999544953, gamma=19.73265740592422\n",
      "Gradient Descent(2724/4999): loss=0.2895973923885694, w0=-0.31466400000452227, gamma=46.49499297640649\n",
      "Gradient Descent(2725/4999): loss=0.28959618330622977, w0=-0.31466399960410335, gamma=0.096876478646487\n",
      "Gradient Descent(2726/4999): loss=0.2895933694040991, w0=-0.3146639996420604, gamma=0.08982586733912093\n",
      "Gradient Descent(2727/4999): loss=0.2895933310709492, w0=-0.3146639996738454, gamma=0.05908786471896367\n",
      "Gradient Descent(2728/4999): loss=0.2895933249428543, w0=-0.3146639996928756, gamma=0.03547032293989703\n",
      "Gradient Descent(2729/4999): loss=0.2895933212201574, w0=-0.3146639997036244, gamma=0.03522475967007182\n",
      "Gradient Descent(2730/4999): loss=0.28959331897479834, w0=-0.3146639997139202, gamma=0.22314655750642837\n",
      "Gradient Descent(2731/4999): loss=0.28959331680060574, w0=-0.3146639997768457, gamma=0.47243352375653347\n",
      "Gradient Descent(2732/4999): loss=0.28959330303965175, w0=-0.31466399988034005, gamma=0.26486549236235035\n",
      "Gradient Descent(2733/4999): loss=0.28959327397238255, w0=-0.3146639999109511, gamma=0.08776210868514475\n",
      "Gradient Descent(2734/4999): loss=0.28959325772961036, w0=-0.3146639999184075, gamma=0.07915301441079321\n",
      "Gradient Descent(2735/4999): loss=0.28959325231706035, w0=-0.31466399992454225, gamma=0.0550417447778694\n",
      "Gradient Descent(2736/4999): loss=0.28959324746063914, w0=-0.31466399992847055, gamma=0.04393342161732484\n",
      "Gradient Descent(2737/4999): loss=0.2895932440842927, w0=-0.3146639999314335, gamma=0.05843321757458745\n",
      "Gradient Descent(2738/4999): loss=0.28959324138974135, w0=-0.3146639999352012, gamma=0.32749530132944454\n",
      "Gradient Descent(2739/4999): loss=0.2895932378063102, w0=-0.3146639999550838, gamma=0.5629620734402279\n",
      "Gradient Descent(2740/4999): loss=0.2895932177276574, w0=-0.3146639999780687, gamma=0.0423200586779958\n",
      "Gradient Descent(2741/4999): loss=0.2895931832601579, w0=-0.3146639999788238, gamma=0.03504978201748367\n",
      "Gradient Descent(2742/4999): loss=0.2895931806425624, w0=-0.3146639999794228, gamma=0.035407559609497055\n",
      "Gradient Descent(2743/4999): loss=0.289593178494846, w0=-0.31466399998000666, gamma=1.368162959473003\n",
      "Gradient Descent(2744/4999): loss=0.289593176326432, w0=-0.3146640000017681, gamma=1.604837697769432\n",
      "Gradient Descent(2745/4999): loss=0.289593092557003, w0=-0.31466399999237066, gamma=0.12945876985113894\n",
      "Gradient Descent(2746/4999): loss=0.28959299436509905, w0=-0.3146639999928292, gamma=0.11991513812515424\n",
      "Gradient Descent(2747/4999): loss=0.2895929864088003, w0=-0.3146639999931989, gamma=0.09279240765044193\n",
      "Gradient Descent(2748/4999): loss=0.2895929790693472, w0=-0.3146639999934507, gamma=0.04855797894580074\n",
      "Gradient Descent(2749/4999): loss=0.2895929733902192, w0=-0.3146639999935702, gamma=0.039203875567243966\n",
      "Gradient Descent(2750/4999): loss=0.2895929704183401, w0=-0.314663999993662, gamma=0.0421181484782201\n",
      "Gradient Descent(2751/4999): loss=0.2895929680189728, w0=-0.3146639999937568, gamma=0.47623621688798234\n",
      "Gradient Descent(2752/4999): loss=0.2895929654412493, w0=-0.3146639999947834, gamma=7.335671699726567\n",
      "Gradient Descent(2753/4999): loss=0.2895929362946224, w0=-0.3146640000030658, gamma=0.10611380894582045\n",
      "Gradient Descent(2754/4999): loss=0.2895924873616656, w0=-0.31466400000230677, gamma=0.03537975967936742\n",
      "Gradient Descent(2755/4999): loss=0.2895924808895255, w0=-0.31466400000208056, gamma=0.03508681329689483\n",
      "Gradient Descent(2756/4999): loss=0.28959247869553845, w0=-0.3146640000018642, gamma=0.05118606009616794\n",
      "Gradient Descent(2757/4999): loss=0.289592476548402, w0=-0.31466400000155953, gamma=0.06771019254198246\n",
      "Gradient Descent(2758/4999): loss=0.28959247341612127, w0=-0.31466400000117717, gamma=0.7732385175713257\n",
      "Gradient Descent(2759/4999): loss=0.28959246927267407, w0=-0.31466399999710654, gamma=3.1309253473952743\n",
      "Gradient Descent(2760/4999): loss=0.2895924219554802, w0=-0.31466399999336947, gamma=0.23822514564449854\n",
      "Gradient Descent(2761/4999): loss=0.28959223037018034, w0=-0.31466399999397543, gamma=0.07084792357821396\n",
      "Gradient Descent(2762/4999): loss=0.2895922157966974, w0=-0.3146639999941127, gamma=0.05329184693565834\n",
      "Gradient Descent(2763/4999): loss=0.2895922114567545, w0=-0.31466399999420863, gamma=0.0439928065746487\n",
      "Gradient Descent(2764/4999): loss=0.28959220819522913, w0=-0.3146639999942836, gamma=0.03766733158811852\n",
      "Gradient Descent(2765/4999): loss=0.2895922055032745, w0=-0.314663999994345, gamma=0.053875919723108054\n",
      "Gradient Descent(2766/4999): loss=0.28959220319842827, w0=-0.3146639999944295, gamma=0.1836820545343955\n",
      "Gradient Descent(2767/4999): loss=0.2895921999018028, w0=-0.314663999994702, gamma=0.2558394187496778\n",
      "Gradient Descent(2768/4999): loss=0.289592188662488, w0=-0.3146639999950118, gamma=0.17066028759145585\n",
      "Gradient Descent(2769/4999): loss=0.2895921730080064, w0=-0.31466399999516564, gamma=0.038720871329281176\n",
      "Gradient Descent(2770/4999): loss=0.2895921625656109, w0=-0.31466399999519457, gamma=0.03527069545756984\n",
      "Gradient Descent(2771/4999): loss=0.28959216019629763, w0=-0.31466399999521993, gamma=0.05611036350979787\n",
      "Gradient Descent(2772/4999): loss=0.28959215803814387, w0=-0.31466399999525885, gamma=16.821555345082583\n",
      "Gradient Descent(2773/4999): loss=0.2895921546048497, w0=-0.3146640000062623, gamma=85.03655799617638\n",
      "Gradient Descent(2774/4999): loss=0.2895911253991438, w0=-0.3146639991262638, gamma=0.0712200482311313\n",
      "Gradient Descent(2775/4999): loss=0.2895859815944252, w0=-0.3146639991882006, gamma=0.04398511193905129\n",
      "Gradient Descent(2776/4999): loss=0.2895859380454728, w0=-0.3146639992237281, gamma=0.03559931385851505\n",
      "Gradient Descent(2777/4999): loss=0.2895859210815961, w0=-0.3146639992512175, gamma=0.03793514686328537\n",
      "Gradient Descent(2778/4999): loss=0.28958591743884404, w0=-0.3146639992794678, gamma=0.10980518300419034\n",
      "Gradient Descent(2779/4999): loss=0.2895859148119872, w0=-0.3146639993581377, gamma=0.11411408146739635\n",
      "Gradient Descent(2780/4999): loss=0.2895859078115389, w0=-0.3146639994309174, gamma=0.1474229901883946\n",
      "Gradient Descent(2781/4999): loss=0.28958590078101465, w0=-0.31466399951421137, gamma=0.1428532373082152\n",
      "Gradient Descent(2782/4999): loss=0.2895858917153119, w0=-0.31466399958302466, gamma=0.04624680416881632\n",
      "Gradient Descent(2783/4999): loss=0.28958588294223, w0=-0.3146639996021196, gamma=0.036052435579188274\n",
      "Gradient Descent(2784/4999): loss=0.2895858800998236, w0=-0.31466399961631697, gamma=0.04411834727835714\n",
      "Gradient Descent(2785/4999): loss=0.2895858778865922, w0=-0.3146639996330643, gamma=1.5142188516466546\n",
      "Gradient Descent(2786/4999): loss=0.28958587517915096, w0=-0.3146640001825033, gamma=1.8970682771132723\n",
      "Gradient Descent(2787/4999): loss=0.28958578243256194, w0=-0.3146639998285372, gamma=0.050039014046492386\n",
      "Gradient Descent(2788/4999): loss=0.2895856667296055, w0=-0.3146639998369128, gamma=0.037742531915275886\n",
      "Gradient Descent(2789/4999): loss=0.28958566352149673, w0=-0.31466399984291404, gamma=0.03711080870303399\n",
      "Gradient Descent(2790/4999): loss=0.28958566118960244, w0=-0.3146639998485921, gamma=0.06609481328096456\n",
      "Gradient Descent(2791/4999): loss=0.2895856589207095, w0=-0.31466399985832955, gamma=0.07981864543001252\n",
      "Gradient Descent(2792/4999): loss=0.28958565488299354, w0=-0.31466399986931165, gamma=0.08050159154352639\n",
      "Gradient Descent(2793/4999): loss=0.28958565000789055, w0=-0.3146639998795036, gamma=0.06143716493863495\n",
      "Gradient Descent(2794/4999): loss=0.28958564509111046, w0=-0.3146639998866558, gamma=0.04985978184537006\n",
      "Gradient Descent(2795/4999): loss=0.28958564133873, w0=-0.31466399989210353, gamma=0.0603861891905862\n",
      "Gradient Descent(2796/4999): loss=0.2895856382934614, w0=-0.31466399989837246, gamma=0.17165631157955463\n",
      "Gradient Descent(2797/4999): loss=0.2895856346052793, w0=-0.3146639999151167, gamma=0.28852139586854886\n",
      "Gradient Descent(2798/4999): loss=0.28958562412111616, w0=-0.31466399993842953, gamma=0.05164438313958371\n",
      "Gradient Descent(2799/4999): loss=0.28958560649934373, w0=-0.3146639999413985, gamma=0.03534151511793729\n",
      "Gradient Descent(2800/4999): loss=0.28958560334508215, w0=-0.3146639999433253, gamma=0.03640650085216633\n",
      "Gradient Descent(2801/4999): loss=0.28958560118655674, w0=-0.31466399994523997, gamma=14.524161533727773\n",
      "Gradient Descent(2802/4999): loss=0.28958559896299585, w0=-0.31466400068129247, gamma=91.11130676262329\n",
      "Gradient Descent(2803/4999): loss=0.289584711942344, w0=-0.3146639382359544, gamma=0.2208098770757071\n",
      "Gradient Descent(2804/4999): loss=0.28957915843315796, w0=-0.3146639518731652, gamma=0.035895243830824396\n",
      "Gradient Descent(2805/4999): loss=0.28957917205139355, w0=-0.3146639536005433, gamma=0.03504541338000602\n",
      "Gradient Descent(2806/4999): loss=0.28957913732210927, w0=-0.3146639552264885, gamma=0.050050893242608786\n",
      "Gradient Descent(2807/4999): loss=0.2895791349175501, w0=-0.31466395746723863, gamma=0.3314549974457988\n",
      "Gradient Descent(2808/4999): loss=0.2895791315681549, w0=-0.3146639715635832, gamma=0.43438502220282316\n",
      "Gradient Descent(2809/4999): loss=0.28957911020119537, w0=-0.31466398391416533, gamma=0.14732864227314177\n",
      "Gradient Descent(2810/4999): loss=0.28957908356486367, w0=-0.31466398628346876, gamma=0.10440862149561711\n",
      "Gradient Descent(2811/4999): loss=0.28957907444672293, w0=-0.3146639877151671, gamma=0.10137255298500678\n",
      "Gradient Descent(2812/4999): loss=0.28957906804219063, w0=-0.3146639889600986, gamma=0.04388648083847264\n",
      "Gradient Descent(2813/4999): loss=0.2895790618481507, w0=-0.3146639894444221, gamma=0.037149046706877686\n",
      "Gradient Descent(2814/4999): loss=0.2895790591663224, w0=-0.31466398983640026, gamma=0.06576386359215082\n",
      "Gradient Descent(2815/4999): loss=0.2895790568969492, w0=-0.3146639905045297, gamma=1.360718980282277\n",
      "Gradient Descent(2816/4999): loss=0.2895790528800121, w0=-0.31466400341964756, gamma=1.4368568615657964\n",
      "Gradient Descent(2817/4999): loss=0.2895789698297918, w0=-0.3146639985002443, gamma=0.03678767574491852\n",
      "Gradient Descent(2818/4999): loss=0.2895788825236439, w0=-0.31466399855526683, gamma=0.035035368978511824\n",
      "Gradient Descent(2819/4999): loss=0.2895788799863296, w0=-0.31466399860574074, gamma=0.036022916871712767\n",
      "Gradient Descent(2820/4999): loss=0.28957887784924635, w0=-0.3146639986558191, gamma=0.3193340101717787\n",
      "Gradient Descent(2821/4999): loss=0.2895788756527738, w0=-0.31466399908375964, gamma=0.3760263712082998\n",
      "Gradient Descent(2822/4999): loss=0.28957885618330675, w0=-0.3146639994267569, gamma=0.16966537240698434\n",
      "Gradient Descent(2823/4999): loss=0.28957883326004935, w0=-0.3146639995233246, gamma=0.127399412398331\n",
      "Gradient Descent(2824/4999): loss=0.289578822916877, w0=-0.31466399958353325, gamma=0.1349324536239755\n",
      "Gradient Descent(2825/4999): loss=0.28957881515048955, w0=-0.3146639996391779, gamma=0.26656853900836924\n",
      "Gradient Descent(2826/4999): loss=0.2895788069249395, w0=-0.3146639997342747, gamma=0.23548211033541622\n",
      "Gradient Descent(2827/4999): loss=0.28957879067485137, w0=-0.31466399979588805, gamma=0.050420507898306786\n",
      "Gradient Descent(2828/4999): loss=0.28957877631992557, w0=-0.31466399980597387, gamma=0.036745225575847446\n",
      "Gradient Descent(2829/4999): loss=0.28957877324625736, w0=-0.31466399981295357, gamma=0.0371794314717272\n",
      "Gradient Descent(2830/4999): loss=0.2895787710062711, w0=-0.31466399981975623, gamma=0.2797736304635079\n",
      "Gradient Descent(2831/4999): loss=0.2895787687398263, w0=-0.31466399986904287, gamma=1.3557305821975842\n",
      "Gradient Descent(2832/4999): loss=0.2895787516849597, w0=-0.31466400004105743, gamma=0.23757492745563658\n",
      "Gradient Descent(2833/4999): loss=0.2895786690412969, w0=-0.3146640000303345, gamma=0.06643794820938578\n",
      "Gradient Descent(2834/4999): loss=0.28957865456017795, w0=-0.31466400002804823, gamma=0.04943713302002312\n",
      "Gradient Descent(2835/4999): loss=0.28957865050931797, w0=-0.31466400002646, gamma=0.035428425706141055\n",
      "Gradient Descent(2836/4999): loss=0.28957864749563184, w0=-0.3146640000253781, gamma=0.03572835050588188\n",
      "Gradient Descent(2837/4999): loss=0.28957864533598165, w0=-0.3146640000243257, gamma=0.37166118928478137\n",
      "Gradient Descent(2838/4999): loss=0.2895786431580741, w0=-0.3146640000137694, gamma=1.2230877226402759\n",
      "Gradient Descent(2839/4999): loss=0.2895786205026209, w0=-0.31466399999194117, gamma=0.15284671071354755\n",
      "Gradient Descent(2840/4999): loss=0.28957854594760296, w0=-0.31466399999254974, gamma=0.07587464782110144\n",
      "Gradient Descent(2841/4999): loss=0.2895785366307383, w0=-0.31466399999280564, gamma=0.073535329104651\n",
      "Gradient Descent(2842/4999): loss=0.28957853200541184, w0=-0.31466399999303485, gamma=0.0428295267899995\n",
      "Gradient Descent(2843/4999): loss=0.2895785275230311, w0=-0.3146639999931585, gamma=0.03995033978192258\n",
      "Gradient Descent(2844/4999): loss=0.28957852491233815, w0=-0.31466399999326894, gamma=0.1321509618938305\n",
      "Gradient Descent(2845/4999): loss=0.28957852247715116, w0=-0.31466399999361966, gamma=4.843817912543686\n",
      "Gradient Descent(2846/4999): loss=0.28957851442185145, w0=-0.314664000004775, gamma=0.6755943986826345\n",
      "Gradient Descent(2847/4999): loss=0.2895782191724491, w0=-0.3146639999987946, gamma=0.035107209748869654\n",
      "Gradient Descent(2848/4999): loss=0.2895781781039253, w0=-0.31466399999869377, gamma=0.03504731146030893\n",
      "Gradient Descent(2849/4999): loss=0.2895781758549266, w0=-0.3146639999985967, gamma=0.05558105386288996\n",
      "Gradient Descent(2850/4999): loss=0.28957817371877514, w0=-0.3146639999984481, gamma=0.06433266695645087\n",
      "Gradient Descent(2851/4999): loss=0.2895781703311378, w0=-0.31466399999828565, gamma=1.0508700713908112\n",
      "Gradient Descent(2852/4999): loss=0.2895781664101054, w0=-0.3146639999958027, gamma=16.58083510544865\n",
      "Gradient Descent(2853/4999): loss=0.2895781023606284, w0=-0.3146639999977965, gamma=0.2088389661717523\n",
      "Gradient Descent(2854/4999): loss=0.2895770919057394, w0=-0.31466399999740535, gamma=0.07316464424377367\n",
      "Gradient Descent(2855/4999): loss=0.2895770792606571, w0=-0.31466399999729694, gamma=0.06158066560460624\n",
      "Gradient Descent(2856/4999): loss=0.2895770746862009, w0=-0.3146639999972124, gamma=0.054704848379513275\n",
      "Gradient Descent(2857/4999): loss=0.2895770709256494, w0=-0.3146639999971419, gamma=0.047538121891849354\n",
      "Gradient Descent(2858/4999): loss=0.2895770675920212, w0=-0.314663999997084, gamma=0.042202455792603746\n",
      "Gradient Descent(2859/4999): loss=0.28957706469542926, w0=-0.31466399999703504, gamma=0.05143162007651397\n",
      "Gradient Descent(2860/4999): loss=0.28957706212399575, w0=-0.31466399999697786, gamma=0.10990149881890576\n",
      "Gradient Descent(2861/4999): loss=0.28957705899023634, w0=-0.314663999996862, gamma=0.2069256743289743\n",
      "Gradient Descent(2862/4999): loss=0.28957705229388897, w0=-0.31466399999666783, gamma=0.12517540878536312\n",
      "Gradient Descent(2863/4999): loss=0.28957703968584936, w0=-0.3146639999965747, gamma=0.038787894132540184\n",
      "Gradient Descent(2864/4999): loss=0.2895770320589157, w0=-0.3146639999965494, gamma=0.03559783446398323\n",
      "Gradient Descent(2865/4999): loss=0.2895770296955501, w0=-0.31466399999652717, gamma=0.08423798840969213\n",
      "Gradient Descent(2866/4999): loss=0.2895770275265753, w0=-0.3146639999964763, gamma=150.72633613476611\n",
      "Gradient Descent(2867/4999): loss=0.2895770223939627, w0=-0.3146639999131721, gamma=91.33444839820716\n",
      "Gradient Descent(2868/4999): loss=0.2895678445439307, w0=-0.31466400747182854, gamma=0.03508772460107045\n",
      "Gradient Descent(2869/4999): loss=0.28957695006214484, w0=-0.3146640072095166, gamma=0.03503300442934776\n",
      "Gradient Descent(2870/4999): loss=0.2895623126411555, w0=-0.31466400695680313, gamma=0.08657996699283736\n",
      "Gradient Descent(2871/4999): loss=0.2895623079128886, w0=-0.3146640063541313, gamma=0.12603077450205766\n",
      "Gradient Descent(2872/4999): loss=0.28956229880945916, w0=-0.3146640055528027, gamma=0.4022586777799421\n",
      "Gradient Descent(2873/4999): loss=0.2895622878936062, w0=-0.3146640033175039, gamma=0.44630935950161055\n",
      "Gradient Descent(2874/4999): loss=0.2895622582731617, w0=-0.31466400183505605, gamma=0.06283469070783645\n",
      "Gradient Descent(2875/4999): loss=0.28956223174932977, w0=-0.3146640017194954, gamma=0.055738882744172956\n",
      "Gradient Descent(2876/4999): loss=0.2895622255856536, w0=-0.314664001623426, gamma=0.05843892489040394\n",
      "Gradient Descent(2877/4999): loss=0.28956222203026755, w0=-0.31466400152831714, gamma=0.2221594376187467\n",
      "Gradient Descent(2878/4999): loss=0.2895622183666549, w0=-0.31466400118788374, gamma=0.7154541006933145\n",
      "Gradient Descent(2879/4999): loss=0.2895622044918895, w0=-0.3146640003350984, gamma=0.32210947373433596\n",
      "Gradient Descent(2880/4999): loss=0.28956216024809145, w0=-0.31466400022585034, gamma=0.04442864680217364\n",
      "Gradient Descent(2881/4999): loss=0.28956214119381557, w0=-0.31466400021563545, gamma=0.03605293718183675\n",
      "Gradient Descent(2882/4999): loss=0.28956213785615004, w0=-0.31466400020771457, gamma=0.03824227334210188\n",
      "Gradient Descent(2883/4999): loss=0.2895621355897004, w0=-0.3146640001996156, gamma=0.09661596682957226\n",
      "Gradient Descent(2884/4999): loss=0.28956213324418917, w0=-0.31466400017993673, gamma=0.1101901534686082\n",
      "Gradient Descent(2885/4999): loss=0.289562127340645, w0=-0.31466400015966145, gamma=0.5408842464488742\n",
      "Gradient Descent(2886/4999): loss=0.2895621206184347, w0=-0.3146640000711039, gamma=0.5035708592295765\n",
      "Gradient Descent(2887/4999): loss=0.28956208766006475, w0=-0.3146640000332505, gamma=0.03924762323429218\n",
      "Gradient Descent(2888/4999): loss=0.28956205720430994, w0=-0.3146640000317859, gamma=0.03581911106997005\n",
      "Gradient Descent(2889/4999): loss=0.28956205465420254, w0=-0.3146640000305017, gamma=0.04011962491646851\n",
      "Gradient Descent(2890/4999): loss=0.28956205247353395, w0=-0.3146640000291149, gamma=0.07627831618535091\n",
      "Gradient Descent(2891/4999): loss=0.28956205003552676, w0=-0.3146640000265839, gamma=0.14263746441865857\n",
      "Gradient Descent(2892/4999): loss=0.2895620454012846, w0=-0.3146640000222121, gamma=0.8958076196896232\n",
      "Gradient Descent(2893/4999): loss=0.28956203673666253, w0=-0.31466399999867206, gamma=0.39480865069904914\n",
      "Gradient Descent(2894/4999): loss=0.28956198234105085, w0=-0.3146639999975911, gamma=0.04748447744918078\n",
      "Gradient Descent(2895/4999): loss=0.2895619584434751, w0=-0.31466399999751243, gamma=0.04265876922050639\n",
      "Gradient Descent(2896/4999): loss=0.2895619554994131, w0=-0.3146639999974451, gamma=0.03638042746640429\n",
      "Gradient Descent(2897/4999): loss=0.28956195290775977, w0=-0.31466399999739014, gamma=0.037425518972900355\n",
      "Gradient Descent(2898/4999): loss=0.28956195069985813, w0=-0.31466399999733563, gamma=0.16131479301988155\n",
      "Gradient Descent(2899/4999): loss=0.28956194842868677, w0=-0.3146639999971096, gamma=0.7265552844032913\n",
      "Gradient Descent(2900/4999): loss=0.28956193863942575, w0=-0.3146639999962556, gamma=0.32090372344346985\n",
      "Gradient Descent(2901/4999): loss=0.2895618945513856, w0=-0.3146639999961525, gamma=0.05489781782954346\n",
      "Gradient Descent(2902/4999): loss=0.2895618750842365, w0=-0.31466399999614053, gamma=0.05067407970243903\n",
      "Gradient Descent(2903/4999): loss=0.2895618717489501, w0=-0.3146639999961301, gamma=0.03709051746738924\n",
      "Gradient Descent(2904/4999): loss=0.28956186867417244, w0=-0.3146639999961228, gamma=0.03805830510522481\n",
      "Gradient Descent(2905/4999): loss=0.2895618664236707, w0=-0.31466399999611566, gamma=0.2407278285749949\n",
      "Gradient Descent(2906/4999): loss=0.2895618641144717, w0=-0.314663999996072, gamma=0.8009511071678181\n",
      "Gradient Descent(2907/4999): loss=0.2895618495083117, w0=-0.31466399999596173, gamma=0.25390558185064627\n",
      "Gradient Descent(2908/4999): loss=0.28956180091155553, w0=-0.31466399999595474, gamma=0.06258952300391746\n",
      "Gradient Descent(2909/4999): loss=0.2895617855071755, w0=-0.31466399999595346, gamma=0.0417840572881228\n",
      "Gradient Descent(2910/4999): loss=0.28956178170906793, w0=-0.3146639999959527, gamma=0.03571143866655442\n",
      "Gradient Descent(2911/4999): loss=0.28956177917375053, w0=-0.314663999995952, gamma=0.040640642323744966\n",
      "Gradient Descent(2912/4999): loss=0.28956177700705055, w0=-0.3146639999959513, gamma=0.1685598980411885\n",
      "Gradient Descent(2913/4999): loss=0.2895617745412965, w0=-0.3146639999959484, gamma=0.4268098331158568\n",
      "Gradient Descent(2914/4999): loss=0.2895617643144331, w0=-0.31466399999594236, gamma=0.36192071823388555\n",
      "Gradient Descent(2915/4999): loss=0.28956173841917604, w0=-0.31466399999593947, gamma=0.08783298567171244\n",
      "Gradient Descent(2916/4999): loss=0.28956171646115536, w0=-0.314663999995939, gamma=0.059294550938907835\n",
      "Gradient Descent(2917/4999): loss=0.2895617111321405, w0=-0.31466399999593875, gamma=0.035824373355052085\n",
      "Gradient Descent(2918/4999): loss=0.28956170753468424, w0=-0.3146639999959386, gamma=0.03619928142014892\n",
      "Gradient Descent(2919/4999): loss=0.28956170536118514, w0=-0.3146639999959384, gamma=1.4766794179854401\n",
      "Gradient Descent(2920/4999): loss=0.28956170316495206, w0=-0.31466399999593236, gamma=9.57087557321728\n",
      "Gradient Descent(2921/4999): loss=0.2895616135745479, w0=-0.3146639999959527, gamma=0.12646678675886333\n",
      "Gradient Descent(2922/4999): loss=0.289561032980802, w0=-0.3146639999959504, gamma=0.07118651201301425\n",
      "Gradient Descent(2923/4999): loss=0.289561025288876, w0=-0.3146639999959493, gamma=0.04259341310380832\n",
      "Gradient Descent(2924/4999): loss=0.28956102095886416, w0=-0.3146639999959487, gamma=0.03515718373077148\n",
      "Gradient Descent(2925/4999): loss=0.28956101836980624, w0=-0.3146639999959482, gamma=0.03557066908156094\n",
      "Gradient Descent(2926/4999): loss=0.2895610162369263, w0=-0.3146639999959477, gamma=0.09349817094328454\n",
      "Gradient Descent(2927/4999): loss=0.2895610140792384, w0=-0.31466399999594646, gamma=0.12459468055017253\n",
      "Gradient Descent(2928/4999): loss=0.28956100840773624, w0=-0.314663999995945, gamma=1.2297774005900666\n",
      "Gradient Descent(2929/4999): loss=0.2895610008499705, w0=-0.3146639999959326, gamma=1.814335638336886\n",
      "Gradient Descent(2930/4999): loss=0.2895609262536719, w0=-0.31466399999593686, gamma=0.06326121695225748\n",
      "Gradient Descent(2931/4999): loss=0.28956081621172086, w0=-0.31466399999593675, gamma=0.05582795154443702\n",
      "Gradient Descent(2932/4999): loss=0.2895608123648342, w0=-0.31466399999593664, gamma=0.051234366189266835\n",
      "Gradient Descent(2933/4999): loss=0.2895608089782036, w0=-0.3146639999959366, gamma=0.039911021118944706\n",
      "Gradient Descent(2934/4999): loss=0.28956080587054456, w0=-0.3146639999959365, gamma=0.04238217164417988\n",
      "Gradient Descent(2935/4999): loss=0.28956080344972385, w0=-0.31466399999593647, gamma=0.21447568683611284\n",
      "Gradient Descent(2936/4999): loss=0.289560800879019, w0=-0.31466399999593614, gamma=1.4302476740224992\n",
      "Gradient Descent(2937/4999): loss=0.2895607878699448, w0=-0.31466399999593464, gamma=0.2194548654451043\n",
      "Gradient Descent(2938/4999): loss=0.2895607011188184, w0=-0.3146639999959347, gamma=0.038011193478617644\n",
      "Gradient Descent(2939/4999): loss=0.2895606878101858, w0=-0.3146639999959347, gamma=0.03536754650460619\n",
      "Gradient Descent(2940/4999): loss=0.2895606855025237, w0=-0.3146639999959347, gamma=0.04545770094337599\n",
      "Gradient Descent(2941/4999): loss=0.2895606833573151, w0=-0.3146639999959347, gamma=0.08840957493196122\n",
      "Gradient Descent(2942/4999): loss=0.2895606806001428, w0=-0.31466399999593475, gamma=0.12599759522112375\n",
      "Gradient Descent(2943/4999): loss=0.2895606752378122, w0=-0.3146639999959348, gamma=3.4835722692512814\n",
      "Gradient Descent(2944/4999): loss=0.28956066759566157, w0=-0.3146639999959357, gamma=3.9027926975135885\n",
      "Gradient Descent(2945/4999): loss=0.2895604563095177, w0=-0.31466399999593414, gamma=0.03954973407294317\n",
      "Gradient Descent(2946/4999): loss=0.2895602198809318, w0=-0.3146639999959342, gamma=0.035845372241769605\n",
      "Gradient Descent(2947/4999): loss=0.289560217228318, w0=-0.31466399999593425, gamma=0.04480081492266627\n",
      "Gradient Descent(2948/4999): loss=0.2895602150422905, w0=-0.3146639999959343, gamma=0.07939434686996416\n",
      "Gradient Descent(2949/4999): loss=0.2895602123223516, w0=-0.3146639999959344, gamma=0.07984477686879052\n",
      "Gradient Descent(2950/4999): loss=0.2895602075067458, w0=-0.31466399999593453, gamma=0.08046653770912976\n",
      "Gradient Descent(2951/4999): loss=0.2895602026645367, w0=-0.3146639999959346, gamma=0.06216877438041754\n",
      "Gradient Descent(2952/4999): loss=0.2895601977846278, w0=-0.31466399999593464, gamma=0.05112800719853511\n",
      "Gradient Descent(2953/4999): loss=0.28956019401439476, w0=-0.3146639999959347, gamma=0.06184837989709118\n",
      "Gradient Descent(2954/4999): loss=0.2895601909137334, w0=-0.31466399999593475, gamma=0.16205272554614567\n",
      "Gradient Descent(2955/4999): loss=0.2895601871629381, w0=-0.31466399999593486, gamma=0.2518031493389913\n",
      "Gradient Descent(2956/4999): loss=0.28956017733526473, w0=-0.31466399999593503, gamma=0.051520448154732786\n",
      "Gradient Descent(2957/4999): loss=0.28956016206475627, w0=-0.31466399999593503, gamma=0.03546257024533174\n",
      "Gradient Descent(2958/4999): loss=0.28956015894030346, w0=-0.31466399999593503, gamma=0.036975808958424346\n",
      "Gradient Descent(2959/4999): loss=0.28956015678968894, w0=-0.31466399999593503, gamma=12.895489871241447\n",
      "Gradient Descent(2960/4999): loss=0.28956015454731193, w0=-0.3146639999959405, gamma=3814.9853255306502\n",
      "Gradient Descent(2961/4999): loss=0.2895593725504287, w0=-0.3146639999786154, gamma=0.06603229973788709\n",
      "Gradient Descent(2962/4999): loss=0.2893359838179919, w0=-0.3146639999797696, gamma=0.035035117158634145\n",
      "Gradient Descent(2963/4999): loss=0.2893350648098138, w0=-0.31466399998034184, gamma=0.0350324605985939\n",
      "Gradient Descent(2964/4999): loss=0.28933177409682703, w0=-0.31466399998089384, gamma=0.10262466627649604\n",
      "Gradient Descent(2965/4999): loss=0.2893317719466981, w0=-0.3146639999824542, gamma=0.10282239597710457\n",
      "Gradient Descent(2966/4999): loss=0.28933176598542476, w0=-0.31466399998385713, gamma=1.1188915114669231\n",
      "Gradient Descent(2967/4999): loss=0.2893317601329637, w0=-0.3146639999975537, gamma=1.210838744808908\n",
      "Gradient Descent(2968/4999): loss=0.28933169645558376, w0=-0.31466399999579164, gamma=0.05520606489677622\n",
      "Gradient Descent(2969/4999): loss=0.289331627607761, w0=-0.3146639999958086, gamma=0.05364152578917483\n",
      "Gradient Descent(2970/4999): loss=0.2893316244181641, w0=-0.3146639999958241, gamma=0.055382594086720874\n",
      "Gradient Descent(2971/4999): loss=0.28933162136608126, w0=-0.3146639999958393, gamma=0.4448547119091127\n",
      "Gradient Descent(2972/4999): loss=0.28933161821498615, w0=-0.3146639999959547, gamma=2.142847911182723\n",
      "Gradient Descent(2973/4999): loss=0.2893315929042458, w0=-0.31466399999626293, gamma=0.06707314836104929\n",
      "Gradient Descent(2974/4999): loss=0.289331470987245, w0=-0.31466399999625194, gamma=0.03587228664840848\n",
      "Gradient Descent(2975/4999): loss=0.2893314671706727, w0=-0.31466399999624645, gamma=0.035045947654630774\n",
      "Gradient Descent(2976/4999): loss=0.2893314651285903, w0=-0.3146639999962413, gamma=0.05062088156883569\n",
      "Gradient Descent(2977/4999): loss=0.28933146313468494, w0=-0.31466399999623407, gamma=0.8177688954387298\n",
      "Gradient Descent(2978/4999): loss=0.2893314602546616, w0=-0.3146639999961234, gamma=7.972780297455236\n",
      "Gradient Descent(2979/4999): loss=0.2893314137287123, w0=-0.31466399999592726, gamma=0.15006362809910156\n",
      "Gradient Descent(2980/4999): loss=0.28933096015823045, w0=-0.314663999995953, gamma=0.07269576607660255\n",
      "Gradient Descent(2981/4999): loss=0.289330951623313, w0=-0.3146639999959636, gamma=0.072473951434726\n",
      "Gradient Descent(2982/4999): loss=0.28933094747342936, w0=-0.31466399999597344, gamma=0.037173661563830625\n",
      "Gradient Descent(2983/4999): loss=0.28933094335066967, w0=-0.3146639999959781, gamma=0.03635756238222365\n",
      "Gradient Descent(2984/4999): loss=0.28933094123599995, w0=-0.3146639999959825, gamma=0.33640569030772893\n",
      "Gradient Descent(2985/4999): loss=0.28933093916776403, w0=-0.3146639999960217, gamma=4.685369227181093\n",
      "Gradient Descent(2986/4999): loss=0.28933092003102706, w0=-0.31466399999638456, gamma=1.2018612121481602\n",
      "Gradient Descent(2987/4999): loss=0.28933065350577525, w0=-0.31466399999604155, gamma=0.04528021681724493\n",
      "Gradient Descent(2988/4999): loss=0.2893305852365462, w0=-0.31466399999604416, gamma=0.03693567512725948\n",
      "Gradient Descent(2989/4999): loss=0.28933058257943634, w0=-0.3146639999960462, gamma=0.03877953885846752\n",
      "Gradient Descent(2990/4999): loss=0.28933058046885096, w0=-0.31466399999604827, gamma=0.09383567349885785\n",
      "Gradient Descent(2991/4999): loss=0.28933057826113046, w0=-0.31466399999605305, gamma=0.1078057306680189\n",
      "Gradient Descent(2992/4999): loss=0.28933057292238756, w0=-0.31466399999605804, gamma=0.09265855068750985\n",
      "Gradient Descent(2993/4999): loss=0.2893305667902968, w0=-0.31466399999606187, gamma=0.046309078409391\n",
      "Gradient Descent(2994/4999): loss=0.2893305615198514, w0=-0.3146639999960636, gamma=0.03782817852692397\n",
      "Gradient Descent(2995/4999): loss=0.2893305588857761, w0=-0.3146639999960649, gamma=0.0531798352841529\n",
      "Gradient Descent(2996/4999): loss=0.28933055673410324, w0=-0.31466399999606676, gamma=1.0511871348821422\n",
      "Gradient Descent(2997/4999): loss=0.2893305537092286, w0=-0.314663999996101, gamma=2.4616603478253634\n",
      "Gradient Descent(2998/4999): loss=0.28933049391789434, w0=-0.3146639999960969, gamma=0.05475787486720087\n",
      "Gradient Descent(2999/4999): loss=0.28933035390733275, w0=-0.31466399999609707, gamma=0.035293744207651336\n",
      "Gradient Descent(3000/4999): loss=0.289330350789539, w0=-0.3146639999960972, gamma=0.03543954016359687\n",
      "Gradient Descent(3001/4999): loss=0.2893303487806524, w0=-0.31466399999609723, gamma=0.2032089839063582\n",
      "Gradient Descent(3002/4999): loss=0.28933034676482544, w0=-0.3146639999960977, gamma=0.21392250558081524\n",
      "Gradient Descent(3003/4999): loss=0.28933033520663465, w0=-0.314663999996098, gamma=0.32403951164319783\n",
      "Gradient Descent(3004/4999): loss=0.2893303230394309, w0=-0.31466399999609845, gamma=0.3695445083759524\n",
      "Gradient Descent(3005/4999): loss=0.2893303046092138, w0=-0.31466399999609884, gamma=0.0634084901562302\n",
      "Gradient Descent(3006/4999): loss=0.28933028359100454, w0=-0.3146639999960989, gamma=0.03856318433589249\n",
      "Gradient Descent(3007/4999): loss=0.2893302799845462, w0=-0.3146639999960989, gamma=0.035640525635858296\n",
      "Gradient Descent(3008/4999): loss=0.28933027779120696, w0=-0.3146639999960989, gamma=0.077234300171781\n",
      "Gradient Descent(3009/4999): loss=0.2893302757641218, w0=-0.31466399999609895, gamma=0.908307850336909\n",
      "Gradient Descent(3010/4999): loss=0.2893302713713597, w0=-0.3146639999960995, gamma=5.442475632051429\n",
      "Gradient Descent(3011/4999): loss=0.2893302197108631, w0=-0.31466399999610034, gamma=0.13872447684955239\n",
      "Gradient Descent(3012/4999): loss=0.2893299101841739, w0=-0.3146639999961003, gamma=0.04254462675731397\n",
      "Gradient Descent(3013/4999): loss=0.2893299023029326, w0=-0.3146639999961003, gamma=0.035606866587277713\n",
      "Gradient Descent(3014/4999): loss=0.2893298998687586, w0=-0.3146639999961003, gamma=0.035971526591584924\n",
      "Gradient Descent(3015/4999): loss=0.2893298978430536, w0=-0.3146639999961003, gamma=0.0644247166421347\n",
      "Gradient Descent(3016/4999): loss=0.28932989579733115, w0=-0.31466399999610023, gamma=0.08787783333568931\n",
      "Gradient Descent(3017/4999): loss=0.28932989213349103, w0=-0.3146639999961002, gamma=0.3227263330253669\n",
      "Gradient Descent(3018/4999): loss=0.28932988713588437, w0=-0.31466399999610006, gamma=1.8309505679586493\n",
      "Gradient Descent(3019/4999): loss=0.28932986878250616, w0=-0.3146639999960997, gamma=0.21547533463855284\n",
      "Gradient Descent(3020/4999): loss=0.2893297646581163, w0=-0.31466399999609973, gamma=0.04845751385734206\n",
      "Gradient Descent(3021/4999): loss=0.2893297524064155, w0=-0.31466399999609973, gamma=0.04034437312026379\n",
      "Gradient Descent(3022/4999): loss=0.2893297496487178, w0=-0.31466399999609973, gamma=0.03641535720102786\n",
      "Gradient Descent(3023/4999): loss=0.28932974735425504, w0=-0.31466399999609973, gamma=0.04427259275086619\n",
      "Gradient Descent(3024/4999): loss=0.2893297452833882, w0=-0.31466399999609973, gamma=0.143018033936335\n",
      "Gradient Descent(3025/4999): loss=0.2893297427657065, w0=-0.31466399999609973, gamma=0.396264995618721\n",
      "Gradient Descent(3026/4999): loss=0.2893297346326099, w0=-0.31466399999609973, gamma=0.7042187397016394\n",
      "Gradient Descent(3027/4999): loss=0.2893297120980356, w0=-0.31466399999609995, gamma=0.06028020347252496\n",
      "Gradient Descent(3028/4999): loss=0.28932967205153626, w0=-0.31466399999609995, gamma=0.041150416306829724\n",
      "Gradient Descent(3029/4999): loss=0.2893296686233441, w0=-0.31466399999609995, gamma=0.03523652736384303\n",
      "Gradient Descent(3030/4999): loss=0.28932966628319967, w0=-0.31466399999609995, gamma=0.040199467533685994\n",
      "Gradient Descent(3031/4999): loss=0.28932966427942036, w0=-0.31466399999609995, gamma=1.6673530226352693\n",
      "Gradient Descent(3032/4999): loss=0.2893296619934196, w0=-0.31466399999609984, gamma=4.378364038992186\n",
      "Gradient Descent(3033/4999): loss=0.28932956717764624, w0=-0.31466399999610056, gamma=0.12746546331005762\n",
      "Gradient Descent(3034/4999): loss=0.28932931822609004, w0=-0.3146639999961005, gamma=0.08491603293782424\n",
      "Gradient Descent(3035/4999): loss=0.28932931096204473, w0=-0.3146639999961005, gamma=0.05064477255155219\n",
      "Gradient Descent(3036/4999): loss=0.2893293061313831, w0=-0.3146639999961005, gamma=0.039020129088361985\n",
      "Gradient Descent(3037/4999): loss=0.2893293032504673, w0=-0.3146639999961005, gamma=0.035499621198553793\n",
      "Gradient Descent(3038/4999): loss=0.2893293010316006, w0=-0.3146639999961005, gamma=0.04337641675530916\n",
      "Gradient Descent(3039/4999): loss=0.28932929901305726, w0=-0.3146639999961005, gamma=0.25943501349872217\n",
      "Gradient Descent(3040/4999): loss=0.28932929654663647, w0=-0.31466399999610045, gamma=1.3230670798989288\n",
      "Gradient Descent(3041/4999): loss=0.28932928179496126, w0=-0.31466399999610023, gamma=0.3172334077112132\n",
      "Gradient Descent(3042/4999): loss=0.28932920656504535, w0=-0.3146639999961003, gamma=0.061742189483305315\n",
      "Gradient Descent(3043/4999): loss=0.28932918852860756, w0=-0.3146639999961003, gamma=0.05414299993915152\n",
      "Gradient Descent(3044/4999): loss=0.2893291850167132, w0=-0.3146639999961003, gamma=0.03784380134384033\n",
      "Gradient Descent(3045/4999): loss=0.28932918193813634, w0=-0.3146639999961003, gamma=0.03734455812101776\n",
      "Gradient Descent(3046/4999): loss=0.2893291797863578, w0=-0.3146639999961003, gamma=0.1288248764389354\n",
      "Gradient Descent(3047/4999): loss=0.2893291776629805, w0=-0.31466399999610034, gamma=0.3290003333541671\n",
      "Gradient Descent(3048/4999): loss=0.2893291703381279, w0=-0.3146639999961003, gamma=1.967900379432098\n",
      "Gradient Descent(3049/4999): loss=0.2893291516315545, w0=-0.3146639999961003, gamma=0.09383235359374299\n",
      "Gradient Descent(3050/4999): loss=0.289329039740905, w0=-0.3146639999961003, gamma=0.03574388087103227\n",
      "Gradient Descent(3051/4999): loss=0.2893290344066601, w0=-0.3146639999961003, gamma=0.035057034049709196\n",
      "Gradient Descent(3052/4999): loss=0.2893290323730082, w0=-0.3146639999961003, gamma=0.05386133235439343\n",
      "Gradient Descent(3053/4999): loss=0.2893290303797767, w0=-0.3146639999961003, gamma=0.39312671369713104\n",
      "Gradient Descent(3054/4999): loss=0.2893290273173961, w0=-0.3146639999961004, gamma=6.305222920753023\n",
      "Gradient Descent(3055/4999): loss=0.2893290049655364, w0=-0.3146639999961007, gamma=0.7120279161561357\n",
      "Gradient Descent(3056/4999): loss=0.2893286464832826, w0=-0.31466399999610073, gamma=0.07962621972078807\n",
      "Gradient Descent(3057/4999): loss=0.2893286060649337, w0=-0.31466399999610073, gamma=0.06758147997096257\n",
      "Gradient Descent(3058/4999): loss=0.28932860148104117, w0=-0.31466399999610073, gamma=0.056818392584147406\n",
      "Gradient Descent(3059/4999): loss=0.2893285976343986, w0=-0.31466399999610073, gamma=0.040862102503220865\n",
      "Gradient Descent(3060/4999): loss=0.28932859440391995, w0=-0.31466399999610073, gamma=0.03708922478483878\n",
      "Gradient Descent(3061/4999): loss=0.2893285920808239, w0=-0.31466399999610073, gamma=0.05859655435267667\n",
      "Gradient Descent(3062/4999): loss=0.2893285899722882, w0=-0.31466399999610073, gamma=0.14319704600283306\n",
      "Gradient Descent(3063/4999): loss=0.28932858664106714, w0=-0.31466399999610073, gamma=0.29029309084149424\n",
      "Gradient Descent(3064/4999): loss=0.2893285785003203, w0=-0.3146639999961008, gamma=0.3620915770493003\n",
      "Gradient Descent(3065/4999): loss=0.2893285619972191, w0=-0.31466399999610084, gamma=0.04362359925518099\n",
      "Gradient Descent(3066/4999): loss=0.2893285414125936, w0=-0.31466399999610084, gamma=0.03520474886244912\n",
      "Gradient Descent(3067/4999): loss=0.2893285389325196, w0=-0.31466399999610084, gamma=0.03693513828142274\n",
      "Gradient Descent(3068/4999): loss=0.28932853693114907, w0=-0.31466399999610084, gamma=0.5727239447918652\n",
      "Gradient Descent(3069/4999): loss=0.28932853483141513, w0=-0.31466399999610084, gamma=1.2568049184722856\n",
      "Gradient Descent(3070/4999): loss=0.2893285022725973, w0=-0.3146639999961009, gamma=0.15808791840683217\n",
      "Gradient Descent(3071/4999): loss=0.2893284308257551, w0=-0.3146639999961009, gamma=0.10280968485176391\n",
      "Gradient Descent(3072/4999): loss=0.2893284218384611, w0=-0.3146639999961009, gamma=0.08966317103228771\n",
      "Gradient Descent(3073/4999): loss=0.2893284159938425, w0=-0.3146639999961009, gamma=0.03655396562424505\n",
      "Gradient Descent(3074/4999): loss=0.289328410896737, w0=-0.3146639999961009, gamma=0.035655398353553615\n",
      "Gradient Descent(3075/4999): loss=0.28932840881872823, w0=-0.3146639999961009, gamma=0.36451077209526755\n",
      "Gradient Descent(3076/4999): loss=0.2893284067918164, w0=-0.31466399999610095, gamma=514.3267944137126\n",
      "Gradient Descent(3077/4999): loss=0.2893283860704155, w0=-0.3146639999961275, gamma=9.468191553164104\n",
      "Gradient Descent(3078/4999): loss=0.2892992058322819, w0=-0.31466399999608996, gamma=0.036958217009738635\n",
      "Gradient Descent(3079/4999): loss=0.2893128345714271, w0=-0.3146639999960915, gamma=0.03572302847219757\n",
      "Gradient Descent(3080/4999): loss=0.2892989685530785, w0=-0.3146639999960927, gamma=0.049287739041784635\n",
      "Gradient Descent(3081/4999): loss=0.2892987141451396, w0=-0.31466399999609423, gamma=0.061203513401498655\n",
      "Gradient Descent(3082/4999): loss=0.2892986680411666, w0=-0.31466399999609607, gamma=0.0699044903279139\n",
      "Gradient Descent(3083/4999): loss=0.2892986616567072, w0=-0.314663999996098, gamma=0.09968105852385321\n",
      "Gradient Descent(3084/4999): loss=0.28929865739515276, w0=-0.3146639999961006, gamma=0.09075917999169171\n",
      "Gradient Descent(3085/4999): loss=0.28929865168417573, w0=-0.31466399999610273, gamma=0.03881102856967227\n",
      "Gradient Descent(3086/4999): loss=0.28929864656140764, w0=-0.31466399999610356, gamma=0.035347653033704116\n",
      "Gradient Descent(3087/4999): loss=0.28929864436002717, w0=-0.3146639999961043, gamma=0.049249556086892146\n",
      "Gradient Descent(3088/4999): loss=0.28929864236575875, w0=-0.3146639999961053, gamma=0.1713335741041065\n",
      "Gradient Descent(3089/4999): loss=0.2892986395877255, w0=-0.3146639999961085, gamma=0.17393492687781903\n",
      "Gradient Descent(3090/4999): loss=0.2892986299246528, w0=-0.31466399999611117, gamma=0.22259842525937953\n",
      "Gradient Descent(3091/4999): loss=0.28929862011548874, w0=-0.31466399999611405, gamma=0.05732771320483932\n",
      "Gradient Descent(3092/4999): loss=0.2892986075619666, w0=-0.3146639999961146, gamma=0.03612204807111682\n",
      "Gradient Descent(3093/4999): loss=0.28929860432894616, w0=-0.31466399999611494, gamma=0.036898290511738076\n",
      "Gradient Descent(3094/4999): loss=0.28929860229182974, w0=-0.3146639999961153, gamma=1.7772499185672928\n",
      "Gradient Descent(3095/4999): loss=0.2892986002109431, w0=-0.31466399999613065, gamma=38.05973849275729\n",
      "Gradient Descent(3096/4999): loss=0.2892984999832881, w0=-0.3146639999958766, gamma=0.19860717152362556\n",
      "Gradient Descent(3097/4999): loss=0.2892963541767567, w0=-0.3146639999959261, gamma=0.04353926882177866\n",
      "Gradient Descent(3098/4999): loss=0.2892963437396913, w0=-0.3146639999959348, gamma=0.035410435113462584\n",
      "Gradient Descent(3099/4999): loss=0.28929634039780844, w0=-0.3146639999959416, gamma=0.036569871631141344\n",
      "Gradient Descent(3100/4999): loss=0.28929633833381074, w0=-0.3146639999959483, gamma=0.0988928951238392\n",
      "Gradient Descent(3101/4999): loss=0.2892963362651452, w0=-0.31466399999596584, gamma=0.10859557679601604\n",
      "Gradient Descent(3102/4999): loss=0.2892963306853105, w0=-0.3146639999959832, gamma=0.09476252131360682\n",
      "Gradient Descent(3103/4999): loss=0.2892963245644567, w0=-0.3146639999959967, gamma=0.0750661736623899\n",
      "Gradient Descent(3104/4999): loss=0.2892963192235423, w0=-0.3146639999960064, gamma=0.05920280672253716\n",
      "Gradient Descent(3105/4999): loss=0.289296314992754, w0=-0.31466399999601347, gamma=0.04953727269945522\n",
      "Gradient Descent(3106/4999): loss=0.28929631165604214, w0=-0.314663999996019, gamma=0.06515624805307799\n",
      "Gradient Descent(3107/4999): loss=0.2892963088640891, w0=-0.31466399999602596, gamma=0.21028910862905698\n",
      "Gradient Descent(3108/4999): loss=0.2892963051918436, w0=-0.31466399999604694, gamma=0.2721701258090613\n",
      "Gradient Descent(3109/4999): loss=0.2892962933398417, w0=-0.3146639999960684, gamma=0.04451097165870975\n",
      "Gradient Descent(3110/4999): loss=0.28929627800028795, w0=-0.314663999996071, gamma=0.035238815544657215\n",
      "Gradient Descent(3111/4999): loss=0.28929627549160236, w0=-0.3146639999960729, gamma=0.03785725993410456\n",
      "Gradient Descent(3112/4999): loss=0.2892962735055358, w0=-0.3146639999960749, gamma=78.81465988920299\n",
      "Gradient Descent(3113/4999): loss=0.2892962713718977, w0=-0.3146640000000881, gamma=3781.666091027693\n",
      "Gradient Descent(3114/4999): loss=0.28929183069206366, w0=-0.3146639850303849, gamma=0.04910648286411717\n",
      "Gradient Descent(3115/4999): loss=0.2891487588059131, w0=-0.3146639857653087, gamma=0.03610461247339176\n",
      "Gradient Descent(3116/4999): loss=0.2890976299160386, w0=-0.31466398627911285, gamma=0.03743662774243362\n",
      "Gradient Descent(3117/4999): loss=0.28908636918921443, w0=-0.314663986792638, gamma=0.11925840897694846\n",
      "Gradient Descent(3118/4999): loss=0.2890841222849533, w0=-0.31466398836728565, gamma=0.12493029881787826\n",
      "Gradient Descent(3119/4999): loss=0.28908196388034774, w0=-0.314663989820102, gamma=0.06793272575998685\n",
      "Gradient Descent(3120/4999): loss=0.2890819535088683, w0=-0.314663990511399, gamma=0.03530057207649962\n",
      "Gradient Descent(3121/4999): loss=0.28908194966398754, w0=-0.31466399084622143, gamma=0.0350419501903713\n",
      "Gradient Descent(3122/4999): loss=0.28908194674331517, w0=-0.31466399116685806, gamma=0.125834506746371\n",
      "Gradient Descent(3123/4999): loss=0.289081944871236, w0=-0.3146639922779067, gamma=0.44446062677780224\n",
      "Gradient Descent(3124/4999): loss=0.2890819381540982, w0=-0.3146639957084292, gamma=0.4371077427149659\n",
      "Gradient Descent(3125/4999): loss=0.2890819144706506, w0=-0.3146639975826912, gamma=0.06751594073165867\n",
      "Gradient Descent(3126/4999): loss=0.289081891214608, w0=-0.31466399774564835, gamma=0.06391325518902495\n",
      "Gradient Descent(3127/4999): loss=0.28908188760880993, w0=-0.3146639978894949, gamma=0.05901399436636305\n",
      "Gradient Descent(3128/4999): loss=0.2890818842065498, w0=-0.31466399801382594, gamma=0.06283772294221766\n",
      "Gradient Descent(3129/4999): loss=0.2890818810652587, w0=-0.3146639981384002, gamma=0.101791978696916\n",
      "Gradient Descent(3130/4999): loss=0.289081877720439, w0=-0.3146639983275196, gamma=0.13792991683350564\n",
      "Gradient Descent(3131/4999): loss=0.2890818723021124, w0=-0.3146639985576945, gamma=0.06293037727585961\n",
      "Gradient Descent(3132/4999): loss=0.28908186496020283, w0=-0.3146639986482266, gamma=0.03819013103554257\n",
      "Gradient Descent(3133/4999): loss=0.2890818616104691, w0=-0.31466399869970973, gamma=0.03997631940254963\n",
      "Gradient Descent(3134/4999): loss=0.2890818595776395, w0=-0.31466399875154266, gamma=0.6261105289642674\n",
      "Gradient Descent(3135/4999): loss=0.28908185744973636, w0=-0.3146639995308991, gamma=14.323447622191155\n",
      "Gradient Descent(3136/4999): loss=0.28908182412255196, w0=-0.314664006197061, gamma=0.1395150268673247\n",
      "Gradient Descent(3137/4999): loss=0.28908106177023535, w0=-0.31466400533196187, gamma=0.03503365670273013\n",
      "Gradient Descent(3138/4999): loss=0.2890810544703205, w0=-0.3146640051450342, gamma=0.03503207299401876\n",
      "Gradient Descent(3139/4999): loss=0.28908105246576693, w0=-0.3146640049646634, gamma=0.406715110199578\n",
      "Gradient Descent(3140/4999): loss=0.28908105060152334, w0=-0.31466400294395563, gamma=1.2029443226923113\n",
      "Gradient Descent(3141/4999): loss=0.289081028958108, w0=-0.314663999398092, gamma=0.12540694232893984\n",
      "Gradient Descent(3142/4999): loss=0.28908096494419555, w0=-0.31466399947311163, gamma=0.054768044411194185\n",
      "Gradient Descent(3143/4999): loss=0.2890809582709786, w0=-0.3146639995017657, gamma=0.05400449542581181\n",
      "Gradient Descent(3144/4999): loss=0.28908095535616285, w0=-0.3146639995284729, gamma=0.13744521300079574\n",
      "Gradient Descent(3145/4999): loss=0.28908095248238913, w0=-0.31466399959277375, gamma=1.258999861897157\n",
      "Gradient Descent(3146/4999): loss=0.2890809451684464, w0=-0.31466400010081563, gamma=1.9753469448589465\n",
      "Gradient Descent(3147/4999): loss=0.2890808781731326, w0=-0.31466399989436467, gamma=0.08137086974722493\n",
      "Gradient Descent(3148/4999): loss=0.28908077306902824, w0=-0.31466399990265936, gamma=0.07272327519007742\n",
      "Gradient Descent(3149/4999): loss=0.28908076873164845, w0=-0.31466399990946936, gamma=0.07232201165603108\n",
      "Gradient Descent(3150/4999): loss=0.28908076486187206, w0=-0.3146639999157493, gamma=0.12605395466121364\n",
      "Gradient Descent(3151/4999): loss=0.2890807610135787, w0=-0.31466399992590327, gamma=0.23906155188266195\n",
      "Gradient Descent(3152/4999): loss=0.28908075430620334, w0=-0.3146639999427329, gamma=0.08851219811155962\n",
      "Gradient Descent(3153/4999): loss=0.2890807415857074, w0=-0.31466399994747446, gamma=0.03781240309257055\n",
      "Gradient Descent(3154/4999): loss=0.28908073687598307, w0=-0.31466399994932076, gamma=0.03593929689254414\n",
      "Gradient Descent(3155/4999): loss=0.28908073486397445, w0=-0.31466399995100924, gamma=0.13051275516746758\n",
      "Gradient Descent(3156/4999): loss=0.289080732951649, w0=-0.3146639999569205, gamma=5.171855574803977\n",
      "Gradient Descent(3157/4999): loss=0.2890807260070895, w0=-0.3146640001605957, gamma=2.4902928560360835\n",
      "Gradient Descent(3158/4999): loss=0.2890804508201614, w0=-0.3146639997514568, gamma=0.050007035891182286\n",
      "Gradient Descent(3159/4999): loss=0.28908031860268685, w0=-0.3146639997637008, gamma=0.04600819412003828\n",
      "Gradient Descent(3160/4999): loss=0.2890803156746573, w0=-0.31466399977440235, gamma=0.03593595719533498\n",
      "Gradient Descent(3161/4999): loss=0.2890803132177121, w0=-0.3146639997823765, gamma=0.03533022674198633\n",
      "Gradient Descent(3162/4999): loss=0.2890803113049387, w0=-0.3146639997899346, gamma=0.05834067838343529\n",
      "Gradient Descent(3163/4999): loss=0.2890803094252608, w0=-0.31466399980197424, gamma=0.12727032599143087\n",
      "Gradient Descent(3164/4999): loss=0.28908030632136744, w0=-0.31466399982670645, gamma=1.2795561353688534\n",
      "Gradient Descent(3165/4999): loss=0.289080299550232, w0=-0.3146640000437139, gamma=1.9738776152322632\n",
      "Gradient Descent(3166/4999): loss=0.28908023147489514, w0=-0.31466399995012956, gamma=0.07289220733178585\n",
      "Gradient Descent(3167/4999): loss=0.28908012646563724, w0=-0.3146639999534952, gamma=0.04574596758159987\n",
      "Gradient Descent(3168/4999): loss=0.28908012258576554, w0=-0.31466399995545347, gamma=0.03749745882859833\n",
      "Gradient Descent(3169/4999): loss=0.28908012015125445, w0=-0.3146639999569852, gamma=0.03678705361596514\n",
      "Gradient Descent(3170/4999): loss=0.28908011815631196, w0=-0.31466399995843153, gamma=0.12773676185404603\n",
      "Gradient Descent(3171/4999): loss=0.2890801161992282, w0=-0.31466399996326905, gamma=0.3549267204084598\n",
      "Gradient Descent(3172/4999): loss=0.28908010940362544, w0=-0.3146639999749935, gamma=0.3641799649663606\n",
      "Gradient Descent(3173/4999): loss=0.2890800905216868, w0=-0.3146639999827538, gamma=0.056763018782800864\n",
      "Gradient Descent(3174/4999): loss=0.28908007114778983, w0=-0.31466399998352285, gamma=0.04422864618687118\n",
      "Gradient Descent(3175/4999): loss=0.2890800681279311, w0=-0.31466399998408806, gamma=0.03593341110670726\n",
      "Gradient Descent(3176/4999): loss=0.28908006577500933, w0=-0.314663999984527, gamma=0.04591501616364202\n",
      "Gradient Descent(3177/4999): loss=0.2890800638633965, w0=-0.31466399998506767, gamma=6.052444862250749\n",
      "Gradient Descent(3178/4999): loss=0.2890800614207776, w0=-0.3146640000530661, gamma=20.014452299540366\n",
      "Gradient Descent(3179/4999): loss=0.28907973944692666, w0=-0.3146639989169794, gamma=0.06466924611525737\n",
      "Gradient Descent(3180/4999): loss=0.2890786760326917, w0=-0.3146639989867785, gamma=0.03630175306735161\n",
      "Gradient Descent(3181/4999): loss=0.2890786721100506, w0=-0.314663999023426, gamma=0.035589877540764425\n",
      "Gradient Descent(3182/4999): loss=0.28907866971051094, w0=-0.3146639990580506, gamma=0.1330135594978719\n",
      "Gradient Descent(3183/4999): loss=0.2890786677648153, w0=-0.31466399918285093, gamma=0.19790436162130962\n",
      "Gradient Descent(3184/4999): loss=0.28907866059090315, w0=-0.31466399934383676, gamma=0.19845918058742867\n",
      "Gradient Descent(3185/4999): loss=0.28907865004577793, w0=-0.3146639994733248, gamma=0.058315555947343764\n",
      "Gradient Descent(3186/4999): loss=0.28907863949159707, w0=-0.3146639995038226, gamma=0.046561190576067474\n",
      "Gradient Descent(3187/4999): loss=0.2890786363873144, w0=-0.3146639995267531, gamma=0.0367148713179392\n",
      "Gradient Descent(3188/4999): loss=0.2890786339109075, w0=-0.3146639995439926, gamma=0.044411600036184\n",
      "Gradient Descent(3189/4999): loss=0.28907863195836686, w0=-0.3146639995640805, gamma=0.46851606626444453\n",
      "Gradient Descent(3190/4999): loss=0.2890786295965673, w0=-0.314663999766584, gamma=0.5405034268360915\n",
      "Gradient Descent(3191/4999): loss=0.2890786046821292, w0=-0.3146639998907482, gamma=0.05488231657206841\n",
      "Gradient Descent(3192/4999): loss=0.28907857594109015, w0=-0.31466399989654137, gamma=0.035379236390493764\n",
      "Gradient Descent(3193/4999): loss=0.28907857302266504, w0=-0.3146639999000709, gamma=0.03530969003738649\n",
      "Gradient Descent(3194/4999): loss=0.28907857114135665, w0=-0.3146639999034688, gamma=0.893539504288246\n",
      "Gradient Descent(3195/4999): loss=0.289078569263786, w0=-0.31466399998642036, gamma=2.4119021922752313\n",
      "Gradient Descent(3196/4999): loss=0.2890785217505917, w0=-0.31466400001025774, gamma=0.1695086842426163\n",
      "Gradient Descent(3197/4999): loss=0.2890783935042705, w0=-0.3146640000078924, gamma=0.08343340205283287\n",
      "Gradient Descent(3198/4999): loss=0.2890783844908261, w0=-0.3146640000069255, gamma=0.07297128050217344\n",
      "Gradient Descent(3199/4999): loss=0.28907838005310543, w0=-0.3146640000061504, gamma=0.04120423429084937\n",
      "Gradient Descent(3200/4999): loss=0.2890783761730995, w0=-0.3146640000057447, gamma=0.036342465410062474\n",
      "Gradient Descent(3201/4999): loss=0.28907837398219294, w0=-0.31466400000540157, gamma=0.0517978912628511\n",
      "Gradient Descent(3202/4999): loss=0.28907837204981807, w0=-0.31466400000493033, gamma=0.7821476320309061\n",
      "Gradient Descent(3203/4999): loss=0.289078369295662, w0=-0.3146639999981831, gamma=1.945033764524799\n",
      "Gradient Descent(3204/4999): loss=0.28907832770811076, w0=-0.314663999994528, gamma=0.1489488385215632\n",
      "Gradient Descent(3205/4999): loss=0.28907822429125407, w0=-0.3146639999947925, gamma=0.03624151594722206\n",
      "Gradient Descent(3206/4999): loss=0.289078216374256, w0=-0.3146639999948473, gamma=0.0350806634656079\n",
      "Gradient Descent(3207/4999): loss=0.28907821444421256, w0=-0.31466399999489836, gamma=0.04668725450666312\n",
      "Gradient Descent(3208/4999): loss=0.28907821257900207, w0=-0.31466399999496397, gamma=0.19413385957487145\n",
      "Gradient Descent(3209/4999): loss=0.28907821009669266, w0=-0.3146639999952241, gamma=0.34089341000552126\n",
      "Gradient Descent(3210/4999): loss=0.28907819977485305, w0=-0.3146639999955922, gamma=0.4446058442260961\n",
      "Gradient Descent(3211/4999): loss=0.2890781816501041, w0=-0.31466399999590866, gamma=0.09163232130247348\n",
      "Gradient Descent(3212/4999): loss=0.28907815801134507, w0=-0.31466399999594485, gamma=0.06137573169603872\n",
      "Gradient Descent(3213/4999): loss=0.289078153139398, w0=-0.3146639999959669, gamma=0.054210410312875344\n",
      "Gradient Descent(3214/4999): loss=0.2890781498761733, w0=-0.31466399999598516, gamma=0.06632990926921237\n",
      "Gradient Descent(3215/4999): loss=0.2890781469939324, w0=-0.3146639999960063, gamma=0.1550370233293359\n",
      "Gradient Descent(3216/4999): loss=0.28907814346733024, w0=-0.3146639999960525, gamma=0.18591303395183642\n",
      "Gradient Descent(3217/4999): loss=0.2890781352243955, w0=-0.31466399999609923, gamma=0.04789049553146974\n",
      "Gradient Descent(3218/4999): loss=0.28907812533989696, w0=-0.31466399999610906, gamma=0.035725038487953824\n",
      "Gradient Descent(3219/4999): loss=0.28907812279367906, w0=-0.314663999996116, gamma=0.04017552530868626\n",
      "Gradient Descent(3220/4999): loss=0.28907812089427615, w0=-0.31466399999612354, gamma=13.070675341490135\n",
      "Gradient Descent(3221/4999): loss=0.2890781187582562, w0=-0.3146639999984829, gamma=462.6130640556345\n",
      "Gradient Descent(3222/4999): loss=0.28907742386342233, w0=-0.31466399899077807, gamma=0.03951626358840066\n",
      "Gradient Descent(3223/4999): loss=0.2890532388871207, w0=-0.3146639990305135, gamma=0.035059838734806545\n",
      "Gradient Descent(3224/4999): loss=0.2890528821373284, w0=-0.31466399906437464, gamma=0.035463218963137165\n",
      "Gradient Descent(3225/4999): loss=0.28905287393552737, w0=-0.31466399909742454, gamma=0.08126252657198614\n",
      "Gradient Descent(3226/4999): loss=0.2890528719035514, w0=-0.3146639991704714, gamma=0.08347756940289001\n",
      "Gradient Descent(3227/4999): loss=0.28905286752461834, w0=-0.3146639992394116, gamma=0.22738727468138292\n",
      "Gradient Descent(3228/4999): loss=0.28905286310553163, w0=-0.31466399941152384, gamma=0.2373733618206199\n",
      "Gradient Descent(3229/4999): loss=0.28905285108917694, w0=-0.3146639995503398, gamma=0.06118183075781839\n",
      "Gradient Descent(3230/4999): loss=0.2890528385533648, w0=-0.3146639995776259, gamma=0.053291673294061696\n",
      "Gradient Descent(3231/4999): loss=0.2890528353221195, w0=-0.314663999599939, gamma=0.04887836059358006\n",
      "Gradient Descent(3232/4999): loss=0.289052832507737, w0=-0.3146639996193137, gamma=0.06608505383934694\n",
      "Gradient Descent(3233/4999): loss=0.2890528299264315, w0=-0.3146639996442284, gamma=0.22291878940549856\n",
      "Gradient Descent(3234/4999): loss=0.28905282643643004, w0=-0.31466399972271714, gamma=0.27575000645364156\n",
      "Gradient Descent(3235/4999): loss=0.2890528146639408, w0=-0.31466399979816423, gamma=0.04340287606035914\n",
      "Gradient Descent(3236/4999): loss=0.2890528001014971, w0=-0.3146639998067649, gamma=0.03520870621046256\n",
      "Gradient Descent(3237/4999): loss=0.28905279780933285, w0=-0.31466399981343907, gamma=0.03811643922912647\n",
      "Gradient Descent(3238/4999): loss=0.2890527959499485, w0=-0.31466399982041, gamma=46.039674116295856\n",
      "Gradient Descent(3239/4999): loss=0.2890527939370099, w0=-0.31466400791943827, gamma=143.81841586911696\n",
      "Gradient Descent(3240/4999): loss=0.28905036298524783, w0=-0.31466286843003444, gamma=0.10356537143646859\n",
      "Gradient Descent(3241/4999): loss=0.28904316432051697, w0=-0.3146629856211184, gamma=0.06612681441677674\n",
      "Gradient Descent(3242/4999): loss=0.2890428575210742, w0=-0.3146630526985199, gamma=0.03513513441115601\n",
      "Gradient Descent(3243/4999): loss=0.2890428349461519, w0=-0.3146630859819559, gamma=0.035088594969787376\n",
      "Gradient Descent(3244/4999): loss=0.2890427835466666, w0=-0.31466311805343616, gamma=0.4549543019057184\n",
      "Gradient Descent(3245/4999): loss=0.2890427795854371, w0=-0.31466351929715086, gamma=0.5247990989325496\n",
      "Gradient Descent(3246/4999): loss=0.2890427410972214, w0=-0.31466377156764846, gamma=0.09464092504537143\n",
      "Gradient Descent(3247/4999): loss=0.2890427142931332, w0=-0.3146637931863505, gamma=0.07711419099216626\n",
      "Gradient Descent(3248/4999): loss=0.28904270785419034, w0=-0.314663809134335, gamma=0.06176298549603011\n",
      "Gradient Descent(3249/4999): loss=0.289042703709669, w0=-0.3146638209225418, gamma=0.05358630346488437\n",
      "Gradient Descent(3250/4999): loss=0.289042700447325, w0=-0.3146638305184443, gamma=0.05311182069896298\n",
      "Gradient Descent(3251/4999): loss=0.28904269762282003, w0=-0.31466383951972354, gamma=0.08260974601964773\n",
      "Gradient Descent(3252/4999): loss=0.28904269482384887, w0=-0.3146638527766555, gamma=0.16111958027478868\n",
      "Gradient Descent(3253/4999): loss=0.28904269047085684, w0=-0.31466387649662814, gamma=0.095225312992246\n",
      "Gradient Descent(3254/4999): loss=0.28904268198224353, w0=-0.3146638882569262, gamma=0.03804732101894437\n",
      "Gradient Descent(3255/4999): loss=0.2890426769661873, w0=-0.3146638925083113, gamma=0.035807189786541224\n",
      "Gradient Descent(3256/4999): loss=0.2890426749616419, w0=-0.3146638963571553, gamma=0.11821945598750962\n",
      "Gradient Descent(3257/4999): loss=0.28904267307551323, w0=-0.31466390860932253, gamma=0.7438173549733315\n",
      "Gradient Descent(3258/4999): loss=0.2890426668485263, w0=-0.3146639765845671, gamma=0.7408053569333589\n",
      "Gradient Descent(3259/4999): loss=0.2890426276730934, w0=-0.3146639939281289, gamma=0.03884456050184763\n",
      "Gradient Descent(3260/4999): loss=0.28904258866021043, w0=-0.3146639941638456, gamma=0.035038827415012276\n",
      "Gradient Descent(3261/4999): loss=0.2890425866132041, w0=-0.31466399436820913, gamma=0.03559392372189051\n",
      "Gradient Descent(3262/4999): loss=0.2890425847679025, w0=-0.31466399456853617, gamma=3.5436384742284806\n",
      "Gradient Descent(3263/4999): loss=0.2890425828933838, w0=-0.3146640138026867, gamma=4.602036820917048\n",
      "Gradient Descent(3264/4999): loss=0.2890423962737191, w0=-0.31466395026534033, gamma=0.2806149580217014\n",
      "Gradient Descent(3265/4999): loss=0.28904215395811994, w0=-0.31466396422060094, gamma=0.15615479936715176\n",
      "Gradient Descent(3266/4999): loss=0.28904213915739546, w0=-0.3146639698071533, gamma=0.05901477999341815\n",
      "Gradient Descent(3267/4999): loss=0.2890421309422578, w0=-0.31466397158876114, gamma=0.05769798409715318\n",
      "Gradient Descent(3268/4999): loss=0.28904212782096544, w0=-0.3146639732278208, gamma=0.046123093257749236\n",
      "Gradient Descent(3269/4999): loss=0.2890421247826776, w0=-0.3146639744624674, gamma=0.04343933449100111\n",
      "Gradient Descent(3270/4999): loss=0.28904212235392657, w0=-0.31466397557164155, gamma=0.08942972941467342\n",
      "Gradient Descent(3271/4999): loss=0.2890421200665039, w0=-0.31466397775593513, gamma=0.3774045224454519\n",
      "Gradient Descent(3272/4999): loss=0.28904211535733454, w0=-0.31466398614956215, gamma=0.26903434330949927\n",
      "Gradient Descent(3273/4999): loss=0.28904209548414056, w0=-0.31466398987481925, gamma=0.03736026319616747\n",
      "Gradient Descent(3274/4999): loss=0.289042081317714, w0=-0.3146639902529617, gamma=0.0350795773834131\n",
      "Gradient Descent(3275/4999): loss=0.2890420793502537, w0=-0.3146639905947551, gamma=0.045571957267054215\n",
      "Gradient Descent(3276/4999): loss=0.2890420775030666, w0=-0.3146639910232034, gamma=8.23297869817894\n",
      "Gradient Descent(3277/4999): loss=0.28904207510338426, w0=-0.31466406489879417, gamma=13.455406740860548\n",
      "Gradient Descent(3278/4999): loss=0.2890416415926918, w0=-0.3146631916097965, gamma=0.14112884995700548\n",
      "Gradient Descent(3279/4999): loss=0.28904093367508427, w0=-0.31466330569645895, gamma=0.08857479663271235\n",
      "Gradient Descent(3280/4999): loss=0.28904092593514463, w0=-0.31466336719392923, gamma=0.05612958225269179\n",
      "Gradient Descent(3281/4999): loss=0.2890409211707219, w0=-0.3146634027128639, gamma=0.03543393679743177\n",
      "Gradient Descent(3282/4999): loss=0.2890409181932514, w0=-0.31466342387696866, gamma=0.03546381050687936\n",
      "Gradient Descent(3283/4999): loss=0.28904091631588363, w0=-0.3146634443083567, gamma=0.298345063370547\n",
      "Gradient Descent(3284/4999): loss=0.2890409144456601, w0=-0.3146636100951217, gamma=0.43784174900576806\n",
      "Gradient Descent(3285/4999): loss=0.28904089871948735, w0=-0.3146637808101508, gamma=0.18183211040032896\n",
      "Gradient Descent(3286/4999): loss=0.2890408756602758, w0=-0.3146638206652372, gamma=0.08502509524476301\n",
      "Gradient Descent(3287/4999): loss=0.2890408660872279, w0=-0.31466383591288083, gamma=0.07676075230646029\n",
      "Gradient Descent(3288/4999): loss=0.28904086160845166, w0=-0.3146638485080502, gamma=0.03934680242222018\n",
      "Gradient Descent(3289/4999): loss=0.289040857567307, w0=-0.31466385446862977, gamma=0.03691123283533463\n",
      "Gradient Descent(3290/4999): loss=0.28904085549582553, w0=-0.31466385984023676, gamma=0.13602718589310075\n",
      "Gradient Descent(3291/4999): loss=0.2890408535526419, w0=-0.31466387890527636, gamma=0.9971283881864392\n",
      "Gradient Descent(3292/4999): loss=0.28904084639160754, w0=-0.31466399964860825, gamma=0.8236756806185078\n",
      "Gradient Descent(3293/4999): loss=0.28904079390138027, w0=-0.31466399993502225, gamma=0.03592516044748053\n",
      "Gradient Descent(3294/4999): loss=0.28904075055277234, w0=-0.31466399993722494, gamma=0.03517089594974537\n",
      "Gradient Descent(3295/4999): loss=0.2890407486527852, w0=-0.3146639999393039, gamma=0.04792567503685582\n",
      "Gradient Descent(3296/4999): loss=0.28904074680140357, w0=-0.31466399994203714, gamma=0.07150589737883202\n",
      "Gradient Descent(3297/4999): loss=0.28904074427865645, w0=-0.31466399994591976, gamma=0.2098539756022813\n",
      "Gradient Descent(3298/4999): loss=0.28904074051468737, w0=-0.3146639999564996, gamma=1.3883437061031207\n",
      "Gradient Descent(3299/4999): loss=0.2890407294682915, w0=-0.3146640000118048, gamma=0.6290959128259422\n",
      "Gradient Descent(3300/4999): loss=0.2890406563885291, w0=-0.3146640000020728, gamma=0.059743632511248104\n",
      "Gradient Descent(3301/4999): loss=0.2890406232774447, w0=-0.31466400000173, gamma=0.05165942308265348\n",
      "Gradient Descent(3302/4999): loss=0.28904062013008386, w0=-0.3146640000014513, gamma=0.040066808442915054\n",
      "Gradient Descent(3303/4999): loss=0.2890406174107407, w0=-0.3146640000012463, gamma=0.039291611234408955\n",
      "Gradient Descent(3304/4999): loss=0.2890406153017212, w0=-0.3146640000010533, gamma=0.08145303278973677\n",
      "Gradient Descent(3305/4999): loss=0.28904061323353125, w0=-0.314664000000669, gamma=0.15750920066313895\n",
      "Gradient Descent(3306/4999): loss=0.2890406089461049, w0=-0.31466399999998634, gamma=0.6165825419836803\n",
      "Gradient Descent(3307/4999): loss=0.28904060065534337, w0=-0.31466399999773503, gamma=0.11065280886519965\n",
      "Gradient Descent(3308/4999): loss=0.28904056820063595, w0=-0.3146639999975801, gamma=0.03540447097458176\n",
      "Gradient Descent(3309/4999): loss=0.2890405623764033, w0=-0.314663999997536, gamma=0.035091903663709935\n",
      "Gradient Descent(3310/4999): loss=0.28904056051271926, w0=-0.3146639999974939, gamma=0.5317400883133602\n",
      "Gradient Descent(3311/4999): loss=0.2890405586656205, w0=-0.3146639999968777, gamma=61.34321866330532\n",
      "Gradient Descent(3312/4999): loss=0.2890405306769919, w0=-0.31466399996358924, gamma=1.1210180040022537\n",
      "Gradient Descent(3313/4999): loss=0.2890373025583542, w0=-0.3146640000003011, gamma=0.05508619595010573\n",
      "Gradient Descent(3314/4999): loss=0.2890372564538588, w0=-0.31466400000008277, gamma=0.053875265621097214\n",
      "Gradient Descent(3315/4999): loss=0.28903724083084864, w0=-0.31466399999988104, gamma=0.06104439082500872\n",
      "Gradient Descent(3316/4999): loss=0.28903723787010777, w0=-0.31466399999966477, gamma=0.058397628490697136\n",
      "Gradient Descent(3317/4999): loss=0.28903723463135655, w0=-0.3146639999994705, gamma=0.044241507471292206\n",
      "Gradient Descent(3318/4999): loss=0.28903723155507477, w0=-0.31466399999933187, gamma=0.03924634233478019\n",
      "Gradient Descent(3319/4999): loss=0.2890372292271603, w0=-0.31466399999921435, gamma=0.051327591595016715\n",
      "Gradient Descent(3320/4999): loss=0.2890372271629424, w0=-0.3146639999990667, gamma=0.09769855504855192\n",
      "Gradient Descent(3321/4999): loss=0.2890372244635202, w0=-0.3146639999988001, gamma=0.10404847846247092\n",
      "Gradient Descent(3322/4999): loss=0.28903721932550985, w0=-0.31466399999854394, gamma=0.05683863942518793\n",
      "Gradient Descent(3323/4999): loss=0.28903721385359865, w0=-0.31466399999841854, gamma=0.04005872160206126\n",
      "Gradient Descent(3324/4999): loss=0.2890372108644564, w0=-0.3146639999983352, gamma=0.04773168437841398\n",
      "Gradient Descent(3325/4999): loss=0.289037208757771, w0=-0.3146639999982399, gamma=0.6133532280810757\n",
      "Gradient Descent(3326/4999): loss=0.2890372062475674, w0=-0.3146639999970734, gamma=3.636745693757745\n",
      "Gradient Descent(3327/4999): loss=0.28903717399147943, w0=-0.31466399999439915, gamma=0.05228738091561901\n",
      "Gradient Descent(3328/4999): loss=0.2890369827445263, w0=-0.3146639999945005, gamma=0.03504551231561726\n",
      "Gradient Descent(3329/4999): loss=0.28903697999116296, w0=-0.3146639999945649, gamma=0.035066634714531524\n",
      "Gradient Descent(3330/4999): loss=0.28903697814703333, w0=-0.3146639999946271, gamma=0.27825190771190234\n",
      "Gradient Descent(3331/4999): loss=0.28903697630298464, w0=-0.31466399999510325, gamma=0.30743746822598983\n",
      "Gradient Descent(3332/4999): loss=0.28903696167062753, w0=-0.31466399999548295, gamma=0.7869725820567616\n",
      "Gradient Descent(3333/4999): loss=0.28903694550358455, w0=-0.314663999996156, gamma=1.2236087317201323\n",
      "Gradient Descent(3334/4999): loss=0.2890369041197506, w0=-0.314663999996379, gamma=0.21936753892226926\n",
      "Gradient Descent(3335/4999): loss=0.2890368397761355, w0=-0.3146639999963701, gamma=0.11448567396381636\n",
      "Gradient Descent(3336/4999): loss=0.28903682824051236, w0=-0.3146639999963665, gamma=0.0673202054885187\n",
      "Gradient Descent(3337/4999): loss=0.2890368222201718, w0=-0.31466399999636463, gamma=0.03827677783461668\n",
      "Gradient Descent(3338/4999): loss=0.28903681868012865, w0=-0.31466399999636363, gamma=0.03539966217144835\n",
      "Gradient Descent(3339/4999): loss=0.28903681666732906, w0=-0.31466399999636274, gamma=0.06580504134062203\n",
      "Gradient Descent(3340/4999): loss=0.28903681480585486, w0=-0.31466399999636113, gamma=0.9557030051773058\n",
      "Gradient Descent(3341/4999): loss=0.2890368113455314, w0=-0.31466399999633937, gamma=5.058085944277116\n",
      "Gradient Descent(3342/4999): loss=0.2890367610905994, w0=-0.3146639999963351, gamma=0.22617219868647243\n",
      "Gradient Descent(3343/4999): loss=0.2890364951261496, w0=-0.31466399999633593, gamma=0.0504388747385104\n",
      "Gradient Descent(3344/4999): loss=0.2890364832435621, w0=-0.3146639999963361, gamma=0.03624205138668823\n",
      "Gradient Descent(3345/4999): loss=0.2890364805804341, w0=-0.3146639999963362, gamma=0.03596547654236659\n",
      "Gradient Descent(3346/4999): loss=0.2890364786725199, w0=-0.3146639999963363, gamma=0.07828406261344867\n",
      "Gradient Descent(3347/4999): loss=0.2890364767813318, w0=-0.3146639999963365, gamma=0.09293213285888596\n",
      "Gradient Descent(3348/4999): loss=0.28903647266509147, w0=-0.3146639999963367, gamma=0.21652163830537002\n",
      "Gradient Descent(3349/4999): loss=0.2890364677787298, w0=-0.31466399999633715, gamma=0.3862914860508707\n",
      "Gradient Descent(3350/4999): loss=0.2890364563940889, w0=-0.31466399999633776, gamma=0.07785219161621222\n",
      "Gradient Descent(3351/4999): loss=0.2890364360830973, w0=-0.3146639999963378, gamma=0.03547513208184734\n",
      "Gradient Descent(3352/4999): loss=0.2890364319896948, w0=-0.3146639999963379, gamma=0.03531528439069333\n",
      "Gradient Descent(3353/4999): loss=0.28903643012441227, w0=-0.31466399999633793, gamma=1.631002858291611\n",
      "Gradient Descent(3354/4999): loss=0.28903642826756293, w0=-0.31466399999633915, gamma=27.425015769340114\n",
      "Gradient Descent(3355/4999): loss=0.2890363425112587, w0=-0.3146639999963281, gamma=0.4895411836571041\n",
      "Gradient Descent(3356/4999): loss=0.28903490075802174, w0=-0.31466399999633404, gamma=0.10922130538213962\n",
      "Gradient Descent(3357/4999): loss=0.2890348753175359, w0=-0.3146639999963347, gamma=0.042388911605722106\n",
      "Gradient Descent(3358/4999): loss=0.2890348695284565, w0=-0.31466399999633493, gamma=0.03514227353896263\n",
      "Gradient Descent(3359/4999): loss=0.2890348670236832, w0=-0.3146639999963351, gamma=0.036398154775091805\n",
      "Gradient Descent(3360/4999): loss=0.289034865159063, w0=-0.31466399999633526, gamma=0.1709435715708256\n",
      "Gradient Descent(3361/4999): loss=0.28903486324255484, w0=-0.3146639999963361, gamma=0.17260457472545035\n",
      "Gradient Descent(3362/4999): loss=0.2890348542523475, w0=-0.3146639999963368, gamma=0.10087476465824691\n",
      "Gradient Descent(3363/4999): loss=0.2890348451806661, w0=-0.31466399999633715, gamma=0.0803169902702402\n",
      "Gradient Descent(3364/4999): loss=0.2890348398789362, w0=-0.3146639999963374, gamma=0.06742012282299073\n",
      "Gradient Descent(3365/4999): loss=0.28903483565768456, w0=-0.3146639999963376, gamma=0.06377714126574371\n",
      "Gradient Descent(3366/4999): loss=0.28903483211426306, w0=-0.31466399999633776, gamma=0.07056843352425077\n",
      "Gradient Descent(3367/4999): loss=0.28903482876231007, w0=-0.31466399999633793, gamma=0.088095045760684\n",
      "Gradient Descent(3368/4999): loss=0.2890348250534289, w0=-0.31466399999633815, gamma=0.08661599968932279\n",
      "Gradient Descent(3369/4999): loss=0.28903482042340234, w0=-0.3146639999963383, gamma=0.057515790981756236\n",
      "Gradient Descent(3370/4999): loss=0.289034815871116, w0=-0.31466399999633843, gamma=0.045401684557215806\n",
      "Gradient Descent(3371/4999): loss=0.28903481284825505, w0=-0.3146639999963385, gamma=0.06020594881774441\n",
      "Gradient Descent(3372/4999): loss=0.2890348104620777, w0=-0.3146639999963386, gamma=0.32233521119873704\n",
      "Gradient Descent(3373/4999): loss=0.2890348072978347, w0=-0.31466399999633904, gamma=0.5913973126335569\n",
      "Gradient Descent(3374/4999): loss=0.28903479035689966, w0=-0.31466399999633976, gamma=0.04330386066066314\n",
      "Gradient Descent(3375/4999): loss=0.28903475927529754, w0=-0.31466399999633976, gamma=0.035064736937710984\n",
      "Gradient Descent(3376/4999): loss=0.28903475699918485, w0=-0.31466399999633976, gamma=0.03562012818802745\n",
      "Gradient Descent(3377/4999): loss=0.28903475515630045, w0=-0.31466399999633976, gamma=659.3207027598761\n",
      "Gradient Descent(3378/4999): loss=0.2890347532842409, w0=-0.31466399999677497, gamma=36645.96548391271\n",
      "Gradient Descent(3379/4999): loss=0.289000180135585, w0=-0.3146639851816752, gamma=0.04191739395377317\n",
      "Gradient Descent(3380/4999): loss=0.28860117533681623, w0=-0.3146639858027366, gamma=0.0350416633808117\n",
      "Gradient Descent(3381/4999): loss=0.28738607455542664, w0=-0.3146639863001644, gamma=0.03513623624265481\n",
      "Gradient Descent(3382/4999): loss=0.2873356107181659, w0=-0.3146639867814566, gamma=0.1761320069770106\n",
      "Gradient Descent(3383/4999): loss=0.2873343994418145, w0=-0.3146639891093218, gamma=0.18809086362925434\n",
      "Gradient Descent(3384/4999): loss=0.2873316968781, w0=-0.3146639911573921, gamma=0.1320042891040315\n",
      "Gradient Descent(3385/4999): loss=0.2873312441716604, w0=-0.3146639923243965, gamma=0.05935967041677252\n",
      "Gradient Descent(3386/4999): loss=0.28733105717766855, w0=-0.3146639927799018, gamma=0.05481017746207305\n",
      "Gradient Descent(3387/4999): loss=0.28733094326101666, w0=-0.31466399317552957, gamma=0.14243492892417126\n",
      "Gradient Descent(3388/4999): loss=0.2873308721186409, w0=-0.31466399414729407, gamma=1.8023856115651882\n",
      "Gradient Descent(3389/4999): loss=0.2873306985990761, w0=-0.314664004692601, gamma=2.3074065148472047\n",
      "Gradient Descent(3390/4999): loss=0.2873293201566473, w0=-0.3146639938603128, gamma=0.050005729031863164\n",
      "Gradient Descent(3391/4999): loss=0.2873316004309379, w0=-0.3146639941672335, gamma=0.03860951117840259\n",
      "Gradient Descent(3392/4999): loss=0.287329486639132, w0=-0.31466399439235737, gamma=0.03580512651265149\n",
      "Gradient Descent(3393/4999): loss=0.28732919446610095, w0=-0.3146639945930689, gamma=0.06887077168145035\n",
      "Gradient Descent(3394/4999): loss=0.28732914809133814, w0=-0.3146639949653121, gamma=0.21809205287496697\n",
      "Gradient Descent(3395/4999): loss=0.28732909119828737, w0=-0.314663996062906, gamma=0.2401950106210992\n",
      "Gradient Descent(3396/4999): loss=0.287329024679463, w0=-0.314663997008101, gamma=0.061761094367145795\n",
      "Gradient Descent(3397/4999): loss=0.28732901386110205, w0=-0.3146639971927617, gamma=0.037656115999378834\n",
      "Gradient Descent(3398/4999): loss=0.2873290098602511, w0=-0.3146639972983969, gamma=0.03542153518642641\n",
      "Gradient Descent(3399/4999): loss=0.28732900746938717, w0=-0.31466399739402173, gamma=0.0545881606997614\n",
      "Gradient Descent(3400/4999): loss=0.2873290057848835, w0=-0.31466399753616925, gamma=0.13101627955769646\n",
      "Gradient Descent(3401/4999): loss=0.28732900321161664, w0=-0.3146639978587119, gamma=0.23289176344945656\n",
      "Gradient Descent(3402/4999): loss=0.28732899706684034, w0=-0.31466399835693937, gamma=1.0829502782103522\n",
      "Gradient Descent(3403/4999): loss=0.28732898619229336, w0=-0.3146640001341491, gamma=0.2852879550482406\n",
      "Gradient Descent(3404/4999): loss=0.2873289361695117, w0=-0.3146640000953133, gamma=0.037667533314278266\n",
      "Gradient Descent(3405/4999): loss=0.2873289245515662, w0=-0.3146640000916485, gamma=0.035206892877779936\n",
      "Gradient Descent(3406/4999): loss=0.28732892143167305, w0=-0.31466400008835216, gamma=0.045067750087937467\n",
      "Gradient Descent(3407/4999): loss=0.2873289198033789, w0=-0.31466400008428114, gamma=0.10789709694995456\n",
      "Gradient Descent(3408/4999): loss=0.2873289177435776, w0=-0.31466400007497386, gamma=0.12241667167608322\n",
      "Gradient Descent(3409/4999): loss=0.28732891283269724, w0=-0.3146640000655535, gamma=1.9822221804599258\n",
      "Gradient Descent(3410/4999): loss=0.28732890727020216, w0=-0.3146639999316884, gamma=1.7840384713846857\n",
      "Gradient Descent(3411/4999): loss=0.2873288175432955, w0=-0.31466400005002776, gamma=0.042217869577482366\n",
      "Gradient Descent(3412/4999): loss=0.2873287432733017, w0=-0.3146640000478321, gamma=0.03842088200948979\n",
      "Gradient Descent(3413/4999): loss=0.287328735751244, w0=-0.3146640000459183, gamma=0.03885806331187397\n",
      "Gradient Descent(3414/4999): loss=0.2873287337759026, w0=-0.31466400004405715, gamma=0.05093956670212524\n",
      "Gradient Descent(3415/4999): loss=0.28732873202142745, w0=-0.3146640000417121, gamma=0.06938701181824901\n",
      "Gradient Descent(3416/4999): loss=0.28732872973723866, w0=-0.3146640000386805, gamma=0.12792678116728892\n",
      "Gradient Descent(3417/4999): loss=0.28732872662893194, w0=-0.31466400003347905, gamma=0.247435802002666\n",
      "Gradient Descent(3418/4999): loss=0.28732872090001166, w0=-0.31466400002470546, gamma=0.07934811127868809\n",
      "Gradient Descent(3419/4999): loss=0.2873287098234239, w0=-0.3146640000225881, gamma=0.03627935357916981\n",
      "Gradient Descent(3420/4999): loss=0.28732870627301754, w0=-0.3146640000216968, gamma=0.035784480958385385\n",
      "Gradient Descent(3421/4999): loss=0.2873287046484567, w0=-0.3146640000208496, gamma=0.6114318226318074\n",
      "Gradient Descent(3422/4999): loss=0.2873287030474097, w0=-0.3146640000068915, gamma=70.27657751404827\n",
      "Gradient Descent(3423/4999): loss=0.28732867570077103, w0=-0.31466399938352063, gamma=1.689380134986969\n",
      "Gradient Descent(3424/4999): loss=0.2873256459108338, w0=-0.31466400042166115, gamma=0.04355775586844771\n",
      "Gradient Descent(3425/4999): loss=0.2873292940844858, w0=-0.31466400040320885, gamma=0.03923448357965314\n",
      "Gradient Descent(3426/4999): loss=0.287325791769418, w0=-0.3146640003873119, gamma=0.03868222512645625\n",
      "Gradient Descent(3427/4999): loss=0.2873255970224713, w0=-0.31466400037225367, gamma=0.05598779528798543\n",
      "Gradient Descent(3428/4999): loss=0.2873255782937377, w0=-0.31466400035130176, gamma=0.08811954941364857\n",
      "Gradient Descent(3429/4999): loss=0.2873255699111264, w0=-0.3146640003201717, gamma=0.09661358389116248\n",
      "Gradient Descent(3430/4999): loss=0.28732556495459793, w0=-0.31466400028904856, gamma=0.04081500922019328\n",
      "Gradient Descent(3431/4999): loss=0.2873255611437381, w0=-0.3146640002771707, gamma=0.03509400474241373\n",
      "Gradient Descent(3432/4999): loss=0.2873255594942548, w0=-0.31466400026737457, gamma=0.036265963733365617\n",
      "Gradient Descent(3433/4999): loss=0.287325558110353, w0=-0.31466400025760655, gamma=0.6165933625304261\n",
      "Gradient Descent(3434/4999): loss=0.28732555668159226, w0=-0.31466400009755396, gamma=0.9773232707943385\n",
      "Gradient Descent(3435/4999): loss=0.2873255324021713, w0=-0.314664000000288, gamma=0.15788922261844043\n",
      "Gradient Descent(3436/4999): loss=0.2873254939893236, w0=-0.31466399999993167, gamma=0.06972824178476247\n",
      "Gradient Descent(3437/4999): loss=0.2873254877887222, w0=-0.31466399999979916, gamma=0.05535279027234517\n",
      "Gradient Descent(3438/4999): loss=0.2873254850325176, w0=-0.3146639999997013, gamma=0.05650306664122838\n",
      "Gradient Descent(3439/4999): loss=0.28732548285728465, w0=-0.3146639999996069, gamma=0.1489636172555402\n",
      "Gradient Descent(3440/4999): loss=0.28732548063876284, w0=-0.3146639999993722, gamma=0.32974270802243044\n",
      "Gradient Descent(3441/4999): loss=0.2873254747907709, w0=-0.3146639999989301, gamma=0.07463481618620019\n",
      "Gradient Descent(3442/4999): loss=0.2873254618502852, w0=-0.314663999998863, gamma=0.03560534449470531\n",
      "Gradient Descent(3443/4999): loss=0.2873254589223592, w0=-0.31466399999883343, gamma=0.035445193224319264\n",
      "Gradient Descent(3444/4999): loss=0.28732545752425004, w0=-0.314663999998805, gamma=1.396240889720489\n",
      "Gradient Descent(3445/4999): loss=0.2873254561337838, w0=-0.31466399999772504, gamma=17.986650641900365\n",
      "Gradient Descent(3446/4999): loss=0.2873254013871304, w0=-0.3146640000032389, gamma=0.39130545897999225\n",
      "Gradient Descent(3447/4999): loss=0.2873247018666531, w0=-0.3146640000012022, gamma=0.07695811683590552\n",
      "Gradient Descent(3448/4999): loss=0.2873246983251419, w0=-0.3146640000009584, gamma=0.046701906962147786\n",
      "Gradient Descent(3449/4999): loss=0.2873246864653494, w0=-0.31466400000082184, gamma=0.036184013276808154\n",
      "Gradient Descent(3450/4999): loss=0.28732468139019923, w0=-0.314664000000721, gamma=0.03648932262968199\n",
      "Gradient Descent(3451/4999): loss=0.28732467958515495, w0=-0.31466400000062295, gamma=0.09305264138651\n",
      "Gradient Descent(3452/4999): loss=0.28732467814293966, w0=-0.314664000000382, gamma=0.10249443063171709\n",
      "Gradient Descent(3453/4999): loss=0.2873246745416487, w0=-0.3146640000001414, gamma=0.10345770046669515\n",
      "Gradient Descent(3454/4999): loss=0.2873246706064651, w0=-0.31466399999992334, gamma=0.08804921926287052\n",
      "Gradient Descent(3455/4999): loss=0.2873246666351892, w0=-0.31466399999975697, gamma=0.059267098395572766\n",
      "Gradient Descent(3456/4999): loss=0.2873246632558758, w0=-0.31466399999965483, gamma=0.04658762987863929\n",
      "Gradient Descent(3457/4999): loss=0.2873246609814327, w0=-0.31466399999957934, gamma=0.05799151538639485\n",
      "Gradient Descent(3458/4999): loss=0.28732465919372846, w0=-0.31466399999948974, gamma=0.23982271992638896\n",
      "Gradient Descent(3459/4999): loss=0.287324656968604, w0=-0.3146639999991407, gamma=0.493556725759844\n",
      "Gradient Descent(3460/4999): loss=0.28732464776800737, w0=-0.31466399999859457, gamma=0.04844061232452748\n",
      "Gradient Descent(3461/4999): loss=0.28732462884754056, w0=-0.3146639999985674, gamma=0.03510389560954014\n",
      "Gradient Descent(3462/4999): loss=0.28732462698493805, w0=-0.3146639999985487, gamma=0.03551982377505723\n",
      "Gradient Descent(3463/4999): loss=0.28732462563846556, w0=-0.31466399999853045, gamma=13.549928866446255\n",
      "Gradient Descent(3464/4999): loss=0.2873246242771153, w0=-0.3146639999918058, gamma=15.661755738861624\n",
      "Gradient Descent(3465/4999): loss=0.28732410705535844, w0=-0.31466400008936946, gamma=2.6499318270312977\n",
      "Gradient Descent(3466/4999): loss=0.287323521586842, w0=-0.3146639998473494, gamma=0.03786829094333934\n",
      "Gradient Descent(3467/4999): loss=0.28732357295650335, w0=-0.3146639998530557, gamma=0.035694615940795996\n",
      "Gradient Descent(3468/4999): loss=0.2873234197976821, w0=-0.3146639998582308, gamma=0.04176887427284238\n",
      "Gradient Descent(3469/4999): loss=0.2873234158940307, w0=-0.31466399986407034, gamma=0.06397659672155678\n",
      "Gradient Descent(3470/4999): loss=0.2873234139379702, w0=-0.3146639998726411, gamma=0.11789379563162329\n",
      "Gradient Descent(3471/4999): loss=0.2873234113510482, w0=-0.31466399988742455, gamma=0.166505242485663\n",
      "Gradient Descent(3472/4999): loss=0.2873234068418468, w0=-0.3146639999058422, gamma=0.10422937437734427\n",
      "Gradient Descent(3473/4999): loss=0.2873234006303253, w0=-0.3146639999154517, gamma=0.044380573801499026\n",
      "Gradient Descent(3474/4999): loss=0.28732339675301094, w0=-0.3146639999191169, gamma=0.03830151716697849\n",
      "Gradient Descent(3475/4999): loss=0.2873233950976411, w0=-0.3146639999221397, gamma=0.039097373175928304\n",
      "Gradient Descent(3476/4999): loss=0.2873233936723382, w0=-0.31466399992510713, gamma=0.18946014840076877\n",
      "Gradient Descent(3477/4999): loss=0.28732339221771225, w0=-0.3146639999389245, gamma=0.9946654923803976\n",
      "Gradient Descent(3478/4999): loss=0.2873233851694579, w0=-0.31466399999772227, gamma=0.3883102420558986\n",
      "Gradient Descent(3479/4999): loss=0.2873233481806081, w0=-0.3146639999978448, gamma=0.04263727713318606\n",
      "Gradient Descent(3480/4999): loss=0.28732333380377, w0=-0.314663999997853, gamma=0.03839165821989885\n",
      "Gradient Descent(3481/4999): loss=0.28732333216727546, w0=-0.3146639999978601, gamma=0.03826374259771436\n",
      "Gradient Descent(3482/4999): loss=0.28732333073857536, w0=-0.3146639999978669, gamma=0.055638205351046625\n",
      "Gradient Descent(3483/4999): loss=0.28732332931694793, w0=-0.31466399999787636, gamma=0.21243930772790418\n",
      "Gradient Descent(3484/4999): loss=0.2873233272499929, w0=-0.31466399999791067, gamma=6.660287493711795\n",
      "Gradient Descent(3485/4999): loss=0.28732331935851363, w0=-0.3146639999987576, gamma=0.34270591169797865\n",
      "Gradient Descent(3486/4999): loss=0.28732307234408505, w0=-0.3146639999985112, gamma=0.03522127476132915\n",
      "Gradient Descent(3487/4999): loss=0.28732306244926037, w0=-0.31466399999849454, gamma=0.03510024429164031\n",
      "Gradient Descent(3488/4999): loss=0.287323058347143, w0=-0.31466399999847855, gamma=0.05269350001707\n",
      "Gradient Descent(3489/4999): loss=0.28732305704661704, w0=-0.31466399999845535, gamma=0.05574247686805733\n",
      "Gradient Descent(3490/4999): loss=0.28732305509959044, w0=-0.3146639999984321, gamma=5.337530038187719\n",
      "Gradient Descent(3491/4999): loss=0.2873230530405318, w0=-0.3146639999963314, gamma=48.16474464374144\n",
      "Gradient Descent(3492/4999): loss=0.2873228560841962, w0=-0.31466400007858425, gamma=0.16193153767051163\n",
      "Gradient Descent(3493/4999): loss=0.28732113243838364, w0=-0.3146640000655422, gamma=0.06144158087193165\n",
      "Gradient Descent(3494/4999): loss=0.28732112400632104, w0=-0.31466400006139494, gamma=0.03855854607004855\n",
      "Gradient Descent(3495/4999): loss=0.2873211013033292, w0=-0.31466400005895223, gamma=0.0353032087636411\n",
      "Gradient Descent(3496/4999): loss=0.28732109066482586, w0=-0.31466400005680195, gamma=0.04156553525740904\n",
      "Gradient Descent(3497/4999): loss=0.28732108914182575, w0=-0.3146640000543596, gamma=0.11177608973402468\n",
      "Gradient Descent(3498/4999): loss=0.2873210875529853, w0=-0.3146640000480649, gamma=0.1399757423541327\n",
      "Gradient Descent(3499/4999): loss=0.2873210834372169, w0=-0.31466400004106315, gamma=0.18730484568859374\n",
      "Gradient Descent(3500/4999): loss=0.2873210783895353, w0=-0.31466400003300543, gamma=0.11609757715558534\n",
      "Gradient Descent(3501/4999): loss=0.28732107167658627, w0=-0.31466400002894646, gamma=0.05664277726801953\n",
      "Gradient Descent(3502/4999): loss=0.2873210675253696, w0=-0.314664000027196, gamma=0.044583369009243576\n",
      "Gradient Descent(3503/4999): loss=0.28732106549849723, w0=-0.31466400002589634, gamma=0.04103410568599221\n",
      "Gradient Descent(3504/4999): loss=0.2873210639054145, w0=-0.3146640000247534, gamma=0.10010497492624791\n",
      "Gradient Descent(3505/4999): loss=0.2873210624397084, w0=-0.31466400002207967, gamma=0.6805928239659045\n",
      "Gradient Descent(3506/4999): loss=0.28732105886492476, w0=-0.3146640000057211, gamma=0.387835221425041\n",
      "Gradient Descent(3507/4999): loss=0.28732103458179814, w0=-0.3146640000027436, gamma=0.035699244655260684\n",
      "Gradient Descent(3508/4999): loss=0.28732102083859745, w0=-0.3146640000025758, gamma=0.03504236069745291\n",
      "Gradient Descent(3509/4999): loss=0.2873210194866147, w0=-0.314664000002417, gamma=0.049903047027392754\n",
      "Gradient Descent(3510/4999): loss=0.2873210182381321, w0=-0.3146640000021988, gamma=0.6186938654345137\n",
      "Gradient Descent(3511/4999): loss=0.28732101646032404, w0=-0.31466399999962824, gamma=2.9451347749535026\n",
      "Gradient Descent(3512/4999): loss=0.28732099442346787, w0=-0.3146639999949626, gamma=0.1800581632271672\n",
      "Gradient Descent(3513/4999): loss=0.28732088966732866, w0=-0.31466399999551753, gamma=0.07405575871856361\n",
      "Gradient Descent(3514/4999): loss=0.2873208833083226, w0=-0.31466399999570466, gamma=0.07167377695243296\n",
      "Gradient Descent(3515/4999): loss=0.2873208805973735, w0=-0.31466399999587236, gamma=0.054451234910132346\n",
      "Gradient Descent(3516/4999): loss=0.2873208780498058, w0=-0.31466399999599065, gamma=0.04902337932517732\n",
      "Gradient Descent(3517/4999): loss=0.28732087611492557, w0=-0.31466399999609135, gamma=0.06446700456040791\n",
      "Gradient Descent(3518/4999): loss=0.2873208743730412, w0=-0.31466399999621725, gamma=0.21499607654450928\n",
      "Gradient Descent(3519/4999): loss=0.2873208720824844, w0=-0.3146639999966101, gamma=0.28845326247808106\n",
      "Gradient Descent(3520/4999): loss=0.28732086444392263, w0=-0.3146639999970239, gamma=0.04463881560450453\n",
      "Gradient Descent(3521/4999): loss=0.2873208541975948, w0=-0.31466399999706945, gamma=0.03521496534114972\n",
      "Gradient Descent(3522/4999): loss=0.28732085261106105, w0=-0.3146639999971038, gamma=0.03746401794716715\n",
      "Gradient Descent(3523/4999): loss=0.2873208513601358, w0=-0.31466399999713907, gamma=62.2627093671048\n",
      "Gradient Descent(3524/4999): loss=0.28732085002942004, w0=-0.31466400005351597, gamma=218.16283845425014\n",
      "Gradient Descent(3525/4999): loss=0.28731865474825735, w0=-0.3146639879529528, gamma=0.0544952907471002\n",
      "Gradient Descent(3526/4999): loss=0.2873137130456605, w0=-0.31466398860935507, gamma=0.03825000345680049\n",
      "Gradient Descent(3527/4999): loss=0.2873118886726696, w0=-0.3146639890449736, gamma=0.036153452300478464\n",
      "Gradient Descent(3528/4999): loss=0.28731143185195374, w0=-0.31466398944096596, gamma=0.0565849791970609\n",
      "Gradient Descent(3529/4999): loss=0.28731139785079723, w0=-0.3146639900383396, gamma=0.08803888799767932\n",
      "Gradient Descent(3530/4999): loss=0.28731137582376554, w0=-0.31466399091518343, gamma=0.13331916429676482\n",
      "Gradient Descent(3531/4999): loss=0.2873113570946414, w0=-0.31466399212610663, gamma=0.3064782758145966\n",
      "Gradient Descent(3532/4999): loss=0.28731133517216484, w0=-0.3146639945386931, gamma=0.09922820477053897\n",
      "Gradient Descent(3533/4999): loss=0.2873112970773226, w0=-0.3146639950804175, gamma=0.035616787028711606\n",
      "Gradient Descent(3534/4999): loss=0.28731128988898647, w0=-0.31466399525556854, gamma=0.03516494741325081\n",
      "Gradient Descent(3535/4999): loss=0.28731128200851525, w0=-0.3146639954223384, gamma=0.38610650545246983\n",
      "Gradient Descent(3536/4999): loss=0.28731127861874634, w0=-0.31466399718905885, gamma=1.7616770289457684\n",
      "Gradient Descent(3537/4999): loss=0.2873112438279835, w0=-0.31466400213763196, gamma=1.017881610684808\n",
      "Gradient Descent(3538/4999): loss=0.28731113394573354, w0=-0.31466399995981076, gamma=0.07848897223512788\n",
      "Gradient Descent(3539/4999): loss=0.2873112535843341, w0=-0.3146639999628136, gamma=0.07058402851035309\n",
      "Gradient Descent(3540/4999): loss=0.287311095747784, w0=-0.31466399996530203, gamma=0.035095917381992775\n",
      "Gradient Descent(3541/4999): loss=0.2873110932746817, w0=-0.314663999966452, gamma=0.035042270746853274\n",
      "Gradient Descent(3542/4999): loss=0.2873110866502307, w0=-0.31466399996755995, gamma=0.22132923880256813\n",
      "Gradient Descent(3543/4999): loss=0.2873110853914232, w0=-0.31466399997431244, gamma=0.42220755873985316\n",
      "Gradient Descent(3544/4999): loss=0.28731107749929696, w0=-0.3146639999843425, gamma=0.1938656418858207\n",
      "Gradient Descent(3545/4999): loss=0.2873110626771241, w0=-0.31466399998700356, gamma=0.057986664512172124\n",
      "Gradient Descent(3546/4999): loss=0.2873110560368596, w0=-0.3146639999876452, gamma=0.05436019283572065\n",
      "Gradient Descent(3547/4999): loss=0.28731105392660866, w0=-0.3146639999882118, gamma=0.09646190167034273\n",
      "Gradient Descent(3548/4999): loss=0.28731105204948526, w0=-0.3146639999891626, gamma=1.184874361754825\n",
      "Gradient Descent(3549/4999): loss=0.28731104872544294, w0=-0.3146639999997151, gamma=2.6507224931443494\n",
      "Gradient Descent(3550/4999): loss=0.28731100821953476, w0=-0.3146639999953498, gamma=0.08637149997856855\n",
      "Gradient Descent(3551/4999): loss=0.28731092010457826, w0=-0.3146639999955846, gamma=0.04096036843722273\n",
      "Gradient Descent(3552/4999): loss=0.28731091705993916, w0=-0.31466399999568634, gamma=0.035358017975906306\n",
      "Gradient Descent(3553/4999): loss=0.2873109151032972, w0=-0.31466399999577055, gamma=0.036195116015241\n",
      "Gradient Descent(3554/4999): loss=0.28731091390469626, w0=-0.3146639999958537, gamma=0.0871889598073024\n",
      "Gradient Descent(3555/4999): loss=0.28731091269719844, w0=-0.3146639999960468, gamma=0.11394617780546305\n",
      "Gradient Descent(3556/4999): loss=0.28731090979087326, w0=-0.31466399999627714, gamma=0.124581363537499\n",
      "Gradient Descent(3557/4999): loss=0.2873109059941602, w0=-0.3146639999965003, gamma=0.0945275927134459\n",
      "Gradient Descent(3558/4999): loss=0.2873109018434284, w0=-0.3146639999966485, gamma=0.07112098944982773\n",
      "Gradient Descent(3559/4999): loss=0.2873108986941202, w0=-0.3146639999967495, gamma=0.07626543390238315\n",
      "Gradient Descent(3560/4999): loss=0.2873108963246839, w0=-0.3146639999968501, gamma=0.1508374743381691\n",
      "Gradient Descent(3561/4999): loss=0.2873108937839087, w0=-0.3146639999970338, gamma=0.15931182427676496\n",
      "Gradient Descent(3562/4999): loss=0.28731088875888, w0=-0.31466399999719863, gamma=0.046843193322200966\n",
      "Gradient Descent(3563/4999): loss=0.28731088345174527, w0=-0.3146639999972394, gamma=0.036047590321466066\n",
      "Gradient Descent(3564/4999): loss=0.2873108818912174, w0=-0.31466399999726924, gamma=0.042098302870271755\n",
      "Gradient Descent(3565/4999): loss=0.287310880690381, w0=-0.3146639999973029, gamma=2.3361558162654346\n",
      "Gradient Descent(3566/4999): loss=0.2873108792879971, w0=-0.3146639999990909, gamma=3.505630086999397\n",
      "Gradient Descent(3567/4999): loss=0.28731080147263754, w0=-0.3146639999955065, gamma=0.05872611826352446\n",
      "Gradient Descent(3568/4999): loss=0.2873106847407628, w0=-0.3146639999956569, gamma=0.03577719383598986\n",
      "Gradient Descent(3569/4999): loss=0.2873106827769248, w0=-0.3146639999957432, gamma=0.03512331173164495\n",
      "Gradient Descent(3570/4999): loss=0.2873106815801245, w0=-0.31466399999582484, gamma=0.1346289447844289\n",
      "Gradient Descent(3571/4999): loss=0.2873106804102054, w0=-0.31466399999612693, gamma=0.46915307103001563\n",
      "Gradient Descent(3572/4999): loss=0.2873106759261206, w0=-0.3146639999970378, gamma=0.4047344505159634\n",
      "Gradient Descent(3573/4999): loss=0.28731066030206215, w0=-0.31466399999745504, gamma=0.055267526221979904\n",
      "Gradient Descent(3574/4999): loss=0.28731064682534, w0=-0.31466399999748895, gamma=0.053521570743124774\n",
      "Gradient Descent(3575/4999): loss=0.2873106449838409, w0=-0.31466399999752, gamma=0.04744304466775778\n",
      "Gradient Descent(3576/4999): loss=0.287310643201569, w0=-0.314663999997546, gamma=0.0694280821055288\n",
      "Gradient Descent(3577/4999): loss=0.2873106416217154, w0=-0.3146639999975823, gamma=0.29095183073548936\n",
      "Gradient Descent(3578/4999): loss=0.28731063930976247, w0=-0.31466399999772376, gamma=0.29887652058570163\n",
      "Gradient Descent(3579/4999): loss=0.2873106296210999, w0=-0.31466399999782685, gamma=0.03992535860778603\n",
      "Gradient Descent(3580/4999): loss=0.28731061966866883, w0=-0.3146639999978365, gamma=0.03511700986083597\n",
      "Gradient Descent(3581/4999): loss=0.2873106183390996, w0=-0.31466399999784467, gamma=0.039387016287112726\n",
      "Gradient Descent(3582/4999): loss=0.28731061716971745, w0=-0.3146639999978535, gamma=76.05691780571154\n",
      "Gradient Descent(3583/4999): loss=0.2873106158581482, w0=-0.3146640000142242, gamma=90.95564665217205\n",
      "Gradient Descent(3584/4999): loss=0.28730808403473024, w0=-0.3146639985451495, gamma=0.3629049213659672\n",
      "Gradient Descent(3585/4999): loss=0.28730512874061453, w0=-0.3146639990724238, gamma=0.03599802787874642\n",
      "Gradient Descent(3586/4999): loss=0.28730543134011305, w0=-0.31466399910574544, gamma=0.035039525042561394\n",
      "Gradient Descent(3587/4999): loss=0.287305067051248, w0=-0.3146639991370123, gamma=0.039722901680384476\n",
      "Gradient Descent(3588/4999): loss=0.28730506404966677, w0=-0.3146639991712163, gamma=0.3702740096307989\n",
      "Gradient Descent(3589/4999): loss=0.2873050611434198, w0=-0.3146639994773812, gamma=0.6520820989436146\n",
      "Gradient Descent(3590/4999): loss=0.28730503818009884, w0=-0.31466399981691767, gamma=0.11762283663513487\n",
      "Gradient Descent(3591/4999): loss=0.28730501447564016, w0=-0.3146639998382262, gamma=0.0662214746657298\n",
      "Gradient Descent(3592/4999): loss=0.287305008971619, w0=-0.31466399984881177, gamma=0.05704626971427754\n",
      "Gradient Descent(3593/4999): loss=0.2873050051815711, w0=-0.3146639998573268, gamma=0.05619103808989757\n",
      "Gradient Descent(3594/4999): loss=0.2873050031646066, w0=-0.31466399986523574, gamma=0.12666248737682248\n",
      "Gradient Descent(3595/4999): loss=0.2873050012521662, w0=-0.3146639998820618, gamma=0.48064585861584247\n",
      "Gradient Descent(3596/4999): loss=0.28730499695427647, w0=-0.31466399993782423, gamma=0.24119956934984996\n",
      "Gradient Descent(3597/4999): loss=0.2873049807381527, w0=-0.31466399995235733, gamma=0.03643589974641599\n",
      "Gradient Descent(3598/4999): loss=0.2873049728070913, w0=-0.31466399995402317, gamma=0.03515928109883475\n",
      "Gradient Descent(3599/4999): loss=0.28730497142393513, w0=-0.3146639999555721, gamma=0.05250809288929386\n",
      "Gradient Descent(3600/4999): loss=0.28730497024542556, w0=-0.314663999957804, gamma=0.1319345246876869\n",
      "Gradient Descent(3601/4999): loss=0.2873049684870804, w0=-0.31466399996311745, gamma=0.4756498397277798\n",
      "Gradient Descent(3602/4999): loss=0.2873049640713538, w0=-0.3146639999797462, gamma=0.7618846715462302\n",
      "Gradient Descent(3603/4999): loss=0.2873049481679139, w0=-0.3146639999937126, gamma=0.10400912313810873\n",
      "Gradient Descent(3604/4999): loss=0.28730492277209785, w0=-0.3146639999941666, gamma=0.07335025187795459\n",
      "Gradient Descent(3605/4999): loss=0.28730491927845264, w0=-0.3146639999944535, gamma=0.04119872107205436\n",
      "Gradient Descent(3606/4999): loss=0.28730491683153636, w0=-0.3146639999946028, gamma=0.03536317514289135\n",
      "Gradient Descent(3607/4999): loss=0.2873049154568134, w0=-0.31466399999472566, gamma=0.043732620931117\n",
      "Gradient Descent(3608/4999): loss=0.28730491427853255, w0=-0.31466399999487227, gamma=1.995268625754429\n",
      "Gradient Descent(3609/4999): loss=0.2873049128215494, w0=-0.3146640000012676, gamma=3.3828500364937844\n",
      "Gradient Descent(3610/4999): loss=0.2873048463992829, w0=-0.3146639999904766, gamma=0.08361802637904574\n",
      "Gradient Descent(3611/4999): loss=0.28730473427234404, w0=-0.3146639999911122, gamma=0.06485721005611736\n",
      "Gradient Descent(3612/4999): loss=0.2873047312292779, w0=-0.31466399999156397, gamma=0.03615082175978897\n",
      "Gradient Descent(3613/4999): loss=0.2873047290652671, w0=-0.31466399999179945, gamma=0.03510408113646051\n",
      "Gradient Descent(3614/4999): loss=0.28730472785040617, w0=-0.3146639999920198, gamma=0.0424369287973374\n",
      "Gradient Descent(3615/4999): loss=0.2873047266843201, w0=-0.3146639999922769, gamma=0.08333231766896387\n",
      "Gradient Descent(3616/4999): loss=0.2873047252746941, w0=-0.3146639999927603, gamma=0.213452455953551\n",
      "Gradient Descent(3617/4999): loss=0.287304722506663, w0=-0.3146639999938953, gamma=0.5456747771606559\n",
      "Gradient Descent(3618/4999): loss=0.2873047154164959, w0=-0.31466399999617756, gamma=0.171375485873748\n",
      "Gradient Descent(3619/4999): loss=0.2873046972911999, w0=-0.3146639999965032, gamma=0.05468722918390979\n",
      "Gradient Descent(3620/4999): loss=0.28730469159887195, w0=-0.3146639999965893, gamma=0.053578097333078945\n",
      "Gradient Descent(3621/4999): loss=0.2873046897822806, w0=-0.31466399999666905, gamma=0.07827840025329039\n",
      "Gradient Descent(3622/4999): loss=0.2873046880026358, w0=-0.31466399999677935, gamma=0.19259097867423858\n",
      "Gradient Descent(3623/4999): loss=0.2873046854025512, w0=-0.31466399999702943, gamma=0.1376682627275673\n",
      "Gradient Descent(3624/4999): loss=0.2873046790054882, w0=-0.3146639999971737, gamma=0.039972090104411606\n",
      "Gradient Descent(3625/4999): loss=0.2873046744327482, w0=-0.31466399999720984, gamma=0.03559467308427607\n",
      "Gradient Descent(3626/4999): loss=0.2873046731050396, w0=-0.31466399999724076, gamma=0.06309673513039804\n",
      "Gradient Descent(3627/4999): loss=0.2873046719227402, w0=-0.31466399999729355, gamma=11.915221138778184\n",
      "Gradient Descent(3628/4999): loss=0.2873046698269439, w0=-0.3146640000066373, gamma=12.81087789664811\n",
      "Gradient Descent(3629/4999): loss=0.2873042740741015, w0=-0.3146639998969894, gamma=0.03907969722629097\n",
      "Gradient Descent(3630/4999): loss=0.28730384915797413, w0=-0.31466399990093996, gamma=0.035034139807149846\n",
      "Gradient Descent(3631/4999): loss=0.2873038473643163, w0=-0.31466399990434313, gamma=0.03518189984057068\n",
      "Gradient Descent(3632/4999): loss=0.28730384619268146, w0=-0.31466399990764093, gamma=0.8863319642776095\n",
      "Gradient Descent(3633/4999): loss=0.28730384502293327, w0=-0.3146639999877991, gamma=0.8947026945199378\n",
      "Gradient Descent(3634/4999): loss=0.28730381557192114, w0=-0.3146639999969966, gamma=0.23011768034073476\n",
      "Gradient Descent(3635/4999): loss=0.2873037858608522, w0=-0.3146639999972457, gamma=0.08562758405056989\n",
      "Gradient Descent(3636/4999): loss=0.2873037782196068, w0=-0.3146639999973171, gamma=0.07498932895314438\n",
      "Gradient Descent(3637/4999): loss=0.28730377537538904, w0=-0.31466399999737427, gamma=0.05772334275999427\n",
      "Gradient Descent(3638/4999): loss=0.2873037728852214, w0=-0.31466399999741496, gamma=0.0674756956904807\n",
      "Gradient Descent(3639/4999): loss=0.28730377096842225, w0=-0.31466399999745975, gamma=0.8216618343917136\n",
      "Gradient Descent(3640/4999): loss=0.287303768727788, w0=-0.3146639999979687, gamma=1.3465003098691828\n",
      "Gradient Descent(3641/4999): loss=0.2873037414434653, w0=-0.31466399999811745, gamma=0.03886396604051308\n",
      "Gradient Descent(3642/4999): loss=0.2873036967361417, w0=-0.31466399999811595, gamma=0.03523006678042199\n",
      "Gradient Descent(3643/4999): loss=0.2873036954419133, w0=-0.3146639999981147, gamma=0.03658256687408742\n",
      "Gradient Descent(3644/4999): loss=0.2873036942720486, w0=-0.3146639999981134, gamma=0.09055537769055548\n",
      "Gradient Descent(3645/4999): loss=0.2873036930573284, w0=-0.3146639999981103, gamma=0.23230308857065907\n",
      "Gradient Descent(3646/4999): loss=0.28730369005045575, w0=-0.31466399999810307, gamma=0.3309781020708836\n",
      "Gradient Descent(3647/4999): loss=0.2873036823369088, w0=-0.3146639999980952, gamma=0.08342348487985667\n",
      "Gradient Descent(3648/4999): loss=0.2873036713469577, w0=-0.31466399999809386, gamma=0.05463495896868545\n",
      "Gradient Descent(3649/4999): loss=0.2873036685769202, w0=-0.3146639999980931, gamma=0.0566118866464349\n",
      "Gradient Descent(3650/4999): loss=0.2873036667627967, w0=-0.3146639999980923, gamma=0.5349245476065703\n",
      "Gradient Descent(3651/4999): loss=0.28730366488303744, w0=-0.31466399999808536, gamma=1.351045134191068\n",
      "Gradient Descent(3652/4999): loss=0.2873036471212758, w0=-0.3146639999980772, gamma=0.04163400087058086\n",
      "Gradient Descent(3653/4999): loss=0.28730360226232105, w0=-0.3146639999980773, gamma=0.035037682360413346\n",
      "Gradient Descent(3654/4999): loss=0.2873036008789452, w0=-0.31466399999807737, gamma=0.03515425596932927\n",
      "Gradient Descent(3655/4999): loss=0.287303599715537, w0=-0.3146639999980774, gamma=4.975079333374322\n",
      "Gradient Descent(3656/4999): loss=0.28730359854829474, w0=-0.3146639999980875, gamma=12.672991510130165\n",
      "Gradient Descent(3657/4999): loss=0.2873034333610896, w0=-0.31466399999798766, gamma=0.08846912155879431\n",
      "Gradient Descent(3658/4999): loss=0.2873030128064146, w0=-0.3146639999979958, gamma=0.057954010840378996\n",
      "Gradient Descent(3659/4999): loss=0.28730300973513107, w0=-0.3146639999980007, gamma=0.05559772501647269\n",
      "Gradient Descent(3660/4999): loss=0.28730300776103523, w0=-0.31466399999800515, gamma=0.11292712031180824\n",
      "Gradient Descent(3661/4999): loss=0.2873030059103071, w0=-0.31466399999801364, gamma=0.1662592697537419\n",
      "Gradient Descent(3662/4999): loss=0.2873030021579837, w0=-0.31466399999802475, gamma=0.1751346236833429\n",
      "Gradient Descent(3663/4999): loss=0.28730299663817715, w0=-0.31466399999803446, gamma=0.059770330315569956\n",
      "Gradient Descent(3664/4999): loss=0.287302990824259, w0=-0.3146639999980372, gamma=0.03888972533481823\n",
      "Gradient Descent(3665/4999): loss=0.28730298884006267, w0=-0.31466399999803885, gamma=0.03602236014695161\n",
      "Gradient Descent(3666/4999): loss=0.2873029875490452, w0=-0.31466399999804034, gamma=0.09801921393736121\n",
      "Gradient Descent(3667/4999): loss=0.2873029863532215, w0=-0.31466399999804423, gamma=1.4417423750621783\n",
      "Gradient Descent(3668/4999): loss=0.2873029830993086, w0=-0.3146639999980962, gamma=2.789591306434659\n",
      "Gradient Descent(3669/4999): loss=0.2873029352384894, w0=-0.31466399999805184, gamma=0.060734365664230024\n",
      "Gradient Descent(3670/4999): loss=0.28730284263945266, w0=-0.31466399999805356, gamma=0.03816343755049181\n",
      "Gradient Descent(3671/4999): loss=0.2873028406209853, w0=-0.31466399999805456, gamma=0.035433845261565976\n",
      "Gradient Descent(3672/4999): loss=0.28730283935301903, w0=-0.3146639999980555, gamma=0.058740711194745984\n",
      "Gradient Descent(3673/4999): loss=0.2873028381766847, w0=-0.31466399999805694, gamma=0.25075751670031915\n",
      "Gradient Descent(3674/4999): loss=0.2873028362266696, w0=-0.3146639999980628, gamma=0.2719444322125056\n",
      "Gradient Descent(3675/4999): loss=0.28730282790259437, w0=-0.31466399999806766, gamma=0.19280455323855683\n",
      "Gradient Descent(3676/4999): loss=0.287302818875416, w0=-0.3146639999980701, gamma=0.04176904761832392\n",
      "Gradient Descent(3677/4999): loss=0.2873028124753158, w0=-0.31466399999807054, gamma=0.03611193716700761\n",
      "Gradient Descent(3678/4999): loss=0.2873028110887823, w0=-0.31466399999807093, gamma=0.04219243826415209\n",
      "Gradient Descent(3679/4999): loss=0.2873028098900537, w0=-0.3146639999980713, gamma=0.7696500058850765\n",
      "Gradient Descent(3680/4999): loss=0.28730280848948553, w0=-0.31466399999807837, gamma=10.097893941426417\n",
      "Gradient Descent(3681/4999): loss=0.2873027829411954, w0=-0.3146639999981003, gamma=0.12455204554113539\n",
      "Gradient Descent(3682/4999): loss=0.28730244776335645, w0=-0.3146639999980979, gamma=0.04863595302755198\n",
      "Gradient Descent(3683/4999): loss=0.2873024436357035, w0=-0.3146639999980971, gamma=0.040176957183226863\n",
      "Gradient Descent(3684/4999): loss=0.2873024420087659, w0=-0.3146639999980964, gamma=0.03589129161652228\n",
      "Gradient Descent(3685/4999): loss=0.28730244067426586, w0=-0.31466399999809586, gamma=0.04089129582859498\n",
      "Gradient Descent(3686/4999): loss=0.2873024394829637, w0=-0.31466399999809525, gamma=0.0793881399058646\n",
      "Gradient Descent(3687/4999): loss=0.28730243812574285, w0=-0.31466399999809413, gamma=0.10042977501312203\n",
      "Gradient Descent(3688/4999): loss=0.2873024354907877, w0=-0.3146639999980928, gamma=1.5842094781115066\n",
      "Gradient Descent(3689/4999): loss=0.28730243215744977, w0=-0.31466399999807415, gamma=1.3168736371338001\n",
      "Gradient Descent(3690/4999): loss=0.2873023795766461, w0=-0.3146639999980834, gamma=0.04011006346400044\n",
      "Gradient Descent(3691/4999): loss=0.2873023358761259, w0=-0.3146639999980833, gamma=0.03698634888190387\n",
      "Gradient Descent(3692/4999): loss=0.2873023345385571, w0=-0.31466399999808325, gamma=0.04010553565165987\n",
      "Gradient Descent(3693/4999): loss=0.28730233331077915, w0=-0.3146639999980832, gamma=0.05529440187710512\n",
      "Gradient Descent(3694/4999): loss=0.2873023319796762, w0=-0.3146639999980831, gamma=0.08035601137237827\n",
      "Gradient Descent(3695/4999): loss=0.2873023301444729, w0=-0.3146639999980829, gamma=0.365891055005097\n",
      "Gradient Descent(3696/4999): loss=0.2873023274774874, w0=-0.3146639999980823, gamma=0.6778843772145686\n",
      "Gradient Descent(3697/4999): loss=0.2873023153337263, w0=-0.3146639999980816, gamma=0.048910174382028326\n",
      "Gradient Descent(3698/4999): loss=0.28730229283526315, w0=-0.3146639999980816, gamma=0.03518269225860104\n",
      "Gradient Descent(3699/4999): loss=0.2873022912118796, w0=-0.3146639999980816, gamma=0.03546916204984589\n",
      "Gradient Descent(3700/4999): loss=0.2873022900441766, w0=-0.3146639999980816, gamma=0.35959667473095075\n",
      "Gradient Descent(3701/4999): loss=0.2873022888669834, w0=-0.31466399999808153, gamma=0.5253616803266694\n",
      "Gradient Descent(3702/4999): loss=0.287302276932278, w0=-0.3146639999980813, gamma=0.3233033054754239\n",
      "Gradient Descent(3703/4999): loss=0.2873022594960563, w0=-0.31466399999808126, gamma=0.116410868181444\n",
      "Gradient Descent(3704/4999): loss=0.2873022487660122, w0=-0.31466399999808126, gamma=0.06642642760176368\n",
      "Gradient Descent(3705/4999): loss=0.28730224490246875, w0=-0.31466399999808126, gamma=0.05588895119121998\n",
      "Gradient Descent(3706/4999): loss=0.2873022426978509, w0=-0.31466399999808126, gamma=0.05275057440368813\n",
      "Gradient Descent(3707/4999): loss=0.28730224084296974, w0=-0.31466399999808126, gamma=0.08098230802942452\n",
      "Gradient Descent(3708/4999): loss=0.28730223909224867, w0=-0.31466399999808126, gamma=0.21045592278982614\n",
      "Gradient Descent(3709/4999): loss=0.28730223640455627, w0=-0.31466399999808126, gamma=0.1377200104063674\n",
      "Gradient Descent(3710/4999): loss=0.2873022294198201, w0=-0.3146639999980813, gamma=0.03912662367734939\n",
      "Gradient Descent(3711/4999): loss=0.28730222484910245, w0=-0.3146639999980813, gamma=0.035508271292515145\n",
      "Gradient Descent(3712/4999): loss=0.2873022235505411, w0=-0.3146639999980813, gamma=0.06989583022411448\n",
      "Gradient Descent(3713/4999): loss=0.2873022223720754, w0=-0.3146639999980813, gamma=177.6579256255523\n",
      "Gradient Descent(3714/4999): loss=0.28730222005234035, w0=-0.31466399999808964, gamma=179.773499139869\n",
      "Gradient Descent(3715/4999): loss=0.2872963266539053, w0=-0.3146639999981274, gamma=0.03504340763088156\n",
      "Gradient Descent(3716/4999): loss=0.28730388882385055, w0=-0.3146639999981261, gamma=0.03503221101321892\n",
      "Gradient Descent(3717/4999): loss=0.28729037409409414, w0=-0.3146639999981251, gamma=0.12719035777538892\n",
      "Gradient Descent(3718/4999): loss=0.2872903723974709, w0=-0.3146639999981216, gamma=0.14901511648722396\n",
      "Gradient Descent(3719/4999): loss=0.28729036743828806, w0=-0.31466399999811806, gamma=0.07953857540554218\n",
      "Gradient Descent(3720/4999): loss=0.2872903624897196, w0=-0.31466399999811645, gamma=0.054080694948796845\n",
      "Gradient Descent(3721/4999): loss=0.2872903598504453, w0=-0.31466399999811545, gamma=0.05384151930665379\n",
      "Gradient Descent(3722/4999): loss=0.28729035805890285, w0=-0.3146639999981145, gamma=0.8100292996077673\n",
      "Gradient Descent(3723/4999): loss=0.28729035627742566, w0=-0.31466399999810096, gamma=2.2333759160278976\n",
      "Gradient Descent(3724/4999): loss=0.28729032947758626, w0=-0.31466399999809413, gamma=0.244403708761252\n",
      "Gradient Descent(3725/4999): loss=0.28729025561151955, w0=-0.3146639999980951, gamma=0.08966602327586157\n",
      "Gradient Descent(3726/4999): loss=0.2872902475339513, w0=-0.31466399999809536, gamma=0.07207131297805747\n",
      "Gradient Descent(3727/4999): loss=0.28729024455830104, w0=-0.3146639999980956, gamma=0.04772223509464661\n",
      "Gradient Descent(3728/4999): loss=0.2872902421742496, w0=-0.3146639999980957, gamma=0.03573423349696814\n",
      "Gradient Descent(3729/4999): loss=0.2872902405959322, w0=-0.3146639999980958, gamma=0.03781700790141426\n",
      "Gradient Descent(3730/4999): loss=0.2872902394141933, w0=-0.3146639999980959, gamma=0.20533394536880323\n",
      "Gradient Descent(3731/4999): loss=0.2872902381636066, w0=-0.31466399999809636, gamma=0.2529445355902128\n",
      "Gradient Descent(3732/4999): loss=0.2872902313733872, w0=-0.3146639999980968, gamma=0.8019558542302678\n",
      "Gradient Descent(3733/4999): loss=0.2872902230088034, w0=-0.31466399999809797, gamma=0.23484331841912234\n",
      "Gradient Descent(3734/4999): loss=0.2872901964894254, w0=-0.314663999998098, gamma=0.03645384638725376\n",
      "Gradient Descent(3735/4999): loss=0.2872901887244241, w0=-0.314663999998098, gamma=0.035172343945511185\n",
      "Gradient Descent(3736/4999): loss=0.2872901875182014, w0=-0.314663999998098, gamma=0.04799759917257095\n",
      "Gradient Descent(3737/4999): loss=0.28729018635513115, w0=-0.314663999998098, gamma=0.1545010346205645\n",
      "Gradient Descent(3738/4999): loss=0.2872901847679647, w0=-0.3146639999980981, gamma=0.6448594643219787\n",
      "Gradient Descent(3739/4999): loss=0.28729017965899417, w0=-0.3146639999980981, gamma=1.1410529281528872\n",
      "Gradient Descent(3740/4999): loss=0.2872901583352025, w0=-0.31466399999809835, gamma=0.07851407343570715\n",
      "Gradient Descent(3741/4999): loss=0.2872901206049042, w0=-0.31466399999809835, gamma=0.057887751321085236\n",
      "Gradient Descent(3742/4999): loss=0.287290118008073, w0=-0.31466399999809835, gamma=0.0467245206309664\n",
      "Gradient Descent(3743/4999): loss=0.2872901160938418, w0=-0.31466399999809835, gamma=0.03672251597992071\n",
      "Gradient Descent(3744/4999): loss=0.28729011454882575, w0=-0.31466399999809835, gamma=0.04312428600494811\n",
      "Gradient Descent(3745/4999): loss=0.287290113334546, w0=-0.31466399999809835, gamma=0.6368430289759649\n",
      "Gradient Descent(3746/4999): loss=0.287290111908585, w0=-0.31466399999809835, gamma=1.9601523020361393\n",
      "Gradient Descent(3747/4999): loss=0.28729009085059226, w0=-0.31466399999809813, gamma=0.13153694411499695\n",
      "Gradient Descent(3748/4999): loss=0.2872900260371144, w0=-0.3146639999980982, gamma=0.04842767833444665\n",
      "Gradient Descent(3749/4999): loss=0.28729002168790196, w0=-0.3146639999980982, gamma=0.03535428995202639\n",
      "Gradient Descent(3750/4999): loss=0.2872900200860768, w0=-0.3146639999980982, gamma=0.03544377496368396\n",
      "Gradient Descent(3751/4999): loss=0.2872900189169865, w0=-0.3146639999980982, gamma=0.11236789617724881\n",
      "Gradient Descent(3752/4999): loss=0.2872900177450291, w0=-0.3146639999980982, gamma=0.1437971524706008\n",
      "Gradient Descent(3753/4999): loss=0.28729001402956444, w0=-0.3146639999980982, gamma=1.0836398534787723\n",
      "Gradient Descent(3754/4999): loss=0.28729000927489445, w0=-0.3146639999980985, gamma=1.0319384532904023\n",
      "Gradient Descent(3755/4999): loss=0.287289973444359, w0=-0.31466399999809846, gamma=0.06643028829062239\n",
      "Gradient Descent(3756/4999): loss=0.28728993932499336, w0=-0.31466399999809846, gamma=0.05731621350111729\n",
      "Gradient Descent(3757/4999): loss=0.28728993712728024, w0=-0.31466399999809846, gamma=0.04003370183362936\n",
      "Gradient Descent(3758/4999): loss=0.28728993523210444, w0=-0.31466399999809846, gamma=0.035604660481012594\n",
      "Gradient Descent(3759/4999): loss=0.28728993390840524, w0=-0.31466399999809846, gamma=0.052384003506455\n",
      "Gradient Descent(3760/4999): loss=0.287289932731161, w0=-0.31466399999809846, gamma=0.8358078635559554\n",
      "Gradient Descent(3761/4999): loss=0.28728993099912054, w0=-0.31466399999809846, gamma=4.219638613171874\n",
      "Gradient Descent(3762/4999): loss=0.28728990336378574, w0=-0.3146639999980987, gamma=0.14914086111602387\n",
      "Gradient Descent(3763/4999): loss=0.2872897638489886, w0=-0.3146639999980987, gamma=0.060930179546600065\n",
      "Gradient Descent(3764/4999): loss=0.2872897589179372, w0=-0.3146639999980987, gamma=0.036982520587898575\n",
      "Gradient Descent(3765/4999): loss=0.2872897569022587, w0=-0.3146639999980987, gamma=0.035306170176249145\n",
      "Gradient Descent(3766/4999): loss=0.28728975567883314, w0=-0.3146639999980987, gamma=0.05713910738590928\n",
      "Gradient Descent(3767/4999): loss=0.28728975451150535, w0=-0.3146639999980987, gamma=0.10595272176628023\n",
      "Gradient Descent(3768/4999): loss=0.2872897526223331, w0=-0.3146639999980987, gamma=0.1494318951893727\n",
      "Gradient Descent(3769/4999): loss=0.287289749119267, w0=-0.3146639999980987, gamma=3.7554569743984643\n",
      "Gradient Descent(3770/4999): loss=0.28728974417867703, w0=-0.314663999998099, gamma=1.2326790145475923\n",
      "Gradient Descent(3771/4999): loss=0.2872896200152518, w0=-0.3146639999980991, gamma=0.037985847955361494\n",
      "Gradient Descent(3772/4999): loss=0.2872895792943948, w0=-0.3146639999980991, gamma=0.03568253364589691\n",
      "Gradient Descent(3773/4999): loss=0.2872895780068271, w0=-0.3146639999980991, gamma=0.0486289212454453\n",
      "Gradient Descent(3774/4999): loss=0.2872895768261739, w0=-0.3146639999980991, gamma=0.07296304149672042\n",
      "Gradient Descent(3775/4999): loss=0.2872895752182248, w0=-0.3146639999980991, gamma=0.07443445821762155\n",
      "Gradient Descent(3776/4999): loss=0.28728957280597833, w0=-0.3146639999980991, gamma=0.23031591844096694\n",
      "Gradient Descent(3777/4999): loss=0.28728957034511793, w0=-0.31466399999809913, gamma=0.2520031699376733\n",
      "Gradient Descent(3778/4999): loss=0.2872895627307063, w0=-0.31466399999809913, gamma=0.042331747280890716\n",
      "Gradient Descent(3779/4999): loss=0.2872895543993351, w0=-0.31466399999809913, gamma=0.03522736867301231\n",
      "Gradient Descent(3780/4999): loss=0.28728955299980485, w0=-0.31466399999809913, gamma=0.03939736521888048\n",
      "Gradient Descent(3781/4999): loss=0.2872895518351669, w0=-0.31466399999809913, gamma=10.509757342218405\n",
      "Gradient Descent(3782/4999): loss=0.28728955053266775, w0=-0.31466399999809724, gamma=29.987847989945347\n",
      "Gradient Descent(3783/4999): loss=0.28728920308323724, w0=-0.3146639999981707, gamma=0.10315701043945769\n",
      "Gradient Descent(3784/4999): loss=0.28728821441487745, w0=-0.31466399999816347, gamma=0.08793546539563943\n",
      "Gradient Descent(3785/4999): loss=0.28728820854287884, w0=-0.314663999998158, gamma=0.03567408801494492\n",
      "Gradient Descent(3786/4999): loss=0.2872882057529563, w0=-0.3146639999981559, gamma=0.03514981409033279\n",
      "Gradient Descent(3787/4999): loss=0.2872882043497027, w0=-0.314663999998154, gamma=0.04771217130273796\n",
      "Gradient Descent(3788/4999): loss=0.2872882031875219, w0=-0.3146639999981514, gamma=0.05388309985559543\n",
      "Gradient Descent(3789/4999): loss=0.2872882016106938, w0=-0.3146639999981487, gamma=0.07024420036142655\n",
      "Gradient Descent(3790/4999): loss=0.28728819982999954, w0=-0.3146639999981453, gamma=2.205447669959749\n",
      "Gradient Descent(3791/4999): loss=0.28728819750861656, w0=-0.3146639999980475, gamma=7.650413177286471\n",
      "Gradient Descent(3792/4999): loss=0.28728812462490677, w0=-0.31466399999845795, gamma=0.09704185682466705\n",
      "Gradient Descent(3793/4999): loss=0.2872878718292674, w0=-0.31466399999842337, gamma=0.0381768940143802\n",
      "Gradient Descent(3794/4999): loss=0.28728786862354877, w0=-0.3146639999984111, gamma=0.03518802262481903\n",
      "Gradient Descent(3795/4999): loss=0.28728786734299067, w0=-0.3146639999984002, gamma=0.045331793935368826\n",
      "Gradient Descent(3796/4999): loss=0.28728786617955576, w0=-0.3146639999983867, gamma=0.17509560609993313\n",
      "Gradient Descent(3797/4999): loss=0.28728786468118356, w0=-0.3146639999983367, gamma=0.17822712729663331\n",
      "Gradient Descent(3798/4999): loss=0.28728785889478053, w0=-0.3146639999982948, gamma=0.23133466750244244\n",
      "Gradient Descent(3799/4999): loss=0.28728785300543824, w0=-0.31466399999825007, gamma=0.0750373804740607\n",
      "Gradient Descent(3800/4999): loss=0.2872878453612374, w0=-0.3146639999982389, gamma=0.04166581878430813\n",
      "Gradient Descent(3801/4999): loss=0.28728784288171083, w0=-0.3146639999982332, gamma=0.03702623581474841\n",
      "Gradient Descent(3802/4999): loss=0.2872878415049092, w0=-0.3146639999982283, gamma=0.0694394349362661\n",
      "Gradient Descent(3803/4999): loss=0.28728784028142074, w0=-0.3146639999982195, gamma=3.559105094768841\n",
      "Gradient Descent(3804/4999): loss=0.2872878379868775, w0=-0.3146639999977993, gamma=8.912363480385892\n",
      "Gradient Descent(3805/4999): loss=0.2872877203815831, w0=-0.3146640000004946, gamma=0.037632323436785665\n",
      "Gradient Descent(3806/4999): loss=0.28728742612430097, w0=-0.31466400000040456, gamma=0.03556870282126214\n",
      "Gradient Descent(3807/4999): loss=0.2872874246613367, w0=-0.3146640000003227, gamma=0.04199137139556969\n",
      "Gradient Descent(3808/4999): loss=0.2872874234828523, w0=-0.3146640000002294, gamma=0.06535116652398418\n",
      "Gradient Descent(3809/4999): loss=0.2872874220948751, w0=-0.31466400000009037, gamma=0.11561607580351943\n",
      "Gradient Descent(3810/4999): loss=0.2872874199353361, w0=-0.3146639999998605, gamma=0.21177658215696443\n",
      "Gradient Descent(3811/4999): loss=0.287287416115129, w0=-0.31466399999948813, gamma=0.19850313216613572\n",
      "Gradient Descent(3812/4999): loss=0.2872874091179487, w0=-0.314663999999213, gamma=0.05033426858192601\n",
      "Gradient Descent(3813/4999): loss=0.2872874025595278, w0=-0.3146639999991571, gamma=0.041297213277036275\n",
      "Gradient Descent(3814/4999): loss=0.2872874008964486, w0=-0.31466399999911354, gamma=0.036202872322047185\n",
      "Gradient Descent(3815/4999): loss=0.2872873995320043, w0=-0.3146639999990769, gamma=0.06390805974302508\n",
      "Gradient Descent(3816/4999): loss=0.287287398335879, w0=-0.3146639999990146, gamma=1.8613618409518906\n",
      "Gradient Descent(3817/4999): loss=0.2872873962243893, w0=-0.3146639999973161, gamma=3.386147118745112\n",
      "Gradient Descent(3818/4999): loss=0.2872873347262796, w0=-0.31466399999997735, gamma=0.06814669707147906\n",
      "Gradient Descent(3819/4999): loss=0.28728722285730424, w0=-0.31466399999984956, gamma=0.03579087621995577\n",
      "Gradient Descent(3820/4999): loss=0.28728722060464656, w0=-0.314663999999787, gamma=0.03525038549886477\n",
      "Gradient Descent(3821/4999): loss=0.28728721941964513, w0=-0.3146639999997276, gamma=0.14964011216497983\n",
      "Gradient Descent(3822/4999): loss=0.287287218254881, w0=-0.31466399999948436, gamma=0.26782410420793124\n",
      "Gradient Descent(3823/4999): loss=0.2872872133106706, w0=-0.3146639999991142, gamma=0.2234064392409934\n",
      "Gradient Descent(3824/4999): loss=0.2872872044622312, w0=-0.31466399999888817, gamma=0.05886702562539038\n",
      "Gradient Descent(3825/4999): loss=0.28728719708148065, w0=-0.3146639999988419, gamma=0.053661853512130805\n",
      "Gradient Descent(3826/4999): loss=0.28728719513659157, w0=-0.31466399999880224, gamma=0.053689113886018686\n",
      "Gradient Descent(3827/4999): loss=0.28728719336373676, w0=-0.31466399999876465, gamma=0.08359117148828388\n",
      "Gradient Descent(3828/4999): loss=0.2872871915899834, w0=-0.3146639999987093, gamma=0.19211381676897143\n",
      "Gradient Descent(3829/4999): loss=0.28728718882834325, w0=-0.3146639999985927, gamma=0.1198964667113162\n",
      "Gradient Descent(3830/4999): loss=0.28728718248139823, w0=-0.3146639999985339, gamma=0.03923354406641821\n",
      "Gradient Descent(3831/4999): loss=0.2872871785203386, w0=-0.31466399999851696, gamma=0.035746498400650036\n",
      "Gradient Descent(3832/4999): loss=0.28728717722416197, w0=-0.31466399999850214, gamma=0.08468807913244258\n",
      "Gradient Descent(3833/4999): loss=0.2872871760431933, w0=-0.3146639999984683, gamma=94.06428720395985\n",
      "Gradient Descent(3834/4999): loss=0.2872871732453272, w0=-0.31466399996403993, gamma=54.971546592664154\n",
      "Gradient Descent(3835/4999): loss=0.28728406633206144, w0=-0.31466400183673254, gamma=0.035263794722295336\n",
      "Gradient Descent(3836/4999): loss=0.2872832961322312, w0=-0.31466400177189563, gamma=0.03503388987243507\n",
      "Gradient Descent(3837/4999): loss=0.2872822569470304, w0=-0.314664001709753, gamma=0.06680450779316159\n",
      "Gradient Descent(3838/4999): loss=0.28728225471981783, w0=-0.31466400159540747, gamma=0.3511403036201804\n",
      "Gradient Descent(3839/4999): loss=0.2872822509834036, w0=-0.31466400103453157, gamma=0.36873190956473095\n",
      "Gradient Descent(3840/4999): loss=0.28728223615874954, w0=-0.3146640006523695, gamma=0.19254634634122494\n",
      "Gradient Descent(3841/4999): loss=0.2872822238977899, w0=-0.31466400052639404, gamma=0.1033797434410899\n",
      "Gradient Descent(3842/4999): loss=0.2872822175357706, w0=-0.31466400047178006, gamma=0.10176337290472884\n",
      "Gradient Descent(3843/4999): loss=0.2872822141021265, w0=-0.31466400042357767, gamma=0.03769176925491216\n",
      "Gradient Descent(3844/4999): loss=0.28728221074489635, w0=-0.314664000407541, gamma=0.03547299520326715\n",
      "Gradient Descent(3845/4999): loss=0.28728220950117006, w0=-0.31466400039301723, gamma=0.09122499360917449\n",
      "Gradient Descent(3846/4999): loss=0.2872822083308916, w0=-0.3146640003569917, gamma=0.47266592864273776\n",
      "Gradient Descent(3847/4999): loss=0.2872822053213562, w0=-0.31466400018736007, gamma=0.4849903997817004\n",
      "Gradient Descent(3848/4999): loss=0.28728218972848224, w0=-0.31466400009557516, gamma=0.07828869507650567\n",
      "Gradient Descent(3849/4999): loss=0.28728217372937415, w0=-0.31466400008794465, gamma=0.035620227750664334\n",
      "Gradient Descent(3850/4999): loss=0.28728217114676113, w0=-0.3146640000847447, gamma=0.03523720228028447\n",
      "Gradient Descent(3851/4999): loss=0.287282169971685, w0=-0.31466400008169193, gamma=0.3258230330503705\n",
      "Gradient Descent(3852/4999): loss=0.2872821688092669, w0=-0.3146640000544587, gamma=4.190819969536986\n",
      "Gradient Descent(3853/4999): loss=0.2872821580609052, w0=-0.3146639998183074, gamma=0.24101326537102635\n",
      "Gradient Descent(3854/4999): loss=0.28728201981484225, w0=-0.31466399986164206, gamma=0.05367763928404135\n",
      "Gradient Descent(3855/4999): loss=0.28728201186898, w0=-0.3146639998689673, gamma=0.05356135554091593\n",
      "Gradient Descent(3856/4999): loss=0.2872820100935152, w0=-0.31466399987588434, gamma=0.03802769201486979\n",
      "Gradient Descent(3857/4999): loss=0.2872820083266947, w0=-0.3146639998805323, gamma=0.04149224690545149\n",
      "Gradient Descent(3858/4999): loss=0.28728200707228224, w0=-0.31466399988541083, gamma=0.6897286370510566\n",
      "Gradient Descent(3859/4999): loss=0.28728200570358636, w0=-0.31466399996314226, gamma=7.106970348392042\n",
      "Gradient Descent(3860/4999): loss=0.28728198295170126, w0=-0.3146640002116519, gamma=0.1013444251782948\n",
      "Gradient Descent(3861/4999): loss=0.28728174852478827, w0=-0.3146640001900106, gamma=0.037351335821677205\n",
      "Gradient Descent(3862/4999): loss=0.2872817451858748, w0=-0.31466400018284285, gamma=0.03512262077384692\n",
      "Gradient Descent(3863/4999): loss=0.2872817439466107, w0=-0.31466400017635454, gamma=0.03964527288549997\n",
      "Gradient Descent(3864/4999): loss=0.28728174278805807, w0=-0.31466400016928797, gamma=0.08104701857527091\n",
      "Gradient Descent(3865/4999): loss=0.2872817414803803, w0=-0.31466400015541446, gamma=0.09747571973222457\n",
      "Gradient Descent(3866/4999): loss=0.2872817388071063, w0=-0.31466400014008106, gamma=0.45220816395208574\n",
      "Gradient Descent(3867/4999): loss=0.287281735591951, w0=-0.3146640000758804, gamma=3.9328084554495324\n",
      "Gradient Descent(3868/4999): loss=0.28728172067627117, w0=-0.3146639997700228, gamma=0.21423865191786667\n",
      "Gradient Descent(3869/4999): loss=0.2872815909583389, w0=-0.3146639998188879, gamma=0.05498980831179712\n",
      "Gradient Descent(3870/4999): loss=0.2872815838948629, w0=-0.31466399982874327, gamma=0.04320307867626434\n",
      "Gradient Descent(3871/4999): loss=0.28728158207827686, w0=-0.3146639998360604, gamma=0.03586677514593269\n",
      "Gradient Descent(3872/4999): loss=0.2872815806529909, w0=-0.3146639998418726, gamma=0.037981413776346756\n",
      "Gradient Descent(3873/4999): loss=0.28728157946998506, w0=-0.31466399984780674, gamma=0.09433953730741547\n",
      "Gradient Descent(3874/4999): loss=0.2872815782172556, w0=-0.3146639998619863, gamma=0.1328414254862109\n",
      "Gradient Descent(3875/4999): loss=0.28728157510569097, w0=-0.31466399988006916, gamma=0.5587066124529744\n",
      "Gradient Descent(3876/4999): loss=0.2872815707242405, w0=-0.31466399994601923, gamma=0.7269037785086101\n",
      "Gradient Descent(3877/4999): loss=0.2872815522967108, w0=-0.31466399998388417, gamma=0.05418750485075728\n",
      "Gradient Descent(3878/4999): loss=0.28728152832201265, w0=-0.31466399998465505, gamma=0.038924981406062854\n",
      "Gradient Descent(3879/4999): loss=0.28728152653458444, w0=-0.3146639999851788, gamma=0.0358603479311385\n",
      "Gradient Descent(3880/4999): loss=0.28728152525071415, w0=-0.31466399998564254, gamma=0.050187098163535046\n",
      "Gradient Descent(3881/4999): loss=0.28728152406796365, w0=-0.31466399998626826, gamma=0.15472189859671875\n",
      "Gradient Descent(3882/4999): loss=0.28728152241268906, w0=-0.3146639999881004, gamma=0.9246021711833873\n",
      "Gradient Descent(3883/4999): loss=0.2872815173096446, w0=-0.3146639999973553, gamma=0.6830718255959943\n",
      "Gradient Descent(3884/4999): loss=0.2872814868144849, w0=-0.31466399999787076, gamma=0.062777436648826\n",
      "Gradient Descent(3885/4999): loss=0.28728146428620516, w0=-0.3146639999978858, gamma=0.045571200940810866\n",
      "Gradient Descent(3886/4999): loss=0.2872814622152342, w0=-0.314663999997896, gamma=0.03549655158150093\n",
      "Gradient Descent(3887/4999): loss=0.28728146071215, w0=-0.3146639999979036, gamma=0.03614808742644243\n",
      "Gradient Descent(3888/4999): loss=0.2872814595414142, w0=-0.31466399999791106, gamma=0.17334125771795528\n",
      "Gradient Descent(3889/4999): loss=0.287281458349198, w0=-0.31466399999794553, gamma=0.49108597692018213\n",
      "Gradient Descent(3890/4999): loss=0.2872814526321577, w0=-0.31466399999802636, gamma=0.2912182073213895\n",
      "Gradient Descent(3891/4999): loss=0.28728143643549076, w0=-0.3146639999980508, gamma=0.07889551258610315\n",
      "Gradient Descent(3892/4999): loss=0.28728142683080465, w0=-0.31466399999805544, gamma=0.07189160313097655\n",
      "Gradient Descent(3893/4999): loss=0.2872814242287005, w0=-0.3146639999980594, gamma=0.05007431609260729\n",
      "Gradient Descent(3894/4999): loss=0.28728142185763667, w0=-0.31466399999806194, gamma=0.04602537030904334\n",
      "Gradient Descent(3895/4999): loss=0.28728142020613173, w0=-0.31466399999806416, gamma=0.09463304814561314\n",
      "Gradient Descent(3896/4999): loss=0.2872814186881665, w0=-0.3146639999980685, gamma=0.640190231316722\n",
      "Gradient Descent(3897/4999): loss=0.2872814155670705, w0=-0.3146639999980951, gamma=0.24412299288304673\n",
      "Gradient Descent(3898/4999): loss=0.28728139445298584, w0=-0.3146639999980988, gamma=0.0357326510604773\n",
      "Gradient Descent(3899/4999): loss=0.2872813864017883, w0=-0.3146639999980992, gamma=0.03505161509211609\n",
      "Gradient Descent(3900/4999): loss=0.2872813852231236, w0=-0.3146639999980996, gamma=0.08419285747663552\n",
      "Gradient Descent(3901/4999): loss=0.28728138406709613, w0=-0.31466399999810046, gamma=4017.9968950658\n",
      "Gradient Descent(3902/4999): loss=0.28728138129035696, w0=-0.31466400003734585, gamma=16216.595844196505\n",
      "Gradient Descent(3903/4999): loss=0.2871501646228961, w0=-0.3146633666682401, gamma=0.047632705105705136\n",
      "Gradient Descent(3904/4999): loss=0.6285890535336669, w0=-0.31466339683548705, gamma=0.04134078282280548\n",
      "Gradient Descent(3905/4999): loss=0.3139733127473901, w0=-0.31466342177075174, gamma=0.037464585295272364\n",
      "Gradient Descent(3906/4999): loss=0.2898934868409014, w0=-0.314663443433833, gamma=0.0453937776978101\n",
      "Gradient Descent(3907/4999): loss=0.28779167435171893, w0=-0.3146634686984285, gamma=0.0948588055218163\n",
      "Gradient Descent(3908/4999): loss=0.28708950828721386, w0=-0.3146635190969637, gamma=0.1178484926782141\n",
      "Gradient Descent(3909/4999): loss=0.28668308536842946, w0=-0.3146635757705383, gamma=0.07327726327255804\n",
      "Gradient Descent(3910/4999): loss=0.28666112055409787, w0=-0.314663606856835, gamma=0.036823522566071534\n",
      "Gradient Descent(3911/4999): loss=0.2866604816593232, w0=-0.31466362133371256, gamma=0.03514740683527226\n",
      "Gradient Descent(3912/4999): loss=0.28665616714002096, w0=-0.3146636346428136, gamma=0.04041669178276459\n",
      "Gradient Descent(3913/4999): loss=0.2866561412853835, w0=-0.31466364940929986, gamma=0.0691587925215901\n",
      "Gradient Descent(3914/4999): loss=0.28665613627120307, w0=-0.31466367365565656, gamma=0.09330987581178939\n",
      "Gradient Descent(3915/4999): loss=0.2866561324950696, w0=-0.3146637041067085, gamma=0.12232817893314266\n",
      "Gradient Descent(3916/4999): loss=0.2866561292021547, w0=-0.31466374030266725, gamma=0.12324353087442624\n",
      "Gradient Descent(3917/4999): loss=0.2866561253771757, w0=-0.3146637723085538, gamma=0.07900399528448468\n",
      "Gradient Descent(3918/4999): loss=0.28665612159960446, w0=-0.3146637902970046, gamma=0.058060865192953746\n",
      "Gradient Descent(3919/4999): loss=0.2866561191864977, w0=-0.3146638024724808, gamma=0.047212535533114275\n",
      "Gradient Descent(3920/4999): loss=0.28665611741738467, w0=-0.31466381179820613, gamma=0.05028428523025833\n",
      "Gradient Descent(3921/4999): loss=0.2866561159803414, w0=-0.31466382126174586, gamma=0.1137020274316231\n",
      "Gradient Descent(3922/4999): loss=0.2866561144506638, w0=-0.31466384158452804, gamma=0.23148238048553887\n",
      "Gradient Descent(3923/4999): loss=0.2866561109934434, w0=-0.31466387825467496, gamma=0.07650714836251411\n",
      "Gradient Descent(3924/4999): loss=0.28665610396013697, w0=-0.31466388756898206, gamma=0.035691814857234085\n",
      "Gradient Descent(3925/4999): loss=0.28665610163667227, w0=-0.3146638915818116, gamma=0.035473511287888546\n",
      "Gradient Descent(3926/4999): loss=0.2866561005516866, w0=-0.3146638954277481, gamma=0.9090867778774833\n",
      "Gradient Descent(3927/4999): loss=0.2866560994744034, w0=-0.31466399049204274, gamma=3.4813831278109575\n",
      "Gradient Descent(3928/4999): loss=0.2866560718760236, w0=-0.31466402358922124, gamma=1.576230735885992\n",
      "Gradient Descent(3929/4999): loss=0.2866559663720097, w0=-0.314663986405508, gamma=0.04956401466976377\n",
      "Gradient Descent(3930/4999): loss=0.28665591958000064, w0=-0.31466398707925347, gamma=0.035152107713315034\n",
      "Gradient Descent(3931/4999): loss=0.28665591746720426, w0=-0.31466398753340796, gamma=0.03552477615030408\n",
      "Gradient Descent(3932/4999): loss=0.2866559162588563, w0=-0.3146639879762435, gamma=0.3150937300276746\n",
      "Gradient Descent(3933/4999): loss=0.286655915160976, w0=-0.31466399176452237, gamma=0.3180946057642385\n",
      "Gradient Descent(3934/4999): loss=0.2866559055519716, w0=-0.31466399438384884, gamma=0.6573028364538095\n",
      "Gradient Descent(3935/4999): loss=0.28665589594704216, w0=-0.3146639980746697, gamma=0.09820012719562723\n",
      "Gradient Descent(3936/4999): loss=0.28665587610857046, w0=-0.31466399826363417, gamma=0.04891321921142424\n",
      "Gradient Descent(3937/4999): loss=0.2866558731449904, w0=-0.314663998348514, gamma=0.03927291173101717\n",
      "Gradient Descent(3938/4999): loss=0.28665587166711476, w0=-0.31466399841333137, gamma=0.037149141669144466\n",
      "Gradient Descent(3939/4999): loss=0.28665587048201274, w0=-0.3146639984722357, gamma=0.11646470895938162\n",
      "Gradient Descent(3940/4999): loss=0.2866558693612184, w0=-0.31466399865004385, gamma=1.9994594691667185\n",
      "Gradient Descent(3941/4999): loss=0.2866558658476369, w0=-0.31466400134712375, gamma=1.8915627779485353\n",
      "Gradient Descent(3942/4999): loss=0.28665580555431025, w0=-0.3146639987969663, gamma=0.044098864035435635\n",
      "Gradient Descent(3943/4999): loss=0.286655749462018, w0=-0.31466399884997237, gamma=0.038707969219110745\n",
      "Gradient Descent(3944/4999): loss=0.2866557473275027, w0=-0.3146639988944469, gamma=0.038253792687658696\n",
      "Gradient Descent(3945/4999): loss=0.28665574611197553, w0=-0.3146639989366983, gamma=0.05099094448931513\n",
      "Gradient Descent(3946/4999): loss=0.2866557449584042, w0=-0.31466399899086345, gamma=0.07199717465499877\n",
      "Gradient Descent(3947/4999): loss=0.28665574342414163, w0=-0.3146639990634428, gamma=0.17424956248838994\n",
      "Gradient Descent(3948/4999): loss=0.28665574125864035, w0=-0.3146639992264543, gamma=0.2396125538795557\n",
      "Gradient Descent(3949/4999): loss=0.28665573601854294, w0=-0.31466399941155376, gamma=0.05792186438362571\n",
      "Gradient Descent(3950/4999): loss=0.28665572881433304, w0=-0.31466399944557677, gamma=0.035824385183470135\n",
      "Gradient Descent(3951/4999): loss=0.2866557270727995, w0=-0.31466399946540097, gamma=0.036592380118670075\n",
      "Gradient Descent(3952/4999): loss=0.28665572599569106, w0=-0.31466399948492474, gamma=3.018111773604666\n",
      "Gradient Descent(3953/4999): loss=0.28665572489567714, w0=-0.31466400103630765, gamma=154.5340260997574\n",
      "Gradient Descent(3954/4999): loss=0.2866556342197392, w0=-0.31466384072914116, gamma=0.23312123218795772\n",
      "Gradient Descent(3955/4999): loss=0.2866511634385664, w0=-0.31466387785831906, gamma=0.035782484349722386\n",
      "Gradient Descent(3956/4999): loss=0.2866518255746244, w0=-0.3146638822288148, gamma=0.03504014863313578\n",
      "Gradient Descent(3957/4999): loss=0.2866511232868279, w0=-0.3146638863554984, gamma=0.042058766325950674\n",
      "Gradient Descent(3958/4999): loss=0.2866511212957102, w0=-0.3146638911352025, gamma=0.14305978703299083\n",
      "Gradient Descent(3959/4999): loss=0.28665111965807055, w0=-0.31466390670922934, gamma=0.15814648317039676\n",
      "Gradient Descent(3960/4999): loss=0.28665111514218156, w0=-0.31466392146267314, gamma=0.09011864916012721\n",
      "Gradient Descent(3961/4999): loss=0.2866511107541246, w0=-0.31466392854025776, gamma=0.06070550776110796\n",
      "Gradient Descent(3962/4999): loss=0.28665110825916856, w0=-0.31466393287819494, gamma=0.06353603600014644\n",
      "Gradient Descent(3963/4999): loss=0.2866511065805641, w0=-0.3146639371427827, gamma=0.3525832371598927\n",
      "Gradient Descent(3964/4999): loss=0.28665110482589484, w0=-0.3146639593048194, gamma=0.42284895237124914\n",
      "Gradient Descent(3965/4999): loss=0.28665109509441095, w0=-0.3146639765122968, gamma=0.04738266355845865\n",
      "Gradient Descent(3966/4999): loss=0.2866510834303146, w0=-0.31466397762515774, gamma=0.03537728205599947\n",
      "Gradient Descent(3967/4999): loss=0.28665108212259305, w0=-0.3146639784166823, gamma=0.03560789360777673\n",
      "Gradient Descent(3968/4999): loss=0.2866510811466365, w0=-0.31466397918518196, gamma=0.8959297123478026\n",
      "Gradient Descent(3969/4999): loss=0.28665108016441654, w0=-0.3146639978328699, gamma=6.182956687692187\n",
      "Gradient Descent(3970/4999): loss=0.28665105545177066, w0=-0.3146640112257511, gamma=0.0902605109351349\n",
      "Gradient Descent(3971/4999): loss=0.2866508850336616, w0=-0.3146640102124158, gamma=0.05770039831293604\n",
      "Gradient Descent(3972/4999): loss=0.2866508824985838, w0=-0.3146640096230957, gamma=0.057229789897968814\n",
      "Gradient Descent(3973/4999): loss=0.2866508808864071, w0=-0.31466400907230885, gamma=0.0364757416640788\n",
      "Gradient Descent(3974/4999): loss=0.2866508793094391, w0=-0.314664008741352, gamma=0.03774265497079654\n",
      "Gradient Descent(3975/4999): loss=0.2866508783043596, w0=-0.31466400841139114, gamma=0.698034047241659\n",
      "Gradient Descent(3976/4999): loss=0.28665087726439675, w0=-0.31466400253923266, gamma=2.1474216668346795\n",
      "Gradient Descent(3977/4999): loss=0.286650858031346, w0=-0.314663997084211, gamma=0.21643224228123642\n",
      "Gradient Descent(3978/4999): loss=0.2866507988764509, w0=-0.31466399771505815, gamma=0.05870361924785252\n",
      "Gradient Descent(3979/4999): loss=0.28665079291974754, w0=-0.31466399784913185, gamma=0.03575949511081574\n",
      "Gradient Descent(3980/4999): loss=0.2866507912976984, w0=-0.31466399792600885, gamma=0.03541537522551252\n",
      "Gradient Descent(3981/4999): loss=0.28665079030951063, w0=-0.31466399799942346, gamma=0.11424329253563272\n",
      "Gradient Descent(3982/4999): loss=0.2866507893339764, w0=-0.3146639982278579, gamma=0.13799294898795877\n",
      "Gradient Descent(3983/4999): loss=0.28665078618742096, w0=-0.31466399847225845, gamma=0.8755154936199814\n",
      "Gradient Descent(3984/4999): loss=0.28665078238697994, w0=-0.31466399980891563, gamma=1.6747276371576225\n",
      "Gradient Descent(3985/4999): loss=0.28665075827568043, w0=-0.3146640001272002, gamma=0.054940956104422026\n",
      "Gradient Descent(3986/4999): loss=0.2866507121796331, w0=-0.31466400012015494, gamma=0.03916133874653571\n",
      "Gradient Descent(3987/4999): loss=0.2866507106521186, w0=-0.31466400011540907, gamma=0.035856228160659954\n",
      "Gradient Descent(3988/4999): loss=0.2866507095707653, w0=-0.3146640001112339, gamma=0.045612503598651565\n",
      "Gradient Descent(3989/4999): loss=0.2866507085834998, w0=-0.3146640001061131, gamma=0.08738503757863791\n",
      "Gradient Descent(3990/4999): loss=0.2866507073277492, w0=-0.31466400009675016, gamma=0.16075344502329736\n",
      "Gradient Descent(3991/4999): loss=0.2866507049220358, w0=-0.3146640000810312, gamma=1.2470202019555119\n",
      "Gradient Descent(3992/4999): loss=0.2866507004965669, w0=-0.3146639999786956, gamma=0.5201585314155345\n",
      "Gradient Descent(3993/4999): loss=0.2866506661688626, w0=-0.31466399998924005, gamma=0.03925295417733858\n",
      "Gradient Descent(3994/4999): loss=0.2866506518674815, w0=-0.31466399998962186, gamma=0.03581635319810186\n",
      "Gradient Descent(3995/4999): loss=0.2866506507723133, w0=-0.3146639999899566, gamma=0.044075361623357456\n",
      "Gradient Descent(3996/4999): loss=0.28665064978598687, w0=-0.3146639999903537, gamma=0.07525839458415924\n",
      "Gradient Descent(3997/4999): loss=0.28665064857282424, w0=-0.314663999991002, gamma=0.0903340380385668\n",
      "Gradient Descent(3998/4999): loss=0.2866506465015457, w0=-0.3146639999917215, gamma=2.4009350256440216\n",
      "Gradient Descent(3999/4999): loss=0.2866506440154044, w0=-0.31466400000911793, gamma=1.8411884142091715\n",
      "Gradient Descent(4000/4999): loss=0.28665057794381754, w0=-0.3146639999904289, gamma=0.03573118577597521\n",
      "Gradient Descent(4001/4999): loss=0.28665052754632947, w0=-0.31466399999073397, gamma=0.035059287532859966\n",
      "Gradient Descent(4002/4999): loss=0.2866505263096079, w0=-0.3146639999910226, gamma=0.05133225904972608\n",
      "Gradient Descent(4003/4999): loss=0.28665052534463503, w0=-0.31466399999143047, gamma=0.1059944894669117\n",
      "Gradient Descent(4004/4999): loss=0.2866505239323615, w0=-0.3146639999922294, gamma=0.15426618285065344\n",
      "Gradient Descent(4005/4999): loss=0.28665052101670446, w0=-0.31466399999326883, gamma=0.27967372305068317\n",
      "Gradient Descent(4006/4999): loss=0.2866505167735565, w0=-0.3146639999948626, gamma=0.27961363111886234\n",
      "Gradient Descent(4007/4999): loss=0.286650509081354, w0=-0.3146639999960104, gamma=0.09656459991635083\n",
      "Gradient Descent(4008/4999): loss=0.2866505013911872, w0=-0.314663999996296, gamma=0.07033802276391812\n",
      "Gradient Descent(4009/4999): loss=0.2866504987353426, w0=-0.3146639999964839, gamma=0.050027894616457375\n",
      "Gradient Descent(4010/4999): loss=0.286650496800892, w0=-0.31466399999660816, gamma=0.04332082164166917\n",
      "Gradient Descent(4011/4999): loss=0.2866504954250317, w0=-0.31466399999671035, gamma=0.0796297177020155\n",
      "Gradient Descent(4012/4999): loss=0.2866504942336385, w0=-0.3146639999968901, gamma=0.8288771783138632\n",
      "Gradient Descent(4013/4999): loss=0.28665049204370796, w0=-0.31466399999861205, gamma=0.5254290372282628\n",
      "Gradient Descent(4014/4999): loss=0.28665046924923604, w0=-0.3146639999987989, gamma=0.035983393431394\n",
      "Gradient Descent(4015/4999): loss=0.2866504548089234, w0=-0.314663999998805, gamma=0.035036856658714574\n",
      "Gradient Descent(4016/4999): loss=0.28665045381165366, w0=-0.31466399999881073, gamma=0.041596862602985674\n",
      "Gradient Descent(4017/4999): loss=0.28665045284826624, w0=-0.3146639999988173, gamma=57.675139154222116\n",
      "Gradient Descent(4018/4999): loss=0.2866504517045148, w0=-0.314664000007501, gamma=190.17825557929712\n",
      "Gradient Descent(4019/4999): loss=0.28664886881040896, w0=-0.3146639983853791, gamma=0.07354718641160264\n",
      "Gradient Descent(4020/4999): loss=0.286645956093244, w0=-0.31466399850405524, gamma=0.059909785263435586\n",
      "Gradient Descent(4021/4999): loss=0.28664398239658867, w0=-0.31466399859361605, gamma=0.058928091836378635\n",
      "Gradient Descent(4022/4999): loss=0.28664373443252605, w0=-0.3146639986764316, gamma=0.09058396248477434\n",
      "Gradient Descent(4023/4999): loss=0.2866437017392632, w0=-0.31466399879623347, gamma=0.10037855245815275\n",
      "Gradient Descent(4024/4999): loss=0.2866436889149223, w0=-0.31466399891696367, gamma=0.04217379092711741\n",
      "Gradient Descent(4025/4999): loss=0.286643685884096, w0=-0.3146639989625965, gamma=0.035183047232894844\n",
      "Gradient Descent(4026/4999): loss=0.28664368394604767, w0=-0.3146639989990597, gamma=0.03637743580336737\n",
      "Gradient Descent(4027/4999): loss=0.2866436829467121, w0=-0.31466399903543435, gamma=0.15738394140313766\n",
      "Gradient Descent(4028/4999): loss=0.2866436819530828, w0=-0.3146639991870813, gamma=0.17816697732094158\n",
      "Gradient Descent(4029/4999): loss=0.28664367767762655, w0=-0.3146639993317353, gamma=0.22007020874696567\n",
      "Gradient Descent(4030/4999): loss=0.28664367285693454, w0=-0.31466399947857643, gamma=0.08555530013458172\n",
      "Gradient Descent(4031/4999): loss=0.2866436669095932, w0=-0.3146639995230999, gamma=0.05410141259659323\n",
      "Gradient Descent(4032/4999): loss=0.2866436645974574, w0=-0.31466399954884583, gamma=0.052488812206163714\n",
      "Gradient Descent(4033/4999): loss=0.28664366313546313, w0=-0.31466399957247293, gamma=0.059424964188308005\n",
      "Gradient Descent(4034/4999): loss=0.28664366171733113, w0=-0.31466399959781827, gamma=0.10337371076073437\n",
      "Gradient Descent(4035/4999): loss=0.28664366011182557, w0=-0.3146639996392881, gamma=0.13528693366993852\n",
      "Gradient Descent(4036/4999): loss=0.2866436573189831, w0=-0.31466399968795, gamma=0.05578385435134586\n",
      "Gradient Descent(4037/4999): loss=0.2866436536640103, w0=-0.31466399970530057, gamma=0.03663186226676855\n",
      "Gradient Descent(4038/4999): loss=0.28664365215693266, w0=-0.3146639997160587, gamma=0.03897137463710366\n",
      "Gradient Descent(4039/4999): loss=0.28664365116727664, w0=-0.3146639997270846, gamma=0.6352955592884095\n",
      "Gradient Descent(4040/4999): loss=0.28664365011442244, w0=-0.31466399989981997, gamma=1.055972745148564\n",
      "Gradient Descent(4041/4999): loss=0.2866436329513828, w0=-0.31466400000453276, gamma=0.3148749358760906\n",
      "Gradient Descent(4042/4999): loss=0.2866436044241059, w0=-0.3146640000027851, gamma=0.03547225824044861\n",
      "Gradient Descent(4043/4999): loss=0.28664359591962285, w0=-0.3146640000026502, gamma=0.035036691004032595\n",
      "Gradient Descent(4044/4999): loss=0.2866435949596128, w0=-0.3146640000025217, gamma=0.064834813796677\n",
      "Gradient Descent(4045/4999): loss=0.286643594013121, w0=-0.3146640000022922, gamma=13.964987468241135\n",
      "Gradient Descent(4046/4999): loss=0.28664359226165603, w0=-0.31466399995607097, gamma=17.561273682957246\n",
      "Gradient Descent(4047/4999): loss=0.28664321504536466, w0=-0.31466400070966283, gamma=0.38733544428668903\n",
      "Gradient Descent(4048/4999): loss=0.2866427417021406, w0=-0.31466400043439163, gamma=0.21685733030835413\n",
      "Gradient Descent(4049/4999): loss=0.28664273055544887, w0=-0.3146640003399702, gamma=0.05246909558431679\n",
      "Gradient Descent(4050/4999): loss=0.2866427250099888, w0=-0.314664000322079, gamma=0.035534069748707243\n",
      "Gradient Descent(4051/4999): loss=0.286642723231867, w0=-0.3146640003105981, gamma=0.03561797242718863\n",
      "Gradient Descent(4052/4999): loss=0.28664272216000136, w0=-0.31466400029949904, gamma=0.11919718743967476\n",
      "Gradient Descent(4053/4999): loss=0.28664272119105894, w0=-0.31466400026367847, gamma=0.12496432482099779\n",
      "Gradient Descent(4054/4999): loss=0.2866427179652946, w0=-0.31466400023060104, gamma=0.12431973465123622\n",
      "Gradient Descent(4055/4999): loss=0.28664271459144003, w0=-0.31466400020180646, gamma=0.09515993365455361\n",
      "Gradient Descent(4056/4999): loss=0.2866427112350608, w0=-0.31466400018250584, gamma=0.04805227849978318\n",
      "Gradient Descent(4057/4999): loss=0.28664270866595937, w0=-0.3146640001736872, gamma=0.03778394334038322\n",
      "Gradient Descent(4058/4999): loss=0.2866427073686577, w0=-0.3146640001670862, gamma=0.05123472930307528\n",
      "Gradient Descent(4059/4999): loss=0.2866427063485817, w0=-0.3146640001584735, gamma=0.7005079620021879\n",
      "Gradient Descent(4060/4999): loss=0.2866427049653693, w0=-0.3146640000467496, gamma=1.050643894175833\n",
      "Gradient Descent(4061/4999): loss=0.2866426860534811, w0=-0.3146639999965649, gamma=0.058582902720263536\n",
      "Gradient Descent(4062/4999): loss=0.28664265768948427, w0=-0.31466399999670663, gamma=0.035154150383320364\n",
      "Gradient Descent(4063/4999): loss=0.28664265610781, w0=-0.3146639999967867, gamma=0.03511429193128403\n",
      "Gradient Descent(4064/4999): loss=0.2866426551586622, w0=-0.31466399999686384, gamma=0.566996977647821\n",
      "Gradient Descent(4065/4999): loss=0.28664265421069435, w0=-0.3146639999980659, gamma=0.9374850365033885\n",
      "Gradient Descent(4066/4999): loss=0.28664263890374453, w0=-0.3146639999989264, gamma=0.15600779419627792\n",
      "Gradient Descent(4067/4999): loss=0.2866426135952654, w0=-0.3146639999989354, gamma=0.07354134076974896\n",
      "Gradient Descent(4068/4999): loss=0.28664260938370956, w0=-0.31466399999893896, gamma=0.07297889270492364\n",
      "Gradient Descent(4069/4999): loss=0.28664260739829533, w0=-0.31466399999894223, gamma=0.26885811227146356\n",
      "Gradient Descent(4070/4999): loss=0.28664260542816933, w0=-0.31466399999895345, gamma=0.2807672939436921\n",
      "Gradient Descent(4071/4999): loss=0.2866425981701363, w0=-0.31466399999896205, gamma=0.04071155182454669\n",
      "Gradient Descent(4072/4999): loss=0.2866425905906758, w0=-0.31466399999896294, gamma=0.035190463911305224\n",
      "Gradient Descent(4073/4999): loss=0.2866425894916057, w0=-0.31466399999896366, gamma=0.03939692197046156\n",
      "Gradient Descent(4074/4999): loss=0.28664258854161867, w0=-0.31466399999896444, gamma=6.066191660409039\n",
      "Gradient Descent(4075/4999): loss=0.28664258747807736, w0=-0.3146639999990836, gamma=47.69920636239911\n",
      "Gradient Descent(4076/4999): loss=0.2866424237218094, w0=-0.3146639999943422, gamma=0.06031282626821959\n",
      "Gradient Descent(4077/4999): loss=0.28664113808461267, w0=-0.3146639999946223, gamma=0.05294357117361425\n",
      "Gradient Descent(4078/4999): loss=0.28664113480120873, w0=-0.31466399999485334, gamma=0.04319235099669894\n",
      "Gradient Descent(4079/4999): loss=0.286641133339336, w0=-0.31466399999503186, gamma=0.03528344845731405\n",
      "Gradient Descent(4080/4999): loss=0.28664113217139264, w0=-0.31466399999517136, gamma=0.03852520654212313\n",
      "Gradient Descent(4081/4999): loss=0.28664113121915086, w0=-0.3146639999953183, gamma=0.39706446838980475\n",
      "Gradient Descent(4082/4999): loss=0.286641130179625, w0=-0.3146639999967747, gamma=0.40580683305130005\n",
      "Gradient Descent(4083/4999): loss=0.2866411194673356, w0=-0.3146639999976721, gamma=0.15149840077706103\n",
      "Gradient Descent(4084/4999): loss=0.286641108520607, w0=-0.31466399999787115, gamma=0.057521711991568455\n",
      "Gradient Descent(4085/4999): loss=0.28664110443392965, w0=-0.31466399999793526, gamma=0.042066054444604685\n",
      "Gradient Descent(4086/4999): loss=0.28664110288225425, w0=-0.31466399999797945, gamma=0.03658134163037927\n",
      "Gradient Descent(4087/4999): loss=0.28664110174751595, w0=-0.31466399999801625, gamma=0.061370408994390384\n",
      "Gradient Descent(4088/4999): loss=0.2866411007607325, w0=-0.31466399999807576, gamma=1.4550708197355486\n",
      "Gradient Descent(4089/4999): loss=0.28664109910526403, w0=-0.3146639999994006, gamma=8.351656295853518\n",
      "Gradient Descent(4090/4999): loss=0.2866410598548988, w0=-0.31466399999593825, gamma=0.05429067059746163\n",
      "Gradient Descent(4091/4999): loss=0.28664083459903195, w0=-0.3146639999961037, gamma=0.0357318322260535\n",
      "Gradient Descent(4092/4999): loss=0.2866408331209306, w0=-0.31466399999620676, gamma=0.03540152750241009\n",
      "Gradient Descent(4093/4999): loss=0.2866408321520213, w0=-0.3146639999963052, gamma=0.12694323512158937\n",
      "Gradient Descent(4094/4999): loss=0.2866408311968594, w0=-0.31466399999664557, gamma=0.17500755309777602\n",
      "Gradient Descent(4095/4999): loss=0.28664082777243666, w0=-0.3146639999970553, gamma=0.13799700968783857\n",
      "Gradient Descent(4096/4999): loss=0.2866408230521183, w0=-0.3146639999973218, gamma=0.07665536623257431\n",
      "Gradient Descent(4097/4999): loss=0.2866408193301248, w0=-0.31466399999744943, gamma=0.06930066233646959\n",
      "Gradient Descent(4098/4999): loss=0.2866408172626082, w0=-0.31466399999755595, gamma=0.04856297192552923\n",
      "Gradient Descent(4099/4999): loss=0.28664081539346803, w0=-0.31466399999762545, gamma=0.04876372617311021\n",
      "Gradient Descent(4100/4999): loss=0.2866408140836546, w0=-0.31466399999769185, gamma=0.1264069247171727\n",
      "Gradient Descent(4101/4999): loss=0.2866408127684275, w0=-0.3146639999978555, gamma=0.6217638148014775\n",
      "Gradient Descent(4102/4999): loss=0.28664080935905595, w0=-0.3146639999985588, gamma=0.1221237665860944\n",
      "Gradient Descent(4103/4999): loss=0.28664079258927944, w0=-0.31466399999861105, gamma=0.035342333879147286\n",
      "Gradient Descent(4104/4999): loss=0.28664078929552084, w0=-0.3146639999986243, gamma=0.035082056160087985\n",
      "Gradient Descent(4105/4999): loss=0.2866407883422273, w0=-0.31466399999863703, gamma=0.6719966494938571\n",
      "Gradient Descent(4106/4999): loss=0.2866407873960255, w0=-0.3146639999988719, gamma=392.7902833583521\n",
      "Gradient Descent(4107/4999): loss=0.28664076927157667, w0=-0.31466400004389333, gamma=115.67765463283945\n",
      "Gradient Descent(4108/4999): loss=0.2866301881807994, w0=-0.3146639948509641, gamma=0.05807073884938865\n",
      "Gradient Descent(4109/4999): loss=0.2866398739791828, w0=-0.314663995149915, gamma=0.03841507302952249\n",
      "Gradient Descent(4110/4999): loss=0.2866314991557184, w0=-0.31466399533619305, gamma=0.036010344861116705\n",
      "Gradient Descent(4111/4999): loss=0.28662849663573087, w0=-0.3146639955041025, gamma=0.07511955274224336\n",
      "Gradient Descent(4112/4999): loss=0.2866280284235954, w0=-0.3146639958417575, gamma=0.18624658706440433\n",
      "Gradient Descent(4113/4999): loss=0.2866274736792331, w0=-0.3146639966160307, gamma=0.2071860935074639\n",
      "Gradient Descent(4114/4999): loss=0.2866270894525368, w0=-0.314663997316936, gamma=0.04742197655498785\n",
      "Gradient Descent(4115/4999): loss=0.28662708439930373, w0=-0.3146639974441251, gamma=0.03524748823976019\n",
      "Gradient Descent(4116/4999): loss=0.28662707401877535, w0=-0.31466399753417823, gamma=0.035219116006958255\n",
      "Gradient Descent(4117/4999): loss=0.2866270716431525, w0=-0.3146639976209873, gamma=0.20126158236572153\n",
      "Gradient Descent(4118/4999): loss=0.286627070498427, w0=-0.3146639980995911, gamma=0.3061425339382874\n",
      "Gradient Descent(4119/4999): loss=0.2866270641531045, w0=-0.3146639986810828, gamma=0.14214279110672018\n",
      "Gradient Descent(4120/4999): loss=0.2866270549328364, w0=-0.3146639988684161, gamma=0.06652456132722877\n",
      "Gradient Descent(4121/4999): loss=0.2866270507318864, w0=-0.3146639989436281, gamma=0.05972598728051778\n",
      "Gradient Descent(4122/4999): loss=0.2866270487285268, w0=-0.3146639990066616, gamma=0.08472374730096219\n",
      "Gradient Descent(4123/4999): loss=0.28662704697203883, w0=-0.3146639990907367, gamma=0.8310360849058261\n",
      "Gradient Descent(4124/4999): loss=0.2866270444897246, w0=-0.3146639998455413, gamma=0.7047304612891933\n",
      "Gradient Descent(4125/4999): loss=0.2866270204168529, w0=-0.3146639999536923, gamma=0.037050876340010704\n",
      "Gradient Descent(4126/4999): loss=0.2866270032036415, w0=-0.3146639999553712, gamma=0.03543315662194177\n",
      "Gradient Descent(4127/4999): loss=0.28662699950220416, w0=-0.3146639999569173, gamma=0.042639813672389074\n",
      "Gradient Descent(4128/4999): loss=0.286626998488992, w0=-0.31466399995871197, gamma=0.06188153146168605\n",
      "Gradient Descent(4129/4999): loss=0.28662699729844565, w0=-0.3146639999612054, gamma=0.22778936241040326\n",
      "Gradient Descent(4130/4999): loss=0.2866269955749692, w0=-0.3146639999698159, gamma=2.5566885965983777\n",
      "Gradient Descent(4131/4999): loss=0.28662698924158336, w0=-0.31466400004444556, gamma=0.5482161763535984\n",
      "Gradient Descent(4132/4999): loss=0.2866269190340193, w0=-0.3146640000195347, gamma=0.049111386831993786\n",
      "Gradient Descent(4133/4999): loss=0.28662690941942015, w0=-0.3146640000185265, gamma=0.04505789910963797\n",
      "Gradient Descent(4134/4999): loss=0.28662690303589283, w0=-0.31466400001764694, gamma=0.03608176283102418\n",
      "Gradient Descent(4135/4999): loss=0.2866269016345689, w0=-0.3146640000169743, gamma=0.03567213526111121\n",
      "Gradient Descent(4136/4999): loss=0.2866269006457869, w0=-0.3146640000163333, gamma=0.06971075294167962\n",
      "Gradient Descent(4137/4999): loss=0.2866268996827027, w0=-0.3146640000151254, gamma=0.12848031572037286\n",
      "Gradient Descent(4138/4999): loss=0.2866268978012934, w0=-0.31466400001305433, gamma=1.417969150250312\n",
      "Gradient Descent(4139/4999): loss=0.2866268943345427, w0=-0.31466399999313394, gamma=0.9861799180327568\n",
      "Gradient Descent(4140/4999): loss=0.28662685610914834, w0=-0.31466399999892464, gamma=0.05261090762902796\n",
      "Gradient Descent(4141/4999): loss=0.28662682992797384, w0=-0.3146639999989289, gamma=0.04879826130830137\n",
      "Gradient Descent(4142/4999): loss=0.2866268281692797, w0=-0.3146639999989327, gamma=0.035600277027324094\n",
      "Gradient Descent(4143/4999): loss=0.28662682684942853, w0=-0.3146639999989353, gamma=0.0356884644985992\n",
      "Gradient Descent(4144/4999): loss=0.2866268258909428, w0=-0.3146639999989378, gamma=0.12564852175049274\n",
      "Gradient Descent(4145/4999): loss=0.28662682493144337, w0=-0.31466399999894634, gamma=0.2294813858095769\n",
      "Gradient Descent(4146/4999): loss=0.28662682155359304, w0=-0.31466399999896, gamma=0.5107893754460119\n",
      "Gradient Descent(4147/4999): loss=0.2866268153850534, w0=-0.3146639999989834, gamma=0.18923654484289143\n",
      "Gradient Descent(4148/4999): loss=0.2866268016578349, w0=-0.31466399999898764, gamma=0.07462224409507694\n",
      "Gradient Descent(4149/4999): loss=0.28662679657386414, w0=-0.31466399999898903, gamma=0.05667969823335919\n",
      "Gradient Descent(4150/4999): loss=0.28662679456796925, w0=-0.31466399999899, gamma=0.03697450409287756\n",
      "Gradient Descent(4151/4999): loss=0.28662679304511146, w0=-0.3146639999989906, gamma=0.038037916457767385\n",
      "Gradient Descent(4152/4999): loss=0.2866267920517254, w0=-0.3146639999989912, gamma=0.8092824059849724\n",
      "Gradient Descent(4153/4999): loss=0.286626791029828, w0=-0.31466399999900296, gamma=4.911874201144158\n",
      "Gradient Descent(4154/4999): loss=0.28662676929039144, w0=-0.3146639999990176, gamma=0.1823224058097558\n",
      "Gradient Descent(4155/4999): loss=0.28662663744197936, w0=-0.3146639999990155, gamma=0.03804035905297233\n",
      "Gradient Descent(4156/4999): loss=0.2866266327049475, w0=-0.3146639999990152, gamma=0.035398630942568836\n",
      "Gradient Descent(4157/4999): loss=0.2866266315165738, w0=-0.31466399999901484, gamma=0.04333746573627746\n",
      "Gradient Descent(4158/4999): loss=0.2866266305633009, w0=-0.31466399999901445, gamma=0.07791003432596377\n",
      "Gradient Descent(4159/4999): loss=0.28662662939966915, w0=-0.3146639999990138, gamma=0.08241902523238627\n",
      "Gradient Descent(4160/4999): loss=0.2866266273089054, w0=-0.3146639999990132, gamma=0.10758999736146595\n",
      "Gradient Descent(4161/4999): loss=0.2866266250973662, w0=-0.3146639999990124, gamma=0.1757230407614715\n",
      "Gradient Descent(4162/4999): loss=0.2866266222104277, w0=-0.3146639999990113, gamma=0.09443437392329439\n",
      "Gradient Descent(4163/4999): loss=0.2866266174953043, w0=-0.3146639999990108, gamma=0.04053759227302984\n",
      "Gradient Descent(4164/4999): loss=0.28662661496138486, w0=-0.31466399999901057, gamma=0.03682846769982867\n",
      "Gradient Descent(4165/4999): loss=0.28662661387365274, w0=-0.3146639999990104, gamma=0.09333630502956473\n",
      "Gradient Descent(4166/4999): loss=0.2866266128854507, w0=-0.31466399999901, gamma=6.322775693595299\n",
      "Gradient Descent(4167/4999): loss=0.2866266103810015, w0=-0.3146639999989847, gamma=4.920216061151511\n",
      "Gradient Descent(4168/4999): loss=0.2866264407305664, w0=-0.31466399999909056, gamma=0.037429987896427215\n",
      "Gradient Descent(4169/4999): loss=0.2866263092578435, w0=-0.3146639999990874, gamma=0.03605147478432055\n",
      "Gradient Descent(4170/4999): loss=0.2866263077318247, w0=-0.31466399999908445, gamma=0.04490111848290629\n",
      "Gradient Descent(4171/4999): loss=0.286626306755097, w0=-0.31466399999908096, gamma=0.053589794769106185\n",
      "Gradient Descent(4172/4999): loss=0.28662630554960505, w0=-0.31466399999907696, gamma=0.05589079425559885\n",
      "Gradient Descent(4173/4999): loss=0.2866263041118695, w0=-0.314663999999073, gamma=0.23344497787206941\n",
      "Gradient Descent(4174/4999): loss=0.2866263026124313, w0=-0.3146639999990575, gamma=0.33361870408971134\n",
      "Gradient Descent(4175/4999): loss=0.2866262963495994, w0=-0.31466399999904043, gamma=0.04511616748634187\n",
      "Gradient Descent(4176/4999): loss=0.2866262873994087, w0=-0.3146639999990389, gamma=0.035138203074375346\n",
      "Gradient Descent(4177/4999): loss=0.2866262861890205, w0=-0.31466399999903777, gamma=0.036301275637715395\n",
      "Gradient Descent(4178/4999): loss=0.28662628524634437, w0=-0.31466399999903666, gamma=22.43857633261943\n",
      "Gradient Descent(4179/4999): loss=0.286626284272469, w0=-0.31466399999836153, gamma=36.09328168495672\n",
      "Gradient Descent(4180/4999): loss=0.28662568233941005, w0=-0.314664000021673, gamma=0.13717775101814617\n",
      "Gradient Descent(4181/4999): loss=0.28662471777138837, w0=-0.3146640000185639, gamma=0.07234831205630315\n",
      "Gradient Descent(4182/4999): loss=0.2866247128118597, w0=-0.3146640000171491, gamma=0.05542305876963318\n",
      "Gradient Descent(4183/4999): loss=0.2866247094633302, w0=-0.3146640000161437, gamma=0.0359901905259424\n",
      "Gradient Descent(4184/4999): loss=0.28662470774559523, w0=-0.314664000015527, gamma=0.03712617871031804\n",
      "Gradient Descent(4185/4999): loss=0.286624706663435, w0=-0.31466400001491374, gamma=0.36628724015939024\n",
      "Gradient Descent(4186/4999): loss=0.28662470560612135, w0=-0.31466400000908784, gamma=0.42349570225089894\n",
      "Gradient Descent(4187/4999): loss=0.2866246954804346, w0=-0.3146640000048192, gamma=0.13010642006339068\n",
      "Gradient Descent(4188/4999): loss=0.28662468411990694, w0=-0.3146640000040632, gamma=0.039070375617051935\n",
      "Gradient Descent(4189/4999): loss=0.2866246806378972, w0=-0.3146640000038657, gamma=0.035124629139049915\n",
      "Gradient Descent(4190/4999): loss=0.2866246795803187, w0=-0.3146640000036951, gamma=0.037198947805213906\n",
      "Gradient Descent(4191/4999): loss=0.286624678638292, w0=-0.31466400000352074, gamma=0.10385339128534077\n",
      "Gradient Descent(4192/4999): loss=0.28662467764081034, w0=-0.3146640000030521, gamma=0.12999103720099198\n",
      "Gradient Descent(4193/4999): loss=0.28662467485606397, w0=-0.3146640000025265, gamma=0.5138276066051412\n",
      "Gradient Descent(4194/4999): loss=0.28662467137050246, w0=-0.31466400000071887, gamma=0.883970712735941\n",
      "Gradient Descent(4195/4999): loss=0.2866246575930037, w0=-0.3146639999992069, gamma=0.10074420759400383\n",
      "Gradient Descent(4196/4999): loss=0.2866246338918523, w0=-0.3146639999991869, gamma=0.06559070647951719\n",
      "Gradient Descent(4197/4999): loss=0.28662463119031173, w0=-0.3146639999991752, gamma=0.04154931226596157\n",
      "Gradient Descent(4198/4999): loss=0.2866246294316128, w0=-0.3146639999991683, gamma=0.03569589869891296\n",
      "Gradient Descent(4199/4999): loss=0.28662462831756264, w0=-0.31466399999916256, gamma=0.042601941441028475\n",
      "Gradient Descent(4200/4999): loss=0.2866246273604844, w0=-0.314663999999156, gamma=0.6435753919510588\n",
      "Gradient Descent(4201/4999): loss=0.28662462621824397, w0=-0.3146639999990611, gamma=1.8045164061907852\n",
      "Gradient Descent(4202/4999): loss=0.2866246089628179, w0=-0.31466399999896605, gamma=0.1364824478345514\n",
      "Gradient Descent(4203/4999): loss=0.28662456058166513, w0=-0.3146639999989718, gamma=0.06164994031166088\n",
      "Gradient Descent(4204/4999): loss=0.2866245569224583, w0=-0.3146639999989741, gamma=0.0432381739346994\n",
      "Gradient Descent(4205/4999): loss=0.28662455526923675, w0=-0.3146639999989756, gamma=0.03580218554129915\n",
      "Gradient Descent(4206/4999): loss=0.2866245541099151, w0=-0.31466399999897676, gamma=0.038102463791652524\n",
      "Gradient Descent(4207/4999): loss=0.2866245531500237, w0=-0.314663999998978, gamma=0.15259906217834274\n",
      "Gradient Descent(4208/4999): loss=0.28662455212846477, w0=-0.31466399999898265, gamma=0.31423321102414176\n",
      "Gradient Descent(4209/4999): loss=0.2866245480371616, w0=-0.31466399999899086, gamma=0.33999372746417944\n",
      "Gradient Descent(4210/4999): loss=0.28662453961233736, w0=-0.31466399999899686, gamma=0.0947225095382105\n",
      "Gradient Descent(4211/4999): loss=0.2866245304968949, w0=-0.31466399999899797, gamma=0.05922461253667146\n",
      "Gradient Descent(4212/4999): loss=0.2866245279573196, w0=-0.3146639999989986, gamma=0.03977232307840099\n",
      "Gradient Descent(4213/4999): loss=0.28662452636947167, w0=-0.31466399999899897, gamma=0.038050686062152186\n",
      "Gradient Descent(4214/4999): loss=0.28662452530315297, w0=-0.31466399999899936, gamma=0.1852701172084796\n",
      "Gradient Descent(4215/4999): loss=0.2866245242829936, w0=-0.3146639999990011, gamma=8.306301598403534\n",
      "Gradient Descent(4216/4999): loss=0.28662451931580585, w0=-0.3146639999990636, gamma=1.0013858716146025\n",
      "Gradient Descent(4217/4999): loss=0.286624296624457, w0=-0.3146639999990088, gamma=0.03639594837274441\n",
      "Gradient Descent(4218/4999): loss=0.2866242698898958, w0=-0.3146639999990088, gamma=0.03538931353497715\n",
      "Gradient Descent(4219/4999): loss=0.28662426880488817, w0=-0.3146639999990088, gamma=0.05356833892526799\n",
      "Gradient Descent(4220/4999): loss=0.2866242678547523, w0=-0.3146639999990088, gamma=0.07197618810020646\n",
      "Gradient Descent(4221/4999): loss=0.28662426641828437, w0=-0.3146639999990088, gamma=0.07966336316074657\n",
      "Gradient Descent(4222/4999): loss=0.2866242644886964, w0=-0.3146639999990088, gamma=0.15345152643177976\n",
      "Gradient Descent(4223/4999): loss=0.286624262353071, w0=-0.31466399999900885, gamma=0.37092528519574763\n",
      "Gradient Descent(4224/4999): loss=0.2866242582393279, w0=-0.3146639999990089, gamma=0.06446034429185145\n",
      "Gradient Descent(4225/4999): loss=0.2866242482955558, w0=-0.3146639999990089, gamma=0.03534958848997603\n",
      "Gradient Descent(4226/4999): loss=0.28662424656750224, w0=-0.3146639999990089, gamma=0.035481359292708195\n",
      "Gradient Descent(4227/4999): loss=0.2866242456198461, w0=-0.3146639999990089, gamma=5.248728835706462\n",
      "Gradient Descent(4228/4999): loss=0.2866242446686638, w0=-0.31466399999900885, gamma=118.17975858565931\n",
      "Gradient Descent(4229/4999): loss=0.28662410396282134, w0=-0.3146639999990581, gamma=1.2851212312192626\n",
      "Gradient Descent(4230/4999): loss=0.2866209368729535, w0=-0.3146639999990003, gamma=0.03634594961326204\n",
      "Gradient Descent(4231/4999): loss=0.2866209314798761, w0=-0.3146639999990008, gamma=0.03552832520241829\n",
      "Gradient Descent(4232/4999): loss=0.2866209018274794, w0=-0.31466399999900124, gamma=0.04845096106784985\n",
      "Gradient Descent(4233/4999): loss=0.28662090057152534, w0=-0.3146639999990018, gamma=0.05788915390911084\n",
      "Gradient Descent(4234/4999): loss=0.2866208992237166, w0=-0.31466399999900246, gamma=0.11014078851000088\n",
      "Gradient Descent(4235/4999): loss=0.28662089766232274, w0=-0.31466399999900363, gamma=0.23529724988208853\n",
      "Gradient Descent(4236/4999): loss=0.2866208947026584, w0=-0.31466399999900585, gamma=0.21252799721029522\n",
      "Gradient Descent(4237/4999): loss=0.2866208883965064, w0=-0.3146639999990074, gamma=0.04012689402495605\n",
      "Gradient Descent(4238/4999): loss=0.2866208827072149, w0=-0.3146639999990076, gamma=0.036075931271168836\n",
      "Gradient Descent(4239/4999): loss=0.2866208816305521, w0=-0.3146639999990078, gamma=0.04084510955161092\n",
      "Gradient Descent(4240/4999): loss=0.2866208806644046, w0=-0.314663999999008, gamma=0.07751631767514644\n",
      "Gradient Descent(4241/4999): loss=0.2866208795706192, w0=-0.3146639999990084, gamma=0.12843317038497812\n",
      "Gradient Descent(4242/4999): loss=0.286620877494844, w0=-0.314663999999009, gamma=1.2774092034666744\n",
      "Gradient Descent(4243/4999): loss=0.2866208740556099, w0=-0.31466399999901423, gamma=0.6468656009959259\n",
      "Gradient Descent(4244/4999): loss=0.2866208398493618, w0=-0.31466399999901346, gamma=0.04051613059853814\n",
      "Gradient Descent(4245/4999): loss=0.2866208225339581, w0=-0.31466399999901346, gamma=0.037356598490939294\n",
      "Gradient Descent(4246/4999): loss=0.28662082144364925, w0=-0.31466399999901346, gamma=0.04030226942421363\n",
      "Gradient Descent(4247/4999): loss=0.2866208204431619, w0=-0.31466399999901346, gamma=0.055727535325940984\n",
      "Gradient Descent(4248/4999): loss=0.28662081936399136, w0=-0.31466399999901346, gamma=0.0775048041948929\n",
      "Gradient Descent(4249/4999): loss=0.28662081787179816, w0=-0.31466399999901346, gamma=0.460128932069894\n",
      "Gradient Descent(4250/4999): loss=0.2866208157964884, w0=-0.31466399999901334, gamma=0.7309479083609695\n",
      "Gradient Descent(4251/4999): loss=0.2866208034758672, w0=-0.3146639999990133, gamma=0.04074775717500638\n",
      "Gradient Descent(4252/4999): loss=0.2866207839040881, w0=-0.3146639999990133, gamma=0.03508665947334103\n",
      "Gradient Descent(4253/4999): loss=0.2866207828127568, w0=-0.3146639999990133, gamma=0.03565004521487001\n",
      "Gradient Descent(4254/4999): loss=0.2866207818732667, w0=-0.3146639999990133, gamma=0.504993109213244\n",
      "Gradient Descent(4255/4999): loss=0.28662078091869886, w0=-0.31466399999901323, gamma=1.61135393857738\n",
      "Gradient Descent(4256/4999): loss=0.28662076739699904, w0=-0.31466399999901307, gamma=0.12002327316713549\n",
      "Gradient Descent(4257/4999): loss=0.28662072425193386, w0=-0.3146639999990131, gamma=0.060883308337580405\n",
      "Gradient Descent(4258/4999): loss=0.28662072103822483, w0=-0.3146639999990131, gamma=0.05629434386626899\n",
      "Gradient Descent(4259/4999): loss=0.2866207194078809, w0=-0.3146639999990131, gamma=0.06120790486543978\n",
      "Gradient Descent(4260/4999): loss=0.2866207179005727, w0=-0.3146639999990131, gamma=0.2995452887413837\n",
      "Gradient Descent(4261/4999): loss=0.2866207162617062, w0=-0.3146639999990132, gamma=1.1704037924611341\n",
      "Gradient Descent(4262/4999): loss=0.2866207082412704, w0=-0.3146639999990132, gamma=0.0708083304206659\n",
      "Gradient Descent(4263/4999): loss=0.2866206769035124, w0=-0.3146639999990132, gamma=0.035325119740911765\n",
      "Gradient Descent(4264/4999): loss=0.28662067500761684, w0=-0.3146639999990132, gamma=0.03507220038049032\n",
      "Gradient Descent(4265/4999): loss=0.28662067406170444, w0=-0.3146639999990132, gamma=0.31846623348131864\n",
      "Gradient Descent(4266/4999): loss=0.2866206731226465, w0=-0.31466399999901323, gamma=3.798488133989164\n",
      "Gradient Descent(4267/4999): loss=0.28662066459572394, w0=-0.3146639999990136, gamma=0.29940040978850724\n",
      "Gradient Descent(4268/4999): loss=0.2866205628926577, w0=-0.3146639999990136, gamma=0.0551827513171125\n",
      "Gradient Descent(4269/4999): loss=0.28662055488008353, w0=-0.3146639999990136, gamma=0.053952184199017414\n",
      "Gradient Descent(4270/4999): loss=0.2866205533988904, w0=-0.3146639999990136, gamma=0.07866946944784921\n",
      "Gradient Descent(4271/4999): loss=0.2866205519543553, w0=-0.3146639999990136, gamma=0.10249157521877218\n",
      "Gradient Descent(4272/4999): loss=0.28662054984806373, w0=-0.3146639999990136, gamma=0.09180454337632919\n",
      "Gradient Descent(4273/4999): loss=0.28662054710397006, w0=-0.3146639999990136, gamma=0.04958531903832539\n",
      "Gradient Descent(4274/4999): loss=0.2866205446460119, w0=-0.3146639999990136, gamma=0.0405720747082588\n",
      "Gradient Descent(4275/4999): loss=0.28662054331842407, w0=-0.3146639999990136, gamma=0.06713140086339817\n",
      "Gradient Descent(4276/4999): loss=0.2866205422321558, w0=-0.3146639999990136, gamma=1.3182043164414052\n",
      "Gradient Descent(4277/4999): loss=0.2866205404347946, w0=-0.31466399999901334, gamma=1.5633527656894266\n",
      "Gradient Descent(4278/4999): loss=0.28662050514161136, w0=-0.3146639999990136, gamma=0.03617113298896215\n",
      "Gradient Descent(4279/4999): loss=0.28662046328984403, w0=-0.3146639999990136, gamma=0.03503254242171404\n",
      "Gradient Descent(4280/4999): loss=0.2866204623170198, w0=-0.3146639999990136, gamma=0.03559877902837502\n",
      "Gradient Descent(4281/4999): loss=0.2866204613790873, w0=-0.3146639999990136, gamma=171.74179042488237\n",
      "Gradient Descent(4282/4999): loss=0.28662046042599987, w0=-0.3146639999990037, gamma=249.1346137055579\n",
      "Gradient Descent(4283/4999): loss=0.2866158641932501, w0=-0.31466400000287176, gamma=0.19229636162286862\n",
      "Gradient Descent(4284/4999): loss=0.28661108987206957, w0=-0.3146640000021295, gamma=0.0881944798002442\n",
      "Gradient Descent(4285/4999): loss=0.2866101254167168, w0=-0.3146640000018558, gamma=0.0644186895207579\n",
      "Gradient Descent(4286/4999): loss=0.2866093665354022, w0=-0.3146640000016734, gamma=0.056776435089620436\n",
      "Gradient Descent(4287/4999): loss=0.28660923389043125, w0=-0.31466400000152306, gamma=0.06806577877302183\n",
      "Gradient Descent(4288/4999): loss=0.28660921683935214, w0=-0.3146640000013531, gamma=0.22203619420628382\n",
      "Gradient Descent(4289/4999): loss=0.286609206489323, w0=-0.31466400000083644, gamma=0.2248556696116283\n",
      "Gradient Descent(4290/4999): loss=0.2866091905018423, w0=-0.3146640000004296, gamma=0.03639420473643957\n",
      "Gradient Descent(4291/4999): loss=0.28660918822181525, w0=-0.3146640000003786, gamma=0.03514376428704464\n",
      "Gradient Descent(4292/4999): loss=0.2866091831156207, w0=-0.3146640000003311, gamma=0.040955481968589294\n",
      "Gradient Descent(4293/4999): loss=0.28660918216057196, w0=-0.3146640000002777, gamma=0.062169965067963205\n",
      "Gradient Descent(4294/4999): loss=0.2866091810636227, w0=-0.3146640000002, gamma=0.1220537093344294\n",
      "Gradient Descent(4295/4999): loss=0.28660917940073366, w0=-0.3146640000000569, gamma=0.3154200497333874\n",
      "Gradient Descent(4296/4999): loss=0.2866091761377382, w0=-0.3146639999997322, gamma=0.3780496635149645\n",
      "Gradient Descent(4297/4999): loss=0.28660916770967093, w0=-0.3146639999994658, gamma=0.07205313963157478\n",
      "Gradient Descent(4298/4999): loss=0.28660915761476075, w0=-0.31466399999943423, gamma=0.05293295569226328\n",
      "Gradient Descent(4299/4999): loss=0.2866091556890134, w0=-0.3146639999994127, gamma=0.04821353643629693\n",
      "Gradient Descent(4300/4999): loss=0.28660915427547623, w0=-0.31466399999939415, gamma=0.052812428750129176\n",
      "Gradient Descent(4301/4999): loss=0.286609152988267, w0=-0.3146639999993748, gamma=0.15154547401146723\n",
      "Gradient Descent(4302/4999): loss=0.28660915157833744, w0=-0.31466399999932215, gamma=0.4254262862585494\n",
      "Gradient Descent(4303/4999): loss=0.2866091475327676, w0=-0.3146639999991969, gamma=0.07138910044149278\n",
      "Gradient Descent(4304/4999): loss=0.28660913617802736, w0=-0.3146639999991848, gamma=0.035246941837094016\n",
      "Gradient Descent(4305/4999): loss=0.28660913427293594, w0=-0.31466399999917927, gamma=0.035221646932621836\n",
      "Gradient Descent(4306/4999): loss=0.28660913333169846, w0=-0.31466399999917394, gamma=1.8377173668957174\n",
      "Gradient Descent(4307/4999): loss=0.28660913239179, w0=-0.314663999998905, gamma=2.8873433774404176\n",
      "Gradient Descent(4308/4999): loss=0.286609083361047, w0=-0.3146639999992597, gamma=1.2587920050546515\n",
      "Gradient Descent(4309/4999): loss=0.2866090063545315, w0=-0.3146639999989678, gamma=0.05642016123044048\n",
      "Gradient Descent(4310/4999): loss=0.28660897281672437, w0=-0.3146639999989712, gamma=0.0379538852905753\n",
      "Gradient Descent(4311/4999): loss=0.2866089712887936, w0=-0.3146639999989734, gamma=0.03666788231310504\n",
      "Gradient Descent(4312/4999): loss=0.28660897026886306, w0=-0.3146639999989754, gamma=0.07537252781596468\n",
      "Gradient Descent(4313/4999): loss=0.2866089692900959, w0=-0.3146639999989793, gamma=0.10489557452900544\n",
      "Gradient Descent(4314/4999): loss=0.2866089672794414, w0=-0.3146639999989844, gamma=0.11442564618739648\n",
      "Gradient Descent(4315/4999): loss=0.28660896448197387, w0=-0.3146639999989894, gamma=0.06834025262229562\n",
      "Gradient Descent(4316/4999): loss=0.28660896143042913, w0=-0.3146639999989921, gamma=0.0383700422172802\n",
      "Gradient Descent(4317/4999): loss=0.286608959607914, w0=-0.31466399999899347, gamma=0.03780255385554088\n",
      "Gradient Descent(4318/4999): loss=0.2866089585846517, w0=-0.3146639999989948, gamma=0.22917029506676842\n",
      "Gradient Descent(4319/4999): loss=0.28660895757652605, w0=-0.3146639999990024, gamma=0.8504739920521627\n",
      "Gradient Descent(4320/4999): loss=0.2866089514649787, w0=-0.31466399999902406, gamma=0.9312426173928049\n",
      "Gradient Descent(4321/4999): loss=0.28660892878452743, w0=-0.3146639999990279, gamma=0.03675635267148026\n",
      "Gradient Descent(4322/4999): loss=0.2866089039516841, w0=-0.3146639999990279, gamma=0.03503525813009049\n",
      "Gradient Descent(4323/4999): loss=0.28660890297018876, w0=-0.3146639999990279, gamma=0.036241217359929044\n",
      "Gradient Descent(4324/4999): loss=0.28660890203588196, w0=-0.3146639999990279, gamma=1.6885011881491925\n",
      "Gradient Descent(4325/4999): loss=0.2866089010694189, w0=-0.31466399999902817, gamma=2.424367713443115\n",
      "Gradient Descent(4326/4999): loss=0.2866088560415436, w0=-0.31466399999902817, gamma=0.21727352348547682\n",
      "Gradient Descent(4327/4999): loss=0.28660879139279144, w0=-0.3146639999990282, gamma=0.07448884903401637\n",
      "Gradient Descent(4328/4999): loss=0.28660878559891995, w0=-0.3146639999990282, gamma=0.055056604189640757\n",
      "Gradient Descent(4329/4999): loss=0.2866087836113255, w0=-0.3146639999990282, gamma=0.05457303456213599\n",
      "Gradient Descent(4330/4999): loss=0.2866087821429706, w0=-0.3146639999990282, gamma=0.12825412505291514\n",
      "Gradient Descent(4331/4999): loss=0.2866087806877062, w0=-0.3146639999990282, gamma=0.20722805910498404\n",
      "Gradient Descent(4332/4999): loss=0.28660877726764933, w0=-0.3146639999990282, gamma=0.30260033798440195\n",
      "Gradient Descent(4333/4999): loss=0.2866087717416711, w0=-0.31466399999902817, gamma=0.04975028481375386\n",
      "Gradient Descent(4334/4999): loss=0.2866087636725127, w0=-0.31466399999902817, gamma=0.03532055755055762\n",
      "Gradient Descent(4335/4999): loss=0.28660876234585986, w0=-0.31466399999902817, gamma=0.03644756373434798\n",
      "Gradient Descent(4336/4999): loss=0.2866087614039988, w0=-0.31466399999902817, gamma=2.765252068190792\n",
      "Gradient Descent(4337/4999): loss=0.28660876043208755, w0=-0.31466399999902844, gamma=7.325342787371591\n",
      "Gradient Descent(4338/4999): loss=0.28660868669438394, w0=-0.3146639999990268, gamma=0.11322360041952487\n",
      "Gradient Descent(4339/4999): loss=0.2866084913976375, w0=-0.314663999999027, gamma=0.09902319630472509\n",
      "Gradient Descent(4340/4999): loss=0.28660848834747027, w0=-0.31466399999902717, gamma=0.038754569155384104\n",
      "Gradient Descent(4341/4999): loss=0.2866084857075228, w0=-0.3146639999990272, gamma=0.03522195863772488\n",
      "Gradient Descent(4342/4999): loss=0.2866084846731345, w0=-0.3146639999990273, gamma=0.0367991053189634\n",
      "Gradient Descent(4343/4999): loss=0.2866084837339844, w0=-0.31466399999902733, gamma=0.0974130072417469\n",
      "Gradient Descent(4344/4999): loss=0.28660848275279704, w0=-0.31466399999902744, gamma=0.24939508466220647\n",
      "Gradient Descent(4345/4999): loss=0.28660848015544366, w0=-0.31466399999902767, gamma=0.42403966083564626\n",
      "Gradient Descent(4346/4999): loss=0.2866084735057566, w0=-0.31466399999902805, gamma=0.08784356122116775\n",
      "Gradient Descent(4347/4999): loss=0.2866084621995203, w0=-0.3146639999990281, gamma=0.05439196773799462\n",
      "Gradient Descent(4348/4999): loss=0.2866084598573301, w0=-0.31466399999902817, gamma=0.055013754531537\n",
      "Gradient Descent(4349/4999): loss=0.286608458407067, w0=-0.31466399999902817, gamma=0.32510764766217154\n",
      "Gradient Descent(4350/4999): loss=0.2866084569402314, w0=-0.3146639999990283, gamma=0.9536256366301288\n",
      "Gradient Descent(4351/4999): loss=0.28660844827187326, w0=-0.31466399999902855, gamma=0.04851122496502807\n",
      "Gradient Descent(4352/4999): loss=0.28660842284556615, w0=-0.31466399999902855, gamma=0.03505264288772242\n",
      "Gradient Descent(4353/4999): loss=0.2866084215520075, w0=-0.31466399999902855, gamma=0.035165793297566195\n",
      "Gradient Descent(4354/4999): loss=0.2866084206173882, w0=-0.31466399999902855, gamma=12.625528135018387\n",
      "Gradient Descent(4355/4999): loss=0.28660841967977246, w0=-0.3146639999990297, gamma=26.850029575520963\n",
      "Gradient Descent(4356/4999): loss=0.2866080830589111, w0=-0.3146639999990139, gamma=0.11280680527542691\n",
      "Gradient Descent(4357/4999): loss=0.2866073693356648, w0=-0.31466399999901573, gamma=0.08986909079525315\n",
      "Gradient Descent(4358/4999): loss=0.2866073645439187, w0=-0.314663999999017, gamma=0.0793100502881295\n",
      "Gradient Descent(4359/4999): loss=0.2866073619162638, w0=-0.31466399999901806, gamma=0.06584683045804286\n",
      "Gradient Descent(4360/4999): loss=0.28660735978238916, w0=-0.31466399999901884, gamma=0.05788349697697568\n",
      "Gradient Descent(4361/4999): loss=0.2866073580232327, w0=-0.3146639999990195, gamma=0.05082974931095055\n",
      "Gradient Descent(4362/4999): loss=0.2866073564794146, w0=-0.31466399999902006, gamma=0.04984312020934173\n",
      "Gradient Descent(4363/4999): loss=0.2866073551242754, w0=-0.31466399999902056, gamma=0.06446264550230725\n",
      "Gradient Descent(4364/4999): loss=0.2866073537956481, w0=-0.31466399999902117, gamma=0.09169163000182239\n",
      "Gradient Descent(4365/4999): loss=0.28660735207744154, w0=-0.31466399999902195, gamma=0.07281564082280446\n",
      "Gradient Descent(4366/4999): loss=0.28660734963355655, w0=-0.31466399999902256, gamma=0.03932794437887546\n",
      "Gradient Descent(4367/4999): loss=0.2866073476928053, w0=-0.31466399999902284, gamma=0.036660228780042776\n",
      "Gradient Descent(4368/4999): loss=0.28660734664459875, w0=-0.3146639999990231, gamma=0.0992956926680919\n",
      "Gradient Descent(4369/4999): loss=0.28660734566749907, w0=-0.3146639999990238, gamma=0.4281876173234198\n",
      "Gradient Descent(4370/4999): loss=0.2866073430209878, w0=-0.3146639999990264, gamma=1.4234058499755093\n",
      "Gradient Descent(4371/4999): loss=0.28660733160859914, w0=-0.3146639999990316, gamma=0.08941591231420797\n",
      "Gradient Descent(4372/4999): loss=0.28660729367122234, w0=-0.3146639999990315, gamma=0.0354089824630447\n",
      "Gradient Descent(4373/4999): loss=0.2866072912881786, w0=-0.31466399999903144, gamma=0.03507322815919982\n",
      "Gradient Descent(4374/4999): loss=0.28660729034424326, w0=-0.3146639999990314, gamma=0.15544751724409098\n",
      "Gradient Descent(4375/4999): loss=0.28660728940945945, w0=-0.31466399999903116, gamma=0.319663943558047\n",
      "Gradient Descent(4376/4999): loss=0.2866072852664235, w0=-0.3146639999990308, gamma=20.877608529875793\n",
      "Gradient Descent(4377/4999): loss=0.2866072767466588, w0=-0.3146639999990161, gamma=64.2785585069806\n",
      "Gradient Descent(4378/4999): loss=0.2866067203391758, w0=-0.3146639999999659, gamma=0.038450853950913466\n",
      "Gradient Descent(4379/4999): loss=0.28660504148959176, w0=-0.31466399999993006, gamma=0.03519876618329956\n",
      "Gradient Descent(4380/4999): loss=0.2866050090868393, w0=-0.31466399999989847, gamma=0.0424450079336127\n",
      "Gradient Descent(4381/4999): loss=0.2866050071322402, w0=-0.3146639999998617, gamma=0.1650792333344637\n",
      "Gradient Descent(4382/4999): loss=0.2866050054613644, w0=-0.31466399999972494, gamma=0.17515671483787035\n",
      "Gradient Descent(4383/4999): loss=0.28660500030835145, w0=-0.31466399999960376, gamma=0.1398978590160123\n",
      "Gradient Descent(4384/4999): loss=0.28660499560836455, w0=-0.3146639999995239, gamma=0.0550764992272709\n",
      "Gradient Descent(4385/4999): loss=0.28660499188350536, w0=-0.31466399999949685, gamma=0.049345166622124494\n",
      "Gradient Descent(4386/4999): loss=0.2866049904095067, w0=-0.3146639999994739, gamma=0.03700698094364455\n",
      "Gradient Descent(4387/4999): loss=0.2866049890949971, w0=-0.3146639999994576, gamma=0.04227139143848622\n",
      "Gradient Descent(4388/4999): loss=0.28660498810930946, w0=-0.31466399999943967, gamma=0.2409093297713718\n",
      "Gradient Descent(4389/4999): loss=0.28660498698347414, w0=-0.3146639999993417, gamma=0.26822568353785503\n",
      "Gradient Descent(4390/4999): loss=0.28660498056753586, w0=-0.31466399999925887, gamma=0.059119586806172936\n",
      "Gradient Descent(4391/4999): loss=0.28660497342437746, w0=-0.3146639999992455, gamma=0.03545601831110738\n",
      "Gradient Descent(4392/4999): loss=0.2866049718499537, w0=-0.31466399999923794, gamma=0.035887379719191696\n",
      "Gradient Descent(4393/4999): loss=0.2866049709057178, w0=-0.31466399999923056, gamma=5.663605968654288\n",
      "Gradient Descent(4394/4999): loss=0.2866049699499976, w0=-0.31466399999811123, gamma=321.4482567276935\n",
      "Gradient Descent(4395/4999): loss=0.2866048191239976, w0=-0.3146640002944494, gamma=0.19508902442516216\n",
      "Gradient Descent(4396/4999): loss=0.2865962678477737, w0=-0.31466400023681906, gamma=0.042991836580028454\n",
      "Gradient Descent(4397/4999): loss=0.28659628167552537, w0=-0.3146640002265967, gamma=0.038247075988244575\n",
      "Gradient Descent(4398/4999): loss=0.2865962600717358, w0=-0.3146640002178935, gamma=0.03769187226491959\n",
      "Gradient Descent(4399/4999): loss=0.28659625807746303, w0=-0.31466400020964463, gamma=0.048411994575114255\n",
      "Gradient Descent(4400/4999): loss=0.2865962570354631, w0=-0.31466400019944907, gamma=0.06492395871253936\n",
      "Gradient Descent(4401/4999): loss=0.2865962557422494, w0=-0.314664000186438, gamma=0.1143295419291272\n",
      "Gradient Descent(4402/4999): loss=0.286596254015571, w0=-0.31466400016501334, gamma=0.13075513544005474\n",
      "Gradient Descent(4403/4999): loss=0.2865962509781105, w0=-0.314664000143312, gamma=0.04745901664306448\n",
      "Gradient Descent(4404/4999): loss=0.2865962475055251, w0=-0.31466400013646517, gamma=0.03568835424778438\n",
      "Gradient Descent(4405/4999): loss=0.28659624624503327, w0=-0.3146640001315608, gamma=0.03858096559595964\n",
      "Gradient Descent(4406/4999): loss=0.28659624529724365, w0=-0.3146640001264482, gamma=0.6611866967714501\n",
      "Gradient Descent(4407/4999): loss=0.2865962442726581, w0=-0.31466400004221035, gamma=0.8626344372690002\n",
      "Gradient Descent(4408/4999): loss=0.2865962267142547, w0=-0.3146640000049736, gamma=0.30838978380211834\n",
      "Gradient Descent(4409/4999): loss=0.2865962038081041, w0=-0.31466400000314504, gamma=0.056453532531375084\n",
      "Gradient Descent(4410/4999): loss=0.28659619562033456, w0=-0.31466400000291356, gamma=0.03793632591039274\n",
      "Gradient Descent(4411/4999): loss=0.2865961941205287, w0=-0.3146640000027668, gamma=0.035541685849265536\n",
      "Gradient Descent(4412/4999): loss=0.2865961931129781, w0=-0.3146640000026345, gamma=0.05851584070278658\n",
      "Gradient Descent(4413/4999): loss=0.28659619216925397, w0=-0.3146640000024244, gamma=0.16168850284425532\n",
      "Gradient Descent(4414/4999): loss=0.2865961906155204, w0=-0.31466400000187783, gamma=0.35988352545243013\n",
      "Gradient Descent(4415/4999): loss=0.28659618632233913, w0=-0.3146640000008581, gamma=0.7684909103426305\n",
      "Gradient Descent(4416/4999): loss=0.2865961767667362, w0=-0.3146639999994641, gamma=0.0848460588505624\n",
      "Gradient Descent(4417/4999): loss=0.28659615636230973, w0=-0.31466399999942846, gamma=0.03984766783528618\n",
      "Gradient Descent(4418/4999): loss=0.2865961541095255, w0=-0.31466399999941314, gamma=0.03510429532779282\n",
      "Gradient Descent(4419/4999): loss=0.2865961530513808, w0=-0.3146639999994002, gamma=0.03751016081038523\n",
      "Gradient Descent(4420/4999): loss=0.28659615211932316, w0=-0.3146639999993868, gamma=0.9556623578115775\n",
      "Gradient Descent(4421/4999): loss=0.28659615112339104, w0=-0.31466399999905953, gamma=2.233396837345372\n",
      "Gradient Descent(4422/4999): loss=0.28659612574975185, w0=-0.31466399999902556, gamma=0.10975823990553044\n",
      "Gradient Descent(4423/4999): loss=0.28659606645412583, w0=-0.3146639999990276, gamma=0.07716924306824265\n",
      "Gradient Descent(4424/4999): loss=0.28659606353872336, w0=-0.3146639999990289, gamma=0.05843885848352746\n",
      "Gradient Descent(4425/4999): loss=0.286596061489674, w0=-0.3146639999990298, gamma=0.051834776814856265\n",
      "Gradient Descent(4426/4999): loss=0.2865960599381154, w0=-0.31466399999903055, gamma=0.04328985564922383\n",
      "Gradient Descent(4427/4999): loss=0.28659605856192155, w0=-0.31466399999903116, gamma=0.06267344574617512\n",
      "Gradient Descent(4428/4999): loss=0.28659605741259336, w0=-0.314663999999032, gamma=0.46005417158817624\n",
      "Gradient Descent(4429/4999): loss=0.2865960557486395, w0=-0.31466399999903766, gamma=0.7167835201300065\n",
      "Gradient Descent(4430/4999): loss=0.2865960435344149, w0=-0.31466399999904243, gamma=0.04013147294503282\n",
      "Gradient Descent(4431/4999): loss=0.2865960245044576, w0=-0.3146639999990425, gamma=0.03504973829729351\n",
      "Gradient Descent(4432/4999): loss=0.28659602343878676, w0=-0.31466399999904254, gamma=0.035743856656074345\n",
      "Gradient Descent(4433/4999): loss=0.286596022508238, w0=-0.3146639999990426, gamma=1.1818536903534538\n",
      "Gradient Descent(4434/4999): loss=0.2865960215592656, w0=-0.3146639999990447, gamma=2.145476287185157\n",
      "Gradient Descent(4435/4999): loss=0.2865959901820525, w0=-0.31466399999904415, gamma=0.14905264457815842\n",
      "Gradient Descent(4436/4999): loss=0.2865959332232752, w0=-0.3146639999990442, gamma=0.10707630026397655\n",
      "Gradient Descent(4437/4999): loss=0.28659592926529237, w0=-0.31466399999904426, gamma=0.08987351627025741\n",
      "Gradient Descent(4438/4999): loss=0.28659592642243703, w0=-0.31466399999904426, gamma=0.06081981735870631\n",
      "Gradient Descent(4439/4999): loss=0.2865959240364373, w0=-0.31466399999904426, gamma=0.05332257253261416\n",
      "Gradient Descent(4440/4999): loss=0.28659592242177123, w0=-0.31466399999904426, gamma=0.06208437815706099\n",
      "Gradient Descent(4441/4999): loss=0.2865959210061474, w0=-0.31466399999904426, gamma=0.12790783959045438\n",
      "Gradient Descent(4442/4999): loss=0.2865959193579135, w0=-0.31466399999904426, gamma=0.18368623626309266\n",
      "Gradient Descent(4443/4999): loss=0.28659591596218237, w0=-0.3146639999990443, gamma=0.05601669100828897\n",
      "Gradient Descent(4444/4999): loss=0.28659591108564064, w0=-0.3146639999990443, gamma=0.03617857357782137\n",
      "Gradient Descent(4445/4999): loss=0.2865959095984974, w0=-0.3146639999990443, gamma=0.03822478499443772\n",
      "Gradient Descent(4446/4999): loss=0.2865959086380215, w0=-0.3146639999990443, gamma=2.971720157631069\n",
      "Gradient Descent(4447/4999): loss=0.28659590762322334, w0=-0.3146639999990452, gamma=150.63611544863633\n",
      "Gradient Descent(4448/4999): loss=0.2865958287300579, w0=-0.3146639999989549, gamma=0.08830156618564669\n",
      "Gradient Descent(4449/4999): loss=0.28659183216972506, w0=-0.3146639999989633, gamma=0.03509142939410845\n",
      "Gradient Descent(4450/4999): loss=0.28659183102339275, w0=-0.3146639999989663, gamma=0.03504307382690362\n",
      "Gradient Descent(4451/4999): loss=0.2865918278740812, w0=-0.31466399999896927, gamma=0.24757278837663713\n",
      "Gradient Descent(4452/4999): loss=0.2865918269364438, w0=-0.3146639999989892, gamma=0.2645803347233064\n",
      "Gradient Descent(4453/4999): loss=0.28659182034686836, w0=-0.31466399999900524, gamma=0.19606131172824354\n",
      "Gradient Descent(4454/4999): loss=0.2865918133323195, w0=-0.31466399999901395, gamma=0.08302513627204801\n",
      "Gradient Descent(4455/4999): loss=0.28659180813445473, w0=-0.31466399999901695, gamma=0.07710781396793125\n",
      "Gradient Descent(4456/4999): loss=0.28659180593331474, w0=-0.31466399999901945, gamma=0.08041286292653889\n",
      "Gradient Descent(4457/4999): loss=0.2865918038890788, w0=-0.3146639999990219, gamma=0.09558731633647001\n",
      "Gradient Descent(4458/4999): loss=0.2865918017572243, w0=-0.31466399999902456, gamma=0.07723385239636078\n",
      "Gradient Descent(4459/4999): loss=0.2865917992230778, w0=-0.3146639999990265, gamma=0.049438451451070495\n",
      "Gradient Descent(4460/4999): loss=0.2865917971755087, w0=-0.31466399999902767, gamma=0.0448792539280363\n",
      "Gradient Descent(4461/4999): loss=0.28659179586483247, w0=-0.31466399999902867, gamma=0.0928313600669774\n",
      "Gradient Descent(4462/4999): loss=0.2865917946750274, w0=-0.3146639999990306, gamma=0.7436847924107971\n",
      "Gradient Descent(4463/4999): loss=0.2865917922139542, w0=-0.3146639999990448, gamma=0.29840478847788104\n",
      "Gradient Descent(4464/4999): loss=0.28659177249803847, w0=-0.3146639999990463, gamma=0.03564473181572678\n",
      "Gradient Descent(4465/4999): loss=0.28659176458741076, w0=-0.31466399999904643, gamma=0.03504256652114046\n",
      "Gradient Descent(4466/4999): loss=0.28659176364208444, w0=-0.31466399999904654, gamma=0.06960171891592484\n",
      "Gradient Descent(4467/4999): loss=0.28659176271307796, w0=-0.31466399999904676, gamma=29.60149323662939\n",
      "Gradient Descent(4468/4999): loss=0.286591760867882, w0=-0.31466399999913636, gamma=30.444792257913786\n",
      "Gradient Descent(4469/4999): loss=0.28659097618605706, w0=-0.3146639999965372, gamma=0.20720812873384817\n",
      "Gradient Descent(4470/4999): loss=0.2865901695198314, w0=-0.31466399999705824, gamma=0.04947673445370288\n",
      "Gradient Descent(4471/4999): loss=0.28659016460469106, w0=-0.3146639999971569, gamma=0.04502347195286711\n",
      "Gradient Descent(4472/4999): loss=0.28659016266376447, w0=-0.3146639999972422, gamma=0.03577250409017714\n",
      "Gradient Descent(4473/4999): loss=0.28659016144649246, w0=-0.31466399999730693, gamma=0.035373123992637964\n",
      "Gradient Descent(4474/4999): loss=0.2865901604956735, w0=-0.31466399999736866, gamma=0.0972425996278052\n",
      "Gradient Descent(4475/4999): loss=0.28659015955728234, w0=-0.3146639999975323, gamma=0.43698572527344287\n",
      "Gradient Descent(4476/4999): loss=0.2865901569776954, w0=-0.3146639999981963, gamma=0.6563464429913346\n",
      "Gradient Descent(4477/4999): loss=0.2865901453867765, w0=-0.3146639999987577, gamma=0.06562964941019839\n",
      "Gradient Descent(4478/4999): loss=0.2865901279856433, w0=-0.31466399999877703, gamma=0.0535844289468438\n",
      "Gradient Descent(4479/4999): loss=0.28659012624128566, w0=-0.31466399999879174, gamma=0.05303848261088785\n",
      "Gradient Descent(4480/4999): loss=0.28659012482034196, w0=-0.3146639999988055, gamma=0.07595913766181069\n",
      "Gradient Descent(4481/4999): loss=0.2865901234141258, w0=-0.3146639999988242, gamma=0.17401151326803316\n",
      "Gradient Descent(4482/4999): loss=0.28659012140024964, w0=-0.31466399999886385, gamma=0.13516452305266707\n",
      "Gradient Descent(4483/4999): loss=0.28659011678688795, w0=-0.3146639999988892, gamma=0.041038966552610225\n",
      "Gradient Descent(4484/4999): loss=0.28659011320366007, w0=-0.3146639999988959, gamma=0.03572623406878004\n",
      "Gradient Descent(4485/4999): loss=0.286590112115623, w0=-0.31466399999890143, gamma=0.05812535016678153\n",
      "Gradient Descent(4486/4999): loss=0.28659011116852573, w0=-0.31466399999891015, gamma=4.379892819812951\n",
      "Gradient Descent(4487/4999): loss=0.28659010962765036, w0=-0.314663999999531, gamma=4.578171920536729\n",
      "Gradient Descent(4488/4999): loss=0.2865899935482309, w0=-0.3146639999973384, gamma=0.03579160636718725\n",
      "Gradient Descent(4489/4999): loss=0.2865898723753626, w0=-0.31466399999739975, gamma=0.035032077019350995\n",
      "Gradient Descent(4490/4999): loss=0.28658987130251373, w0=-0.31466399999745764, gamma=0.03522715444153198\n",
      "Gradient Descent(4491/4999): loss=0.2865898703743367, w0=-0.3146639999975138, gamma=0.2466672400147235\n",
      "Gradient Descent(4492/4999): loss=0.28658986944105125, w0=-0.3146639999978933, gamma=0.30246139139126843\n",
      "Gradient Descent(4493/4999): loss=0.28658986290602445, w0=-0.31466399999824385, gamma=0.613281654488119\n",
      "Gradient Descent(4494/4999): loss=0.2865898548928538, w0=-0.31466399999873956, gamma=0.6816661930292084\n",
      "Gradient Descent(4495/4999): loss=0.28658983864513554, w0=-0.31466399999895267, gamma=0.20772078817457998\n",
      "Gradient Descent(4496/4999): loss=0.2865898205858764, w0=-0.3146639999989734, gamma=0.1332996209635464\n",
      "Gradient Descent(4497/4999): loss=0.2865898150827455, w0=-0.31466399999898387, gamma=0.0674783862701847\n",
      "Gradient Descent(4498/4999): loss=0.2865898115512779, w0=-0.3146639999989885, gamma=0.05539834055101985\n",
      "Gradient Descent(4499/4999): loss=0.28658980976358894, w0=-0.31466399999899203, gamma=0.07344213325386625\n",
      "Gradient Descent(4500/4999): loss=0.2865898082959412, w0=-0.31466399999899647, gamma=2.718801178380646\n",
      "Gradient Descent(4501/4999): loss=0.28658980635026793, w0=-0.314663999999148, gamma=5.621248281959749\n",
      "Gradient Descent(4502/4999): loss=0.28658973432272833, w0=-0.3146639999986109, gamma=0.03801443617384524\n",
      "Gradient Descent(4503/4999): loss=0.286589585471985, w0=-0.3146639999986277, gamma=0.03533754314805824\n",
      "Gradient Descent(4504/4999): loss=0.28658958440284443, w0=-0.3146639999986427, gamma=0.04266367077794573\n",
      "Gradient Descent(4505/4999): loss=0.2865895834654233, w0=-0.3146639999986602, gamma=0.10315669690144093\n",
      "Gradient Descent(4506/4999): loss=0.28658958233479515, w0=-0.31466399999870065, gamma=0.12472145352700723\n",
      "Gradient Descent(4507/4999): loss=0.286589579601798, w0=-0.31466399999874456, gamma=0.12263904655058502\n",
      "Gradient Descent(4508/4999): loss=0.2865895762978576, w0=-0.3146639999987823, gamma=0.06195537335287757\n",
      "Gradient Descent(4509/4999): loss=0.28658957304911586, w0=-0.31466399999879907, gamma=0.051711659989872816\n",
      "Gradient Descent(4510/4999): loss=0.28658957140790076, w0=-0.31466399999881217, gamma=0.054062262955148786\n",
      "Gradient Descent(4511/4999): loss=0.2865895700380468, w0=-0.31466399999882516, gamma=0.11826807652372053\n",
      "Gradient Descent(4512/4999): loss=0.2865895686059258, w0=-0.3146639999988521, gamma=0.3154183722197067\n",
      "Gradient Descent(4513/4999): loss=0.2865895654729812, w0=-0.31466399999891537, gamma=0.08488737771849901\n",
      "Gradient Descent(4514/4999): loss=0.2865895571175027, w0=-0.314663999998927, gamma=0.035809452895101804\n",
      "Gradient Descent(4515/4999): loss=0.2865895548688318, w0=-0.3146639999989315, gamma=0.03541567158078961\n",
      "Gradient Descent(4516/4999): loss=0.2865895539202298, w0=-0.3146639999989358, gamma=0.8128041610652497\n",
      "Gradient Descent(4517/4999): loss=0.28658955298206634, w0=-0.3146639999990308, gamma=519.6249464862517\n",
      "Gradient Descent(4518/4999): loss=0.28658953145087684, w0=-0.31466400001039657, gamma=12.60811975339338\n",
      "Gradient Descent(4519/4999): loss=0.2865757833169653, w0=-0.3146639998675935, gamma=0.035192048731133206\n",
      "Gradient Descent(4520/4999): loss=0.28658118855937764, w0=-0.3146639998722203, gamma=0.035040096838156745\n",
      "Gradient Descent(4521/4999): loss=0.28657546935704103, w0=-0.31466399987666516, gamma=0.1097404101901392\n",
      "Gradient Descent(4522/4999): loss=0.28657546059510847, w0=-0.314663999890098, gamma=0.1562542132501577\n",
      "Gradient Descent(4523/4999): loss=0.2865754470216689, w0=-0.3146639999071254, gamma=0.1370062492445038\n",
      "Gradient Descent(4524/4999): loss=0.28657544181247263, w0=-0.31466399991972244, gamma=0.05368290031610749\n",
      "Gradient Descent(4525/4999): loss=0.28657543825796716, w0=-0.3146639999239821, gamma=0.05363659370283306\n",
      "Gradient Descent(4526/4999): loss=0.2865754367435454, w0=-0.3146639999280096, gamma=0.09285216913650102\n",
      "Gradient Descent(4527/4999): loss=0.286575435329465, w0=-0.31466399993460775, gamma=0.1797303476480263\n",
      "Gradient Descent(4528/4999): loss=0.2865754328815448, w0=-0.3146639999461937, gamma=0.08563442012904117\n",
      "Gradient Descent(4529/4999): loss=0.28657542814330056, w0=-0.3146639999507218, gamma=0.03665902859802002\n",
      "Gradient Descent(4530/4999): loss=0.286575425885757, w0=-0.3146639999524942, gamma=0.03565922089685862\n",
      "Gradient Descent(4531/4999): loss=0.2865754249193068, w0=-0.3146639999541551, gamma=0.24797062526604888\n",
      "Gradient Descent(4532/4999): loss=0.28657542397923896, w0=-0.3146639999652928, gamma=1.1365813986389353\n",
      "Gradient Descent(4533/4999): loss=0.2865754174421286, w0=-0.3146640000036839, gamma=2.353106616269833\n",
      "Gradient Descent(4534/4999): loss=0.2865753874795562, w0=-0.3146639999928284, gamma=0.04823570397618684\n",
      "Gradient Descent(4535/4999): loss=0.28657532545188297, w0=-0.3146639999931295, gamma=0.03544324319054039\n",
      "Gradient Descent(4536/4999): loss=0.2865753241779393, w0=-0.31466399999334005, gamma=0.035271302304153256\n",
      "Gradient Descent(4537/4999): loss=0.28657532324324814, w0=-0.31466399999354216, gamma=0.11794594606569596\n",
      "Gradient Descent(4538/4999): loss=0.28657532231346367, w0=-0.31466399999419425, gamma=0.23186684076692626\n",
      "Gradient Descent(4539/4999): loss=0.2865753192043215, w0=-0.3146639999953249, gamma=0.24797125200118442\n",
      "Gradient Descent(4540/4999): loss=0.28657531309220335, w0=-0.3146639999962538, gamma=0.08097240332134542\n",
      "Gradient Descent(4541/4999): loss=0.2865753065556298, w0=-0.31466399999648187, gamma=0.06272263043207689\n",
      "Gradient Descent(4542/4999): loss=0.2865753044211697, w0=-0.31466399999664424, gamma=0.0662675974601774\n",
      "Gradient Descent(4543/4999): loss=0.2865753027677906, w0=-0.31466399999680505, gamma=0.2194204655517735\n",
      "Gradient Descent(4544/4999): loss=0.2865753010209688, w0=-0.3146639999973022, gamma=0.2934352253762202\n",
      "Gradient Descent(4545/4999): loss=0.28657529523702674, w0=-0.3146639999978212, gamma=0.04432062859995349\n",
      "Gradient Descent(4546/4999): loss=0.2865752875020905, w0=-0.3146639999978766, gamma=0.03520672728833391\n",
      "Gradient Descent(4547/4999): loss=0.2865752863337831, w0=-0.3146639999979186, gamma=0.037398161470919684\n",
      "Gradient Descent(4548/4999): loss=0.2865752854057349, w0=-0.3146639999979617, gamma=18.15167612026098\n",
      "Gradient Descent(4549/4999): loss=0.2865752844199227, w0=-0.3146640000180924, gamma=36.26515125725472\n",
      "Gradient Descent(4550/4999): loss=0.28657480596835605, w0=-0.31466399932828903, gamma=0.06678809427558928\n",
      "Gradient Descent(4551/4999): loss=0.2865738518228804, w0=-0.3146639993730894, gamma=0.055091383919578366\n",
      "Gradient Descent(4552/4999): loss=0.28657384870654434, w0=-0.3146639994075757, gamma=0.039136306936661994\n",
      "Gradient Descent(4553/4999): loss=0.2865738471756479, w0=-0.3146639994307247, gamma=0.036237711732454894\n",
      "Gradient Descent(4554/4999): loss=0.28657384611975906, w0=-0.31466399945132034, gamma=0.09581516778493976\n",
      "Gradient Descent(4555/4999): loss=0.286573845154842, w0=-0.3146639995038033, gamma=0.4486032309962808\n",
      "Gradient Descent(4556/4999): loss=0.28657384260927204, w0=-0.3146639997259827, gamma=0.44321129557740413\n",
      "Gradient Descent(4557/4999): loss=0.286573830751188, w0=-0.3146639998470192, gamma=0.03772749964188852\n",
      "Gradient Descent(4558/4999): loss=0.28657381908158647, w0=-0.3146639998527558, gamma=0.03544259996259456\n",
      "Gradient Descent(4559/4999): loss=0.2865738180753647, w0=-0.31466399985794163, gamma=0.042206414875031614\n",
      "Gradient Descent(4560/4999): loss=0.2865738171411398, w0=-0.31466399986389826, gamma=0.07280675659227945\n",
      "Gradient Descent(4561/4999): loss=0.2865738160288177, w0=-0.3146639998737399, gamma=0.1866412106764893\n",
      "Gradient Descent(4562/4999): loss=0.2865738141100911, w0=-0.3146639998971322, gamma=1.2679723877423423\n",
      "Gradient Descent(4563/4999): loss=0.2865738091914956, w0=-0.3146640000263899, gamma=0.44334070810197146\n",
      "Gradient Descent(4564/4999): loss=0.28657377577873466, w0=-0.3146640000142791, gamma=0.05308189309758672\n",
      "Gradient Descent(4565/4999): loss=0.2865737641057738, w0=-0.3146640000134719, gamma=0.04716886788574886\n",
      "Gradient Descent(4566/4999): loss=0.2865737626989837, w0=-0.3146640000127927, gamma=0.035687885675313565\n",
      "Gradient Descent(4567/4999): loss=0.28657376145580726, w0=-0.31466400001230305, gamma=0.03553022693491201\n",
      "Gradient Descent(4568/4999): loss=0.2865737605154599, w0=-0.314664000011833, gamma=0.24068821159292325\n",
      "Gradient Descent(4569/4999): loss=0.2865737595793139, w0=-0.31466400000876166, gamma=1.083417392633434\n",
      "Gradient Descent(4570/4999): loss=0.2865737532377198, w0=-0.3146639999982645, gamma=0.2101590379775435\n",
      "Gradient Descent(4571/4999): loss=0.28657372469277115, w0=-0.31466399999843436, gamma=0.05805541386769201\n",
      "Gradient Descent(4572/4999): loss=0.2865737191564615, w0=-0.31466399999847144, gamma=0.05742768861571527\n",
      "Gradient Descent(4573/4999): loss=0.28657371762617734, w0=-0.31466399999850597, gamma=0.04184165892086209\n",
      "Gradient Descent(4574/4999): loss=0.28657371611317073, w0=-0.3146639999985297, gamma=0.050646296025373426\n",
      "Gradient Descent(4575/4999): loss=0.2865737150107995, w0=-0.31466399999855715, gamma=0.4125809181478273\n",
      "Gradient Descent(4576/4999): loss=0.28657371367646123, w0=-0.31466399999876987, gamma=1.5498854949702532\n",
      "Gradient Descent(4577/4999): loss=0.28657370280655065, w0=-0.3146639999992394, gamma=0.05048322465134381\n",
      "Gradient Descent(4578/4999): loss=0.28657366197437417, w0=-0.314663999999231, gamma=0.035045465162989294\n",
      "Gradient Descent(4579/4999): loss=0.2865736606438161, w0=-0.31466399999922545, gamma=0.03507358362519467\n",
      "Gradient Descent(4580/4999): loss=0.2865736597204, w0=-0.3146639999992201, gamma=0.5343479446218968\n",
      "Gradient Descent(4581/4999): loss=0.2865736587963805, w0=-0.3146639999991414, gamma=0.8806179153908891\n",
      "Gradient Descent(4582/4999): loss=0.28657364471893076, w0=-0.3146639999990811, gamma=0.19111506821084592\n",
      "Gradient Descent(4583/4999): loss=0.2865736215192276, w0=-0.31466399999907957, gamma=0.0974642420871827\n",
      "Gradient Descent(4584/4999): loss=0.2865736164843444, w0=-0.3146639999990789, gamma=0.08184110585364957\n",
      "Gradient Descent(4585/4999): loss=0.28657361391663566, w0=-0.3146639999990784, gamma=0.09108848921658341\n",
      "Gradient Descent(4586/4999): loss=0.2865736117605639, w0=-0.3146639999990779, gamma=1.19901033152916\n",
      "Gradient Descent(4587/4999): loss=0.28657360936087883, w0=-0.31466399999907185, gamma=2.50445920332179\n",
      "Gradient Descent(4588/4999): loss=0.28657357777362885, w0=-0.3146639999990744, gamma=0.039211736780998714\n",
      "Gradient Descent(4589/4999): loss=0.28657351180190405, w0=-0.31466399999907435, gamma=0.035262552363014524\n",
      "Gradient Descent(4590/4999): loss=0.2865735107633806, w0=-0.3146639999990743, gamma=0.038809848965003675\n",
      "Gradient Descent(4591/4999): loss=0.2865735098343014, w0=-0.31466399999907424, gamma=0.09137921867789481\n",
      "Gradient Descent(4592/4999): loss=0.2865735088118878, w0=-0.31466399999907413, gamma=0.11194091041714012\n",
      "Gradient Descent(4593/4999): loss=0.28657350640462315, w0=-0.314663999999074, gamma=0.1686973681106224\n",
      "Gradient Descent(4594/4999): loss=0.2865735034557121, w0=-0.31466399999907385, gamma=0.333451365583556\n",
      "Gradient Descent(4595/4999): loss=0.2865734990116504, w0=-0.3146639999990736, gamma=0.18072623038013017\n",
      "Gradient Descent(4596/4999): loss=0.28657349022742895, w0=-0.31466399999907346, gamma=0.048890904555670786\n",
      "Gradient Descent(4597/4999): loss=0.2865734854665211, w0=-0.31466399999907346, gamma=0.037598538428717396\n",
      "Gradient Descent(4598/4999): loss=0.286573484178565, w0=-0.31466399999907346, gamma=0.03803180038253461\n",
      "Gradient Descent(4599/4999): loss=0.2865734831880969, w0=-0.31466399999907346, gamma=0.21856901583113375\n",
      "Gradient Descent(4600/4999): loss=0.2865734821862172, w0=-0.3146639999990734, gamma=1.4050646552456632\n",
      "Gradient Descent(4601/4999): loss=0.2865734764284128, w0=-0.31466399999907296, gamma=0.337892949186054\n",
      "Gradient Descent(4602/4999): loss=0.2865734394147424, w0=-0.3146639999990731, gamma=0.05553157395084632\n",
      "Gradient Descent(4603/4999): loss=0.28657343051420675, w0=-0.3146639999990731, gamma=0.04225265090680536\n",
      "Gradient Descent(4604/4999): loss=0.28657342905090194, w0=-0.3146639999990731, gamma=0.03623811630487954\n",
      "Gradient Descent(4605/4999): loss=0.2865734279377842, w0=-0.3146639999990731, gamma=0.04033522767194626\n",
      "Gradient Descent(4606/4999): loss=0.28657342698317045, w0=-0.3146639999990731, gamma=0.10306834067165234\n",
      "Gradient Descent(4607/4999): loss=0.28657342592063223, w0=-0.3146639999990731, gamma=0.2384037617638358\n",
      "Gradient Descent(4608/4999): loss=0.2865734232055382, w0=-0.31466399999907313, gamma=0.9350284266356464\n",
      "Gradient Descent(4609/4999): loss=0.28657341692535776, w0=-0.3146639999990732, gamma=0.1731446773976976\n",
      "Gradient Descent(4610/4999): loss=0.2865733922943759, w0=-0.31466399999907324, gamma=0.04549959788596795\n",
      "Gradient Descent(4611/4999): loss=0.28657338773343954, w0=-0.31466399999907324, gamma=0.03670986509226943\n",
      "Gradient Descent(4612/4999): loss=0.286573386534765, w0=-0.31466399999907324, gamma=0.037837643298947586\n",
      "Gradient Descent(4613/4999): loss=0.2865733855677314, w0=-0.31466399999907324, gamma=0.0854021626736326\n",
      "Gradient Descent(4614/4999): loss=0.2865733845710007, w0=-0.31466399999907324, gamma=0.15737731264801094\n",
      "Gradient Descent(4615/4999): loss=0.2865733823213136, w0=-0.31466399999907324, gamma=2.2307478062577077\n",
      "Gradient Descent(4616/4999): loss=0.286573378175642, w0=-0.314663999999073, gamma=0.6664737097170523\n",
      "Gradient Descent(4617/4999): loss=0.28657331941313, w0=-0.3146639999990733, gamma=0.03880751653667463\n",
      "Gradient Descent(4618/4999): loss=0.2865733018615281, w0=-0.3146639999990733, gamma=0.03584619487192723\n",
      "Gradient Descent(4619/4999): loss=0.28657330083505295, w0=-0.3146639999990733, gamma=0.04578384907652693\n",
      "Gradient Descent(4620/4999): loss=0.28657329989065344, w0=-0.3146639999990733, gamma=0.07260423219992003\n",
      "Gradient Descent(4621/4999): loss=0.2865732986846054, w0=-0.3146639999990733, gamma=0.07673853824006703\n",
      "Gradient Descent(4622/4999): loss=0.2865732967720976, w0=-0.3146639999990733, gamma=0.9546198156464621\n",
      "Gradient Descent(4623/4999): loss=0.2865732947506929, w0=-0.3146639999990733, gamma=0.7599170006961526\n",
      "Gradient Descent(4624/4999): loss=0.2865732696046892, w0=-0.3146639999990734, gamma=0.03616864263278017\n",
      "Gradient Descent(4625/4999): loss=0.28657324958864683, w0=-0.3146639999990734, gamma=0.03506193553059704\n",
      "Gradient Descent(4626/4999): loss=0.286573248634872, w0=-0.3146639999990734, gamma=0.040198035813812635\n",
      "Gradient Descent(4627/4999): loss=0.2865732477113033, w0=-0.3146639999990734, gamma=0.15263053905917215\n",
      "Gradient Descent(4628/4999): loss=0.28657324665244654, w0=-0.3146639999990734, gamma=1.0879661288339697\n",
      "Gradient Descent(4629/4999): loss=0.28657324263200734, w0=-0.3146639999990734, gamma=0.4386162325589259\n",
      "Gradient Descent(4630/4999): loss=0.2865732139740154, w0=-0.31466399999907346, gamma=0.058899931776907215\n",
      "Gradient Descent(4631/4999): loss=0.28657320242098394, w0=-0.31466399999907346, gamma=0.057729358531117096\n",
      "Gradient Descent(4632/4999): loss=0.28657320086910115, w0=-0.31466399999907346, gamma=0.10309521866832205\n",
      "Gradient Descent(4633/4999): loss=0.2865731993484766, w0=-0.31466399999907346, gamma=0.5929900775822776\n",
      "Gradient Descent(4634/4999): loss=0.28657319663289105, w0=-0.3146639999990734, gamma=0.18305902968952734\n",
      "Gradient Descent(4635/4999): loss=0.286573181013239, w0=-0.3146639999990734, gamma=0.03561569145746735\n",
      "Gradient Descent(4636/4999): loss=0.2865731761914688, w0=-0.3146639999990734, gamma=0.03506470214556133\n",
      "Gradient Descent(4637/4999): loss=0.2865731752532596, w0=-0.3146639999990734, gamma=0.1524637589503724\n",
      "Gradient Descent(4638/4999): loss=0.2865731743296434, w0=-0.3146639999990734, gamma=286.70590168760197\n",
      "Gradient Descent(4639/4999): loss=0.28657317031369856, w0=-0.31466399999912564, gamma=138.56275816194983\n",
      "Gradient Descent(4640/4999): loss=0.28656562335955105, w0=-0.31466399999327466, gamma=0.05539924092272712\n",
      "Gradient Descent(4641/4999): loss=0.2865735051710113, w0=-0.31466399999359673, gamma=0.053333772011893495\n",
      "Gradient Descent(4642/4999): loss=0.2865622090141626, w0=-0.31466399999388955, gamma=0.04129456702180138\n",
      "Gradient Descent(4643/4999): loss=0.2865620199572586, w0=-0.3146639999941042, gamma=0.03755989379025392\n",
      "Gradient Descent(4644/4999): loss=0.2865619872148295, w0=-0.3146639999942914, gamma=0.04855633787631929\n",
      "Gradient Descent(4645/4999): loss=0.2865619830734296, w0=-0.31466399999452427, gamma=0.07778874178828613\n",
      "Gradient Descent(4646/4999): loss=0.2865619809578102, w0=-0.3146639999948792, gamma=0.07793173997355554\n",
      "Gradient Descent(4647/4999): loss=0.2865619787253542, w0=-0.31466399999520717, gamma=0.03729342857484496\n",
      "Gradient Descent(4648/4999): loss=0.2865619766351012, w0=-0.3146639999953519, gamma=0.03626596289409305\n",
      "Gradient Descent(4649/4999): loss=0.28656197563387187, w0=-0.31466399999548733, gamma=0.3182631236395849\n",
      "Gradient Descent(4650/4999): loss=0.2865619746615252, w0=-0.3146639999966331, gamma=7.936784559522846\n",
      "Gradient Descent(4651/4999): loss=0.28656196613291485, w0=-0.31466400001611355, gamma=2.3837090707027784\n",
      "Gradient Descent(4652/4999): loss=0.28656175562429803, w0=-0.31466399997552985, gamma=0.035109057695541346\n",
      "Gradient Descent(4653/4999): loss=0.2865617830614768, w0=-0.31466399997635697, gamma=0.035047151886569795\n",
      "Gradient Descent(4654/4999): loss=0.28656169218067196, w0=-0.3146639999771536, gamma=0.07378182968763919\n",
      "Gradient Descent(4655/4999): loss=0.28656169119115615, w0=-0.314663999978772, gamma=0.07669790188690478\n",
      "Gradient Descent(4656/4999): loss=0.28656168922541153, w0=-0.3146639999803302, gamma=0.0701640315778128\n",
      "Gradient Descent(4657/4999): loss=0.28656168721179487, w0=-0.3146639999816463, gamma=0.08267719187908618\n",
      "Gradient Descent(4658/4999): loss=0.28656168536988724, w0=-0.31466399998308836, gamma=0.38943218930853046\n",
      "Gradient Descent(4659/4999): loss=0.28656168319951997, w0=-0.31466399998931915, gamma=1.296186438337764\n",
      "Gradient Descent(4660/4999): loss=0.28656167297670904, w0=-0.31466400000198164, gamma=0.09042182457981411\n",
      "Gradient Descent(4661/4999): loss=0.2865616389553656, w0=-0.31466400000172, gamma=0.05338827937775751\n",
      "Gradient Descent(4662/4999): loss=0.2865616365812098, w0=-0.3146640000015795, gamma=0.0485154650135658\n",
      "Gradient Descent(4663/4999): loss=0.28656163517920136, w0=-0.31466400000145867, gamma=0.03629022436940267\n",
      "Gradient Descent(4664/4999): loss=0.28656163390586503, w0=-0.3146640000013726, gamma=0.04201538124489238\n",
      "Gradient Descent(4665/4999): loss=0.28656163295340864, w0=-0.31466400000127664, gamma=2.539225759988746\n",
      "Gradient Descent(4666/4999): loss=0.2865616318507002, w0=-0.3146639999957201, gamma=7.128763282091878\n",
      "Gradient Descent(4667/4999): loss=0.28656156521165255, w0=-0.31466400001973255, gamma=0.07082453372419567\n",
      "Gradient Descent(4668/4999): loss=0.2865613782663162, w0=-0.31466400001827044, gamma=0.03841191649175869\n",
      "Gradient Descent(4669/4999): loss=0.286561376360877, w0=-0.31466400001753364, gamma=0.03556483882798486\n",
      "Gradient Descent(4670/4999): loss=0.28656137530818115, w0=-0.31466400001687767, gamma=0.05496412904825212\n",
      "Gradient Descent(4671/4999): loss=0.2865613743724498, w0=-0.3146640000158999, gamma=0.12402679195771911\n",
      "Gradient Descent(4672/4999): loss=0.2865613729288577, w0=-0.31466400001381484, gamma=0.12616967774171484\n",
      "Gradient Descent(4673/4999): loss=0.28656136967443846, w0=-0.3146640000119569, gamma=0.07929678265347183\n",
      "Gradient Descent(4674/4999): loss=0.2865613663646034, w0=-0.3146640000109365, gamma=0.03890905471607656\n",
      "Gradient Descent(4675/4999): loss=0.2865613642844007, w0=-0.3146640000104755, gamma=0.03715525852325265\n",
      "Gradient Descent(4676/4999): loss=0.2865613632636934, w0=-0.31466400001005246, gamma=0.19393817871169797\n",
      "Gradient Descent(4677/4999): loss=0.286561362288996, w0=-0.31466400000792616, gamma=3.904437863274147\n",
      "Gradient Descent(4678/4999): loss=0.28656135720140324, w0=-0.31466399997342126, gamma=1.932921114889698\n",
      "Gradient Descent(4679/4999): loss=0.28656125477758465, w0=-0.3146640000230346, gamma=0.03544837220545788\n",
      "Gradient Descent(4680/4999): loss=0.2865612041395166, w0=-0.31466400002218575, gamma=0.0351255716069533\n",
      "Gradient Descent(4681/4999): loss=0.2865612031443211, w0=-0.31466400002137446, gamma=0.053887742189179154\n",
      "Gradient Descent(4682/4999): loss=0.2865612022227696, w0=-0.31466400002017353, gamma=0.060182246580524075\n",
      "Gradient Descent(4683/4999): loss=0.28656120080919645, w0=-0.3146640000189046, gamma=0.1982893614335678\n",
      "Gradient Descent(4684/4999): loss=0.2865611992305416, w0=-0.31466400001497535, gamma=0.3503892512224333\n",
      "Gradient Descent(4685/4999): loss=0.2865611940291828, w0=-0.31466400000940886, gamma=2.554759010215561\n",
      "Gradient Descent(4686/4999): loss=0.2865611848381006, w0=-0.3146639999830441, gamma=0.15840140676053321\n",
      "Gradient Descent(4687/4999): loss=0.2865611178248345, w0=-0.31466399998558564, gamma=0.03829753267679141\n",
      "Gradient Descent(4688/4999): loss=0.2865611136710412, w0=-0.3146639999861028, gamma=0.03588010457516449\n",
      "Gradient Descent(4689/4999): loss=0.2865611126652326, w0=-0.31466399998656874, gamma=0.042882652830005426\n",
      "Gradient Descent(4690/4999): loss=0.28656111172405985, w0=-0.31466399998710565, gamma=0.06199120902940561\n",
      "Gradient Descent(4691/4999): loss=0.2865611105992339, w0=-0.3146639999878485, gamma=0.11799730039045958\n",
      "Gradient Descent(4692/4999): loss=0.28656110897318887, w0=-0.3146639999891748, gamma=2.976392317832498\n",
      "Gradient Descent(4693/4999): loss=0.2865611058780932, w0=-0.31466400001868383, gamma=2.1897803720388223\n",
      "Gradient Descent(4694/4999): loss=0.28656102780729326, w0=-0.314663999975776, gamma=0.03779740735464487\n",
      "Gradient Descent(4695/4999): loss=0.2865609704000786, w0=-0.31466399997665717, gamma=0.03587648057314164\n",
      "Gradient Descent(4696/4999): loss=0.2865609693802286, w0=-0.314663999977462, gamma=0.04501881257674599\n",
      "Gradient Descent(4697/4999): loss=0.286560968438602, w0=-0.3146639999784356, gamma=0.06098803767173524\n",
      "Gradient Descent(4698/4999): loss=0.2865609672577103, w0=-0.3146639999796952, gamma=0.08879035564256904\n",
      "Gradient Descent(4699/4999): loss=0.2865609656580366, w0=-0.3146639999814172, gamma=0.22840313529750558\n",
      "Gradient Descent(4700/4999): loss=0.2865609633291509, w0=-0.3146639999854536, gamma=0.2585350430743913\n",
      "Gradient Descent(4701/4999): loss=0.2865609573383925, w0=-0.31466399998897887, gamma=0.044942930522966704\n",
      "Gradient Descent(4702/4999): loss=0.28656095055734615, w0=-0.3146639999894333, gamma=0.03541282876205867\n",
      "Gradient Descent(4703/4999): loss=0.28656094937853915, w0=-0.31466399998977523, gamma=0.037951367770023614\n",
      "Gradient Descent(4704/4999): loss=0.2865609484497052, w0=-0.31466399999012873, gamma=3.007227762378055\n",
      "Gradient Descent(4705/4999): loss=0.28656094745428984, w0=-0.3146640000170755, gamma=15.74693623696548\n",
      "Gradient Descent(4706/4999): loss=0.2865608685791702, w0=-0.31466399973384784, gamma=0.08053669361976776\n",
      "Gradient Descent(4707/4999): loss=0.28656045567874594, w0=-0.3146639997552096, gamma=0.06006726543932657\n",
      "Gradient Descent(4708/4999): loss=0.28656045348574427, w0=-0.3146639997698588, gamma=0.04779241544623796\n",
      "Gradient Descent(4709/4999): loss=0.2865604518988799, w0=-0.31466399978081433, gamma=0.03620616816007586\n",
      "Gradient Descent(4710/4999): loss=0.28656045064431035, w0=-0.31466399978871723, gamma=0.038193765898994814\n",
      "Gradient Descent(4711/4999): loss=0.2865604496945496, w0=-0.31466399979675214, gamma=0.1437924077395005\n",
      "Gradient Descent(4712/4999): loss=0.2865604486928395, w0=-0.31466399982584675, gamma=0.18075822217559864\n",
      "Gradient Descent(4713/4999): loss=0.2865604449217695, w0=-0.31466399985716187, gamma=0.2794882753590038\n",
      "Gradient Descent(4714/4999): loss=0.2865604401814273, w0=-0.3146639998968291, gamma=0.10356941323474136\n",
      "Gradient Descent(4715/4999): loss=0.28656043285206007, w0=-0.31466399990742017, gamma=0.03705660587502493\n",
      "Gradient Descent(4716/4999): loss=0.2865604301360745, w0=-0.3146639999108171, gamma=0.0352452606273241\n",
      "Gradient Descent(4717/4999): loss=0.2865604291642654, w0=-0.3146639999139283, gamma=0.06688902510747544\n",
      "Gradient Descent(4718/4999): loss=0.28656042823999417, w0=-0.3146639999196247, gamma=0.7368381205310766\n",
      "Gradient Descent(4719/4999): loss=0.28656042648589847, w0=-0.3146639999781778, gamma=1.6746692136583778\n",
      "Gradient Descent(4720/4999): loss=0.28656040716313935, w0=-0.31466400001319855, gamma=0.28308626786938235\n",
      "Gradient Descent(4721/4999): loss=0.286560363247251, w0=-0.31466400000920464, gamma=0.051431832932316575\n",
      "Gradient Descent(4722/4999): loss=0.2865603558244519, w0=-0.31466400000868444, gamma=0.044718989913191745\n",
      "Gradient Descent(4723/4999): loss=0.28656035447507416, w0=-0.3146640000082554, gamma=0.035978800564015534\n",
      "Gradient Descent(4724/4999): loss=0.28656035330235713, w0=-0.31466400000792566, gamma=0.036588821128543156\n",
      "Gradient Descent(4725/4999): loss=0.2865603523588728, w0=-0.31466400000760236, gamma=0.2654254562705045\n",
      "Gradient Descent(4726/4999): loss=0.286560351399395, w0=-0.314664000005343, gamma=1.6527014422315558\n",
      "Gradient Descent(4727/4999): loss=0.28656034443908435, w0=-0.31466399999500877, gamma=0.20013012364102492\n",
      "Gradient Descent(4728/4999): loss=0.286560301100235, w0=-0.31466399999582556, gamma=0.05746001354946638\n",
      "Gradient Descent(4729/4999): loss=0.2865602958525871, w0=-0.31466399999601313, gamma=0.055742189064758835\n",
      "Gradient Descent(4730/4999): loss=0.28656029434545205, w0=-0.31466399999618466, gamma=0.03598984104822719\n",
      "Gradient Descent(4731/4999): loss=0.28656029288373835, w0=-0.31466399999628925, gamma=0.037360395797144785\n",
      "Gradient Descent(4732/4999): loss=0.2865602919399864, w0=-0.3146639999963939, gamma=1.7091749193993817\n",
      "Gradient Descent(4733/4999): loss=0.28656029096029617, w0=-0.31466400000100214, gamma=8.380861783145898\n",
      "Gradient Descent(4734/4999): loss=0.28656024614131853, w0=-0.3146639999849781, gamma=0.13079859635722343\n",
      "Gradient Descent(4735/4999): loss=0.28656002639158884, w0=-0.314663999986824, gamma=0.06578881348082652\n",
      "Gradient Descent(4736/4999): loss=0.28656002295541183, w0=-0.314663999987631, gamma=0.035785922777842566\n",
      "Gradient Descent(4737/4999): loss=0.2865600212287077, w0=-0.3146639999880411, gamma=0.03512591358900127\n",
      "Gradient Descent(4738/4999): loss=0.2865600202874681, w0=-0.31466399998842925, gamma=0.07522035079785176\n",
      "Gradient Descent(4739/4999): loss=0.2865600193664278, w0=-0.31466399998923117, gamma=0.11501427460804703\n",
      "Gradient Descent(4740/4999): loss=0.2865600173941057, w0=-0.31466399999036515, gamma=0.22321044608624624\n",
      "Gradient Descent(4741/4999): loss=0.28656001437839507, w0=-0.31466399999231276, gamma=0.8013516447034276\n",
      "Gradient Descent(4742/4999): loss=0.2865600085257699, w0=-0.314663999997744, gamma=0.6857195904086794\n",
      "Gradient Descent(4743/4999): loss=0.2865599875142809, w0=-0.31466399999866734, gamma=0.042820762293300445\n",
      "Gradient Descent(4744/4999): loss=0.28655996953520413, w0=-0.31466399999868544, gamma=0.036279073405420074\n",
      "Gradient Descent(4745/4999): loss=0.28655996841207715, w0=-0.31466399999870015, gamma=0.038136365563363916\n",
      "Gradient Descent(4746/4999): loss=0.2865599674608239, w0=-0.314663999998715, gamma=0.09256991428112603\n",
      "Gradient Descent(4747/4999): loss=0.28655996646089527, w0=-0.3146639999987498, gamma=0.12607035559625024\n",
      "Gradient Descent(4748/4999): loss=0.28655996403373635, w0=-0.31466399999879274, gamma=0.5483242144632093\n",
      "Gradient Descent(4749/4999): loss=0.2865599607282104, w0=-0.314663999998956, gamma=0.3302984267523054\n",
      "Gradient Descent(4750/4999): loss=0.28655994635135107, w0=-0.3146639999990004, gamma=0.0466999205326714\n",
      "Gradient Descent(4751/4999): loss=0.286559937691174, w0=-0.31466399999900463, gamma=0.03961814954383284\n",
      "Gradient Descent(4752/4999): loss=0.28655993646664224, w0=-0.314663999999008, gamma=0.03683915486898101\n",
      "Gradient Descent(4753/4999): loss=0.28655993542787045, w0=-0.31466399999901107, gamma=0.06451074022837007\n",
      "Gradient Descent(4754/4999): loss=0.2865599344619685, w0=-0.3146639999990162, gamma=0.7517210324491597\n",
      "Gradient Descent(4755/4999): loss=0.2865599327705337, w0=-0.31466399999907213, gamma=10.517925474174392\n",
      "Gradient Descent(4756/4999): loss=0.2865599130608824, w0=-0.31466399999926575, gamma=0.07289928499734935\n",
      "Gradient Descent(4757/4999): loss=0.28655963730302697, w0=-0.314663999999253, gamma=0.03640245591001136\n",
      "Gradient Descent(4758/4999): loss=0.2865596353912226, w0=-0.3146639999992471, gamma=0.03515465289046163\n",
      "Gradient Descent(4759/4999): loss=0.28655963443004995, w0=-0.3146639999992416, gamma=0.04258654691256621\n",
      "Gradient Descent(4760/4999): loss=0.28655963350838143, w0=-0.31466399999923517, gamma=0.06892077483059579\n",
      "Gradient Descent(4761/4999): loss=0.286559632391899, w0=-0.31466399999922523, gamma=0.11815516590436471\n",
      "Gradient Descent(4762/4999): loss=0.2865596305850245, w0=-0.31466399999920935, gamma=0.43015933113832927\n",
      "Gradient Descent(4763/4999): loss=0.28655962748739167, w0=-0.31466399999915845, gamma=1.0706539830032218\n",
      "Gradient Descent(4764/4999): loss=0.28655961621007603, w0=-0.3146639999990864, gamma=0.07640958572399324\n",
      "Gradient Descent(4765/4999): loss=0.28655958814147015, w0=-0.31466399999908673, gamma=0.05306554309911602\n",
      "Gradient Descent(4766/4999): loss=0.2865595861381722, w0=-0.31466399999908695, gamma=0.046286879004949315\n",
      "Gradient Descent(4767/4999): loss=0.28655958474696525, w0=-0.3146639999990871, gamma=0.03727062670891705\n",
      "Gradient Descent(4768/4999): loss=0.28655958353349625, w0=-0.3146639999990873, gamma=0.05502498281100927\n",
      "Gradient Descent(4769/4999): loss=0.286559582556401, w0=-0.3146639999990875, gamma=3.118390544407289\n",
      "Gradient Descent(4770/4999): loss=0.28655958111385427, w0=-0.3146639999990986, gamma=10.394504349141284\n",
      "Gradient Descent(4771/4999): loss=0.2865594993620742, w0=-0.31466399999902056, gamma=0.03971506671094037\n",
      "Gradient Descent(4772/4999): loss=0.28655922700349007, w0=-0.3146639999990234, gamma=0.03530624146171985\n",
      "Gradient Descent(4773/4999): loss=0.2865592258388983, w0=-0.3146639999990258, gamma=0.04082395054239159\n",
      "Gradient Descent(4774/4999): loss=0.2865592249082702, w0=-0.31466399999902844, gamma=0.11300736595126053\n",
      "Gradient Descent(4775/4999): loss=0.2865592238365079, w0=-0.31466399999903555, gamma=0.11684241916210565\n",
      "Gradient Descent(4776/4999): loss=0.2865592208729928, w0=-0.3146639999990421, gamma=0.14125391109526997\n",
      "Gradient Descent(4777/4999): loss=0.28655921781015775, w0=-0.3146639999990491, gamma=0.08850997986447184\n",
      "Gradient Descent(4778/4999): loss=0.28655921410748314, w0=-0.3146639999990528, gamma=0.0383884604608663\n",
      "Gradient Descent(4779/4999): loss=0.28655921178738514, w0=-0.3146639999990543, gamma=0.036341291341715716\n",
      "Gradient Descent(4780/4999): loss=0.286559210781113, w0=-0.31466399999905564, gamma=0.16993732548729665\n",
      "Gradient Descent(4781/4999): loss=0.286559209828505, w0=-0.31466399999906175, gamma=19.004904613997187\n",
      "Gradient Descent(4782/4999): loss=0.286559205373971, w0=-0.3146639999996263, gamma=4.310635670704278\n",
      "Gradient Descent(4783/4999): loss=0.286558707224498, w0=-0.3146639999973241, gamma=0.03888118662365383\n",
      "Gradient Descent(4784/4999): loss=0.28655859650610016, w0=-0.3146639999973928, gamma=0.03659486708173141\n",
      "Gradient Descent(4785/4999): loss=0.2865585933126177, w0=-0.314663999997455, gamma=0.04398171971824375\n",
      "Gradient Descent(4786/4999): loss=0.2865585922814717, w0=-0.314663999997527, gamma=0.05736924088587188\n",
      "Gradient Descent(4787/4999): loss=0.28655859112117626, w0=-0.3146639999976168, gamma=0.05841000529216582\n",
      "Gradient Descent(4788/4999): loss=0.2865585896171754, w0=-0.314663999997703, gamma=0.06483813225473817\n",
      "Gradient Descent(4789/4999): loss=0.2865585880863946, w0=-0.3146639999977931, gamma=0.06147524065521834\n",
      "Gradient Descent(4790/4999): loss=0.28655858638716364, w0=-0.314663999997873, gamma=0.05487096552298768\n",
      "Gradient Descent(4791/4999): loss=0.2865585847760697, w0=-0.31466399999793987, gamma=0.060273850424258466\n",
      "Gradient Descent(4792/4999): loss=0.2865585833380575, w0=-0.3146639999980093, gamma=0.09868663333296618\n",
      "Gradient Descent(4793/4999): loss=0.28655858175845267, w0=-0.3146639999981162, gamma=0.14102425427974338\n",
      "Gradient Descent(4794/4999): loss=0.2865585791721619, w0=-0.31466399999825384, gamma=0.06518745979034948\n",
      "Gradient Descent(4795/4999): loss=0.2865585754763306, w0=-0.3146639999983085, gamma=0.037982420726438045\n",
      "Gradient Descent(4796/4999): loss=0.2865585737679606, w0=-0.3146639999983383, gamma=0.03888526946929727\n",
      "Gradient Descent(4797/4999): loss=0.2865585727725539, w0=-0.3146639999983676, gamma=0.5211449723218962\n",
      "Gradient Descent(4798/4999): loss=0.2865585717534875, w0=-0.31466399999874534, gamma=5.4270573676535445\n",
      "Gradient Descent(4799/4999): loss=0.2865585580958603, w0=-0.314664000000629, gamma=0.2453811742541885\n",
      "Gradient Descent(4800/4999): loss=0.2865584158717391, w0=-0.31466400000025196, gamma=0.03504368912043317\n",
      "Gradient Descent(4801/4999): loss=0.28655840945154, w0=-0.31466400000021133, gamma=0.035032196650878075\n",
      "Gradient Descent(4802/4999): loss=0.2865584085227601, w0=-0.31466400000017214, gamma=1.866245344265223\n",
      "Gradient Descent(4803/4999): loss=0.2865584076047238, w0=-0.31466399999815753, gamma=14.056311653074268\n",
      "Gradient Descent(4804/4999): loss=0.2865583586990534, w0=-0.3146640000113015, gamma=0.4011485067256166\n",
      "Gradient Descent(4805/4999): loss=0.28655799037411156, w0=-0.3146640000064041, gamma=0.15153757514106425\n",
      "Gradient Descent(4806/4999): loss=0.28655797987290316, w0=-0.3146640000052962, gamma=0.12390301641696497\n",
      "Gradient Descent(4807/4999): loss=0.2865579758848414, w0=-0.3146640000045276, gamma=0.10413943894248731\n",
      "Gradient Descent(4808/4999): loss=0.2865579726369042, w0=-0.31466400000396166, gamma=0.09760422115743679\n",
      "Gradient Descent(4809/4999): loss=0.2865579699082115, w0=-0.3146640000034865, gamma=0.04608120532046803\n",
      "Gradient Descent(4810/4999): loss=0.28655796735083167, w0=-0.31466400000328404, gamma=0.0368792626100042\n",
      "Gradient Descent(4811/4999): loss=0.28655796614343276, w0=-0.3146640000031295, gamma=0.0475672876844974\n",
      "Gradient Descent(4812/4999): loss=0.2865579651771411, w0=-0.3146640000029375, gamma=0.8200533379141902\n",
      "Gradient Descent(4813/4999): loss=0.2865579639308086, w0=-0.3146639999997848, gamma=1.5277833532427327\n",
      "Gradient Descent(4814/4999): loss=0.28655794244426824, w0=-0.31466399999872796, gamma=0.11105765258618425\n",
      "Gradient Descent(4815/4999): loss=0.28655790241478973, w0=-0.3146639999987685, gamma=0.03667399906828857\n",
      "Gradient Descent(4816/4999): loss=0.28655789950521243, w0=-0.3146639999987804, gamma=0.03511702201194073\n",
      "Gradient Descent(4817/4999): loss=0.28655789854389163, w0=-0.3146639999987914, gamma=0.05124693633364871\n",
      "Gradient Descent(4818/4999): loss=0.2865578976237901, w0=-0.31466399999880684, gamma=0.2041324752187264\n",
      "Gradient Descent(4819/4999): loss=0.28655789628107287, w0=-0.3146639999988653, gamma=0.28229841151342605\n",
      "Gradient Descent(4820/4999): loss=0.2865578909326298, w0=-0.3146639999989296, gamma=0.8338261863311297\n",
      "Gradient Descent(4821/4999): loss=0.2865578835361962, w0=-0.3146639999990659, gamma=0.1715277036471451\n",
      "Gradient Descent(4822/4999): loss=0.28655786168942254, w0=-0.3146639999990706, gamma=0.06226786687590258\n",
      "Gradient Descent(4823/4999): loss=0.2865578571953461, w0=-0.31466399999907196, gamma=0.04690765714868992\n",
      "Gradient Descent(4824/4999): loss=0.2865578555638471, w0=-0.31466399999907296, gamma=0.03617165751503092\n",
      "Gradient Descent(4825/4999): loss=0.28655785433483844, w0=-0.3146639999990737, gamma=0.03931822489549172\n",
      "Gradient Descent(4826/4999): loss=0.28655785338712403, w0=-0.31466399999907446, gamma=0.5488360025231989\n",
      "Gradient Descent(4827/4999): loss=0.28655785235696946, w0=-0.3146639999990847, gamma=2.28822609037427\n",
      "Gradient Descent(4828/4999): loss=0.28655783797725054, w0=-0.31466399999910377, gamma=0.1358175463076374\n",
      "Gradient Descent(4829/4999): loss=0.28655777802573784, w0=-0.31466399999910233, gamma=0.06762704398305047\n",
      "Gradient Descent(4830/4999): loss=0.28655777446726016, w0=-0.3146639999991017, gamma=0.042215608411687755\n",
      "Gradient Descent(4831/4999): loss=0.2865577726952501, w0=-0.31466399999910133, gamma=0.035307220503073944\n",
      "Gradient Descent(4832/4999): loss=0.2865577715891381, w0=-0.31466399999910105, gamma=0.03691840314713205\n",
      "Gradient Descent(4833/4999): loss=0.2865577706640962, w0=-0.3146639999991008, gamma=0.18144527530465177\n",
      "Gradient Descent(4834/4999): loss=0.2865577696968456, w0=-0.31466399999909944, gamma=0.4524109339556151\n",
      "Gradient Descent(4835/4999): loss=0.2865577649430409, w0=-0.31466399999909667, gamma=0.29649018860248544\n",
      "Gradient Descent(4836/4999): loss=0.28655775309005743, w0=-0.31466399999909567, gamma=0.08557618527967599\n",
      "Gradient Descent(4837/4999): loss=0.28655774532218276, w0=-0.31466399999909545, gamma=0.07329315647571183\n",
      "Gradient Descent(4838/4999): loss=0.28655774308011145, w0=-0.3146639999990953, gamma=0.054499139800810524\n",
      "Gradient Descent(4839/4999): loss=0.28655774115987215, w0=-0.31466399999909517, gamma=0.048829290780144925\n",
      "Gradient Descent(4840/4999): loss=0.2865577397320267, w0=-0.31466399999909506, gamma=0.0778258793993059\n",
      "Gradient Descent(4841/4999): loss=0.2865577384527287, w0=-0.31466399999909495, gamma=0.31093362381476386\n",
      "Gradient Descent(4842/4999): loss=0.2865577364137388, w0=-0.31466399999909445, gamma=0.2199508720620107\n",
      "Gradient Descent(4843/4999): loss=0.2865577282674808, w0=-0.31466399999909417, gamma=0.03801325373953996\n",
      "Gradient Descent(4844/4999): loss=0.28655772250494915, w0=-0.3146639999990941, gamma=0.03513895041773777\n",
      "Gradient Descent(4845/4999): loss=0.2865577215090071, w0=-0.3146639999990941, gamma=0.04980542124596011\n",
      "Gradient Descent(4846/4999): loss=0.28655772058839346, w0=-0.31466399999909406, gamma=1381.2646405787675\n",
      "Gradient Descent(4847/4999): loss=0.28655771928353013, w0=-0.314663999998114, gamma=8386.706199831464\n",
      "Gradient Descent(4848/4999): loss=0.2865216452845409, w0=-0.31466400860359117, gamma=0.03516040244391399\n",
      "Gradient Descent(4849/4999): loss=0.3122547790983517, w0=-0.3146640083010746, gamma=0.03507651673304097\n",
      "Gradient Descent(4850/4999): loss=0.28634500756645626, w0=-0.3146640080098816, gamma=0.05718097140278175\n",
      "Gradient Descent(4851/4999): loss=0.28631467924254617, w0=-0.31466400755183577, gamma=0.05870580933069666\n",
      "Gradient Descent(4852/4999): loss=0.2863088849888574, w0=-0.3146640071084652, gamma=0.08229644401971978\n",
      "Gradient Descent(4853/4999): loss=0.2863088260012065, w0=-0.31466400652341625, gamma=0.08694359170968145\n",
      "Gradient Descent(4854/4999): loss=0.28630881777315165, w0=-0.31466400595619676, gamma=0.10241249885477076\n",
      "Gradient Descent(4855/4999): loss=0.2863088155300053, w0=-0.3146640053461486, gamma=0.07102686234025633\n",
      "Gradient Descent(4856/4999): loss=0.2863088130543258, w0=-0.31466400496638736, gamma=0.03928285588407247\n",
      "Gradient Descent(4857/4999): loss=0.2863088113453843, w0=-0.31466400477127077, gamma=0.037639236759134394\n",
      "Gradient Descent(4858/4999): loss=0.2863088103999491, w0=-0.31466400459166205, gamma=0.14150324233103953\n",
      "Gradient Descent(4859/4999): loss=0.2863088094961224, w0=-0.31466400394184524, gamma=0.4377854865651928\n",
      "Gradient Descent(4860/4999): loss=0.2863088061010684, w0=-0.3146640022159096, gamma=0.3468781509897219\n",
      "Gradient Descent(4861/4999): loss=0.2863087956147614, w0=-0.3146640014470586, gamma=0.03543048995378367\n",
      "Gradient Descent(4862/4999): loss=0.2863087873245313, w0=-0.3146640013957681, gamma=0.035139999064083306\n",
      "Gradient Descent(4863/4999): loss=0.28630878646364816, w0=-0.31466400134670053, gamma=0.051280400566286674\n",
      "Gradient Descent(4864/4999): loss=0.2863087856225117, w0=-0.31466400127761157, gamma=0.058426066115614635\n",
      "Gradient Descent(4865/4999): loss=0.28630878439506574, w0=-0.314664001202932, gamma=0.43627997855316725\n",
      "Gradient Descent(4866/4999): loss=0.286308782996587, w0=-0.3146640006778647, gamma=0.879530759814915\n",
      "Gradient Descent(4867/4999): loss=0.2863087725538685, w0=-0.31466400008115364, gamma=0.4815079722878425\n",
      "Gradient Descent(4868/4999): loss=0.28630875150170454, w0=-0.31466400004179934, gamma=0.08271882266622144\n",
      "Gradient Descent(4869/4999): loss=0.2863087399766565, w0=-0.3146640000382939, gamma=0.04488256165480028\n",
      "Gradient Descent(4870/4999): loss=0.28630873799669204, w0=-0.31466400003654926, gamma=0.03707734864428363\n",
      "Gradient Descent(4871/4999): loss=0.28630873692236125, w0=-0.3146640000351727, gamma=0.03859346927827172\n",
      "Gradient Descent(4872/4999): loss=0.28630873603489454, w0=-0.31466400003379297, gamma=0.17232222237625025\n",
      "Gradient Descent(4873/4999): loss=0.2863087351111429, w0=-0.31466400002787015, gamma=0.30223204293558104\n",
      "Gradient Descent(4874/4999): loss=0.2863087309865414, w0=-0.3146640000192723, gamma=0.5218047391200542\n",
      "Gradient Descent(4875/4999): loss=0.2863087237525135, w0=-0.3146640000089145, gamma=0.05850339823489645\n",
      "Gradient Descent(4876/4999): loss=0.2863087112630128, w0=-0.31466400000835915, gamma=0.03979551953608646\n",
      "Gradient Descent(4877/4999): loss=0.28630870986269485, w0=-0.3146640000080035, gamma=0.0354658153735505\n",
      "Gradient Descent(4878/4999): loss=0.286308708910175, w0=-0.3146640000076992, gamma=0.052267661456516845\n",
      "Gradient Descent(4879/4999): loss=0.2863087080612949, w0=-0.3146640000072666, gamma=4.9149448188470215\n",
      "Gradient Descent(4880/4999): loss=0.28630870681026055, w0=-0.3146639999687124, gamma=59.73302571879415\n",
      "Gradient Descent(4881/4999): loss=0.28630858917166313, w0=-0.31466400180310883, gamma=0.07859068885549397\n",
      "Gradient Descent(4882/4999): loss=0.2863071605035224, w0=-0.314664001661356, gamma=0.038811306057533766\n",
      "Gradient Descent(4883/4999): loss=0.28630715834593884, w0=-0.31466400159685426, gamma=0.03530174580597917\n",
      "Gradient Descent(4884/4999): loss=0.28630715694725695, w0=-0.3146640015404622, gamma=0.045499121579102464\n",
      "Gradient Descent(4885/4999): loss=0.28630715608087226, w0=-0.31466400147034634, gamma=0.14616719916780563\n",
      "Gradient Descent(4886/4999): loss=0.2863071549804064, w0=-0.3146640012553458, gamma=0.15020394634853873\n",
      "Gradient Descent(4887/4999): loss=0.2863071514722936, w0=-0.3146640010667015, gamma=0.12045186598252133\n",
      "Gradient Descent(4888/4999): loss=0.28630714787905276, w0=-0.31466400093814595, gamma=0.04879223630188362\n",
      "Gradient Descent(4889/4999): loss=0.28630714499771176, w0=-0.31466400089234364, gamma=0.03679452426308925\n",
      "Gradient Descent(4890/4999): loss=0.28630714383053, w0=-0.3146640008594891, gamma=0.03661504618884986\n",
      "Gradient Descent(4891/4999): loss=0.28630714295036425, w0=-0.3146640008279978, gamma=0.2650045179327609\n",
      "Gradient Descent(4892/4999): loss=0.2863071420744952, w0=-0.3146640006084221, gamma=0.6980125042819816\n",
      "Gradient Descent(4893/4999): loss=0.28630713573532574, w0=-0.314664000183334, gamma=0.6112683730106048\n",
      "Gradient Descent(4894/4999): loss=0.28630711903823974, w0=-0.31466400007091583, gamma=0.05579262866838503\n",
      "Gradient Descent(4895/4999): loss=0.2863071044164379, w0=-0.31466400006692713, gamma=0.04238643360608689\n",
      "Gradient Descent(4896/4999): loss=0.2863071030816675, w0=-0.3146640000640659, gamma=0.03611664394806873\n",
      "Gradient Descent(4897/4999): loss=0.2863071020677323, w0=-0.3146640000617313, gamma=0.043788088629364924\n",
      "Gradient Descent(4898/4999): loss=0.286307101203797, w0=-0.31466400005900297, gamma=0.15778596785790025\n",
      "Gradient Descent(4899/4999): loss=0.28630710015635785, w0=-0.31466400004960227, gamma=0.28607394253350005\n",
      "Gradient Descent(4900/4999): loss=0.28630709638202095, w0=-0.31466400003524764, gamma=0.571214406186374\n",
      "Gradient Descent(4901/4999): loss=0.28630708953897, w0=-0.31466400001478484, gamma=0.09984632268980421\n",
      "Gradient Descent(4902/4999): loss=0.28630707587525694, w0=-0.3146640000132512, gamma=0.03827579534238258\n",
      "Gradient Descent(4903/4999): loss=0.28630707348690526, w0=-0.31466400001272193, gamma=0.03516647283027305\n",
      "Gradient Descent(4904/4999): loss=0.2863070725713064, w0=-0.3146640000122543, gamma=0.04561843436665496\n",
      "Gradient Descent(4905/4999): loss=0.2863070717301111, w0=-0.31466400001166905, gamma=1.5017362833061834\n",
      "Gradient Descent(4906/4999): loss=0.28630707063890193, w0=-0.3146639999932809, gamma=9.871753791001451\n",
      "Gradient Descent(4907/4999): loss=0.28630703471696617, w0=-0.3146640000539282, gamma=0.10660041435479417\n",
      "Gradient Descent(4908/4999): loss=0.2863067986033053, w0=-0.3146640000481181, gamma=0.07397830298792098\n",
      "Gradient Descent(4909/4999): loss=0.28630679604241405, w0=-0.3146640000445159, gamma=0.05374084516179065\n",
      "Gradient Descent(4910/4999): loss=0.2863067942709043, w0=-0.31466400004209266, gamma=0.03716186697822755\n",
      "Gradient Descent(4911/4999): loss=0.2863067929852722, w0=-0.31466400004050704, gamma=0.03512122290287338\n",
      "Gradient Descent(4912/4999): loss=0.2863067920963759, w0=-0.3146640000390642, gamma=0.04895333590303358\n",
      "Gradient Descent(4913/4999): loss=0.28630679125635183, w0=-0.3146640000371237, gamma=0.43257770853777705\n",
      "Gradient Descent(4914/4999): loss=0.2863067900854944, w0=-0.31466400002081596, gamma=1.3415089894822279\n",
      "Gradient Descent(4915/4999): loss=0.2863067797391919, w0=-0.3146639999921196, gamma=0.2274419919263168\n",
      "Gradient Descent(4916/4999): loss=0.28630674765350655, w0=-0.3146639999937811, gamma=0.081011508839452\n",
      "Gradient Descent(4917/4999): loss=0.2863067422137863, w0=-0.3146639999942383, gamma=0.07572811801933516\n",
      "Gradient Descent(4918/4999): loss=0.28630674027604713, w0=-0.314663999994631, gamma=0.03630112759692488\n",
      "Gradient Descent(4919/4999): loss=0.28630673846483123, w0=-0.31466399999480504, gamma=0.035801384252314966\n",
      "Gradient Descent(4920/4999): loss=0.2863067375966016, w0=-0.3146639999949704, gamma=0.5885661831720942\n",
      "Gradient Descent(4921/4999): loss=0.28630673674032775, w0=-0.31466399999759215, gamma=37.66105333546689\n",
      "Gradient Descent(4922/4999): loss=0.28630672266341417, w0=-0.3146640000666078, gamma=1.0290213909868027\n",
      "Gradient Descent(4923/4999): loss=0.2863058219983619, w0=-0.31466399999747635, gamma=0.04997230799158724\n",
      "Gradient Descent(4924/4999): loss=0.28630579871045986, w0=-0.3146639999975738, gamma=0.04153789312169222\n",
      "Gradient Descent(4925/4999): loss=0.28630579633296044, w0=-0.3146639999976507, gamma=0.037011519616343316\n",
      "Gradient Descent(4926/4999): loss=0.28630579522275074, w0=-0.31466399999771644, gamma=0.0473875587251595\n",
      "Gradient Descent(4927/4999): loss=0.2863057943250662, w0=-0.3146639999977975, gamma=0.09937934236493953\n",
      "Gradient Descent(4928/4999): loss=0.2863057931871446, w0=-0.3146639999979593, gamma=0.10953049051134528\n",
      "Gradient Descent(4929/4999): loss=0.28630579080860924, w0=-0.31466399999812, gamma=0.08266243843991901\n",
      "Gradient Descent(4930/4999): loss=0.28630578818964303, w0=-0.314663999998228, gamma=0.03862762924732008\n",
      "Gradient Descent(4931/4999): loss=0.28630578621326097, w0=-0.31466399999827427, gamma=0.03572842803880948\n",
      "Gradient Descent(4932/4999): loss=0.28630578528968414, w0=-0.3146639999983154, gamma=0.07028080066420048\n",
      "Gradient Descent(4933/4999): loss=0.2863057844354581, w0=-0.31466399999839345, gamma=0.24669036686787763\n",
      "Gradient Descent(4934/4999): loss=0.2863057827551288, w0=-0.3146639999986482, gamma=0.31421009596950394\n",
      "Gradient Descent(4935/4999): loss=0.28630577685708064, w0=-0.3146639999988926, gamma=0.4218238817732328\n",
      "Gradient Descent(4936/4999): loss=0.286305769344746, w0=-0.31466399999911765, gamma=0.04324922491322288\n",
      "Gradient Descent(4937/4999): loss=0.2863057592595827, w0=-0.31466399999913097, gamma=0.03534257701660364\n",
      "Gradient Descent(4938/4999): loss=0.28630575822552146, w0=-0.3146639999991414, gamma=0.03644808860218151\n",
      "Gradient Descent(4939/4999): loss=0.286305757380532, w0=-0.3146639999991518, gamma=0.6047796087589657\n",
      "Gradient Descent(4940/4999): loss=0.28630575650911366, w0=-0.31466399999931766, gamma=3.3294108311302537\n",
      "Gradient Descent(4941/4999): loss=0.2863057420497791, w0=-0.3146639999996791, gamma=0.10903257246411008\n",
      "Gradient Descent(4942/4999): loss=0.2863056624504131, w0=-0.3146639999996515, gamma=0.05781955019510365\n",
      "Gradient Descent(4943/4999): loss=0.2863056598435306, w0=-0.31466399999963846, gamma=0.05712585722687649\n",
      "Gradient Descent(4944/4999): loss=0.28630565846066686, w0=-0.31466399999962635, gamma=0.040109575898542055\n",
      "Gradient Descent(4945/4999): loss=0.2863056570949233, w0=-0.31466399999961836, gamma=0.04320974838905962\n",
      "Gradient Descent(4946/4999): loss=0.28630565613599984, w0=-0.31466399999961003, gamma=0.23532691010930482\n",
      "Gradient Descent(4947/4999): loss=0.2863056551029598, w0=-0.3146639999995668, gamma=0.6985335740743159\n",
      "Gradient Descent(4948/4999): loss=0.28630564947687187, w0=-0.3146639999994686, gamma=0.16777433539442466\n",
      "Gradient Descent(4949/4999): loss=0.28630563277671245, w0=-0.31466399999946154, gamma=0.03547989864712425\n",
      "Gradient Descent(4950/4999): loss=0.28630562876575766, w0=-0.3146639999994603, gamma=0.035058358368963274\n",
      "Gradient Descent(4951/4999): loss=0.28630562791743863, w0=-0.31466399999945915, gamma=0.18934655092016991\n",
      "Gradient Descent(4952/4999): loss=0.2863056270792876, w0=-0.31466399999945294, gamma=30.078134056386123\n",
      "Gradient Descent(4953/4999): loss=0.2863056225525249, w0=-0.31466399999865285, gamma=8.89662728387304\n",
      "Gradient Descent(4954/4999): loss=0.2863049035160362, w0=-0.3146640000055427, gamma=0.058270153759745744\n",
      "Gradient Descent(4955/4999): loss=0.28630469807754366, w0=-0.31466400000518635, gamma=0.056026626042446175\n",
      "Gradient Descent(4956/4999): loss=0.28630468964730793, w0=-0.3146640000048637, gamma=0.06100722154931954\n",
      "Gradient Descent(4957/4999): loss=0.2863046881637069, w0=-0.3146640000045321, gamma=0.0529460606688317\n",
      "Gradient Descent(4958/4999): loss=0.28630468669583964, w0=-0.3146640000042618, gamma=0.03860896569375009\n",
      "Gradient Descent(4959/4999): loss=0.2863046854293109, w0=-0.3146640000040752, gamma=0.037727762412322736\n",
      "Gradient Descent(4960/4999): loss=0.28630468450631535, w0=-0.3146640000038998, gamma=0.07855790011433773\n",
      "Gradient Descent(4961/4999): loss=0.2863046836046189, w0=-0.3146640000035485, gamma=0.11422491332790294\n",
      "Gradient Descent(4962/4999): loss=0.28630468172714696, w0=-0.31466400000307776, gamma=0.13587404397810832\n",
      "Gradient Descent(4963/4999): loss=0.28630467899730905, w0=-0.3146640000025818, gamma=0.06571146412349174\n",
      "Gradient Descent(4964/4999): loss=0.2863046757500935, w0=-0.31466400000237454, gamma=0.03838331652306917\n",
      "Gradient Descent(4965/4999): loss=0.2863046741796749, w0=-0.3146640000022614, gamma=0.039368512501559\n",
      "Gradient Descent(4966/4999): loss=0.28630467326236386, w0=-0.31466400000214984, gamma=0.502899318975931\n",
      "Gradient Descent(4967/4999): loss=0.2863046723215085, w0=-0.3146640000007809, gamma=21.17341563770177\n",
      "Gradient Descent(4968/4999): loss=0.2863046603028985, w0=-0.31466399997212946, gamma=0.2073268317703343\n",
      "Gradient Descent(4969/4999): loss=0.2863041543180536, w0=-0.31466399997778927, gamma=0.03506310493684909\n",
      "Gradient Descent(4970/4999): loss=0.28630414947778005, w0=-0.314663999978548, gamma=0.03503213576447471\n",
      "Gradient Descent(4971/4999): loss=0.2863041485213348, w0=-0.31466399997927946, gamma=0.081402470106767\n",
      "Gradient Descent(4972/4999): loss=0.2863041476842774, w0=-0.31466399998091965, gamma=0.14707264780078255\n",
      "Gradient Descent(4973/4999): loss=0.28630414573925206, w0=-0.3146639999836418, gamma=0.8678169328964559\n",
      "Gradient Descent(4974/4999): loss=0.2863041422251128, w0=-0.31466399999734174, gamma=4.584547859142614\n",
      "Gradient Descent(4975/4999): loss=0.2863041214896442, w0=-0.31466400000690836, gamma=0.11720250764283469\n",
      "Gradient Descent(4976/4999): loss=0.2863040119508693, w0=-0.31466400000603173, gamma=0.06417788699490948\n",
      "Gradient Descent(4977/4999): loss=0.2863040091497572, w0=-0.31466400000560796, gamma=0.05588716820872832\n",
      "Gradient Descent(4978/4999): loss=0.2863040076153934, w0=-0.3146640000052626, gamma=0.05521333582913729\n",
      "Gradient Descent(4979/4999): loss=0.28630400628006597, w0=-0.3146640000049405, gamma=0.12809797323736566\n",
      "Gradient Descent(4980/4999): loss=0.2863040049608725, w0=-0.31466400000423445, gamma=0.475490169231007\n",
      "Gradient Descent(4981/4999): loss=0.286304001900275, w0=-0.31466400000194933, gamma=0.9605056409999828\n",
      "Gradient Descent(4982/4999): loss=0.2863039905395879, w0=-0.31466399999952815, gamma=0.047524824441652236\n",
      "Gradient Descent(4983/4999): loss=0.2863039675909549, w0=-0.31466399999952344, gamma=0.03581662948922047\n",
      "Gradient Descent(4984/4999): loss=0.2863039664553014, w0=-0.31466399999952005, gamma=0.035934516056410015\n",
      "Gradient Descent(4985/4999): loss=0.28630396559953547, w0=-0.3146639999995168, gamma=0.09502408132123773\n",
      "Gradient Descent(4986/4999): loss=0.28630396474097713, w0=-0.3146639999995084, gamma=0.1546195964442108\n",
      "Gradient Descent(4987/4999): loss=0.28630396247063594, w0=-0.314663999999496, gamma=0.9219923205036126\n",
      "Gradient Descent(4988/4999): loss=0.2863039587764269, w0=-0.314663999999434, gamma=0.7829928856510988\n",
      "Gradient Descent(4989/4999): loss=0.28630393674803584, w0=-0.31466399999942973, gamma=0.06535508098431575\n",
      "Gradient Descent(4990/4999): loss=0.28630391804121375, w0=-0.3146639999994297, gamma=0.04826732840697094\n",
      "Gradient Descent(4991/4999): loss=0.2863039164793899, w0=-0.3146639999994296, gamma=0.035454539705794784\n",
      "Gradient Descent(4992/4999): loss=0.2863039153261447, w0=-0.31466399999942957, gamma=0.03556633172482849\n",
      "Gradient Descent(4993/4999): loss=0.2863039144790631, w0=-0.3146639999994295, gamma=0.24642665095829516\n",
      "Gradient Descent(4994/4999): loss=0.2863039136293188, w0=-0.31466399999942934, gamma=0.7785072393103057\n",
      "Gradient Descent(4995/4999): loss=0.28630390774174375, w0=-0.31466399999942885, gamma=0.20553180714812905\n",
      "Gradient Descent(4996/4999): loss=0.2863038891418862, w0=-0.3146639999994288, gamma=0.07319338659601442\n",
      "Gradient Descent(4997/4999): loss=0.2863038842314424, w0=-0.3146639999994288, gamma=0.06999226393382989\n",
      "Gradient Descent(4998/4999): loss=0.28630388248269023, w0=-0.3146639999994288, gamma=0.0544721077584737\n",
      "Gradient Descent(4999/4999): loss=0.2863038808104682, w0=-0.3146639999994288, gamma=0.054532819812444594\n"
     ]
    }
   ],
   "source": [
    "w = np.array(np.random.rand((tX.shape[1])))\n",
    "max_iters = 5000 #choosing number of iterations\n",
    "gamma = 5 *10**(-2)\n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)\n",
    "(w, loss) =adaptative_step_gradient_descent(y, tX, w, max_iters, gamma)\n",
    "#gamma = 5/max_iters #choosing gamma \n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, xx, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = xx.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tX.shape[1]):\n",
    "    col = tX[:,i]\n",
    "    places_999 = (col == -999.0)\n",
    "    mean_col = col[col!=-999.0].mean()\n",
    "    col[places_999] = mean_col\n",
    "    tX[:,i] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_columns = tX_test[:,np.all(tX_test > 0,axis=0)]\n",
    "positive_columns_log = np.log(positive_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_columns_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 211)\n",
      "(568238, 210)\n",
      "(568238, 270)\n",
      "(568238, 282)\n"
     ]
    }
   ],
   "source": [
    "tX_test = build_poly(tX_test, degree)\n",
    "print(tX_test.shape)\n",
    "tX_test = tX_test[:,1:]\n",
    "print(tX_test.shape)\n",
    "tX_test = extend(tX_test,[np.cos,np.sin])\n",
    "print(tX_test.shape)\n",
    "tX_test = np.c_[(tX_test, positive_columns_log)]\n",
    "print(tX_test.shape)\n",
    "tX_test = (tX_test-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 282)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = np.c_[(np.ones(tX_test.shape[0]) , tX_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
