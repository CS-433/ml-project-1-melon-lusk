{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from extend import *\n",
    "from gradient_descent import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 5\n",
    "tX = build_poly(tX, degree)\n",
    "tX = tX[:,1:]\n",
    "#tX = extend(tX,[np.cos,np.sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 150)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(tX,axis = 0)\n",
    "std = np.std(tX,axis = 0)\n",
    "tX = (tX-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[(np.ones(tX.shape[0]) , tX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 151)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=136.91670092901256, w0=0.7787757677770937, gamma=0.022753978942881625\n",
      "Gradient Descent(1/499): loss=119.69933440856066, w0=0.753895662325475, gamma=0.021734642807117018\n",
      "Gradient Descent(2/499): loss=17.889309521841845, w0=0.7306708997466292, gamma=0.04204775473021056\n",
      "Gradient Descent(3/499): loss=13.32985660827959, w0=0.6867169142709354, gamma=0.10186209466281092\n",
      "Gradient Descent(4/499): loss=8.334493130183079, w0=0.5847141567874387, gamma=0.11683183107074006\n",
      "Gradient Descent(5/499): loss=3.9083963986518047, w0=0.4796381599044067, gamma=0.18535925980473764\n",
      "Gradient Descent(6/499): loss=2.3756190167843285, w0=0.33240689948238966, gamma=0.0498367230690517\n",
      "Gradient Descent(7/499): loss=1.4908574848714744, w0=0.30015900625861025, gamma=0.021783510246117545\n",
      "Gradient Descent(8/499): loss=1.4691271871466134, w0=0.28676600300214117, gamma=0.02167060177805778\n",
      "Gradient Descent(9/499): loss=1.1662891294437279, w0=0.2737326529096114, gamma=0.2938914563730767\n",
      "Gradient Descent(10/499): loss=1.1233747436963795, w0=0.10080790365968659, gamma=0.45161274341776697\n",
      "Gradient Descent(11/499): loss=0.708790061421006, w0=-0.08682450256697655, gamma=0.17259945356051593\n",
      "Gradient Descent(12/499): loss=0.5484868240570894, w0=-0.12614947532412102, gamma=0.08394498018155544\n",
      "Gradient Descent(13/499): loss=0.527153750761719, w0=-0.14197432336232477, gamma=0.05272728488104323\n",
      "Gradient Descent(14/499): loss=0.4850909457757092, w0=-0.15107978113862816, gamma=0.021663678209710633\n",
      "Gradient Descent(15/499): loss=0.4856142457976876, w0=-0.15462361701631863, gamma=0.021626553369388086\n",
      "Gradient Descent(16/499): loss=0.47230508409920846, w0=-0.15808473890026128, gamma=0.7878231167165402\n",
      "Gradient Descent(17/499): loss=0.47031760463790406, w0=-0.2814415003962628, gamma=1.068155457434471\n",
      "Gradient Descent(18/499): loss=0.41790011525145915, w0=-0.3169282946619562, gamma=0.4298282855718894\n",
      "Gradient Descent(19/499): loss=0.39194734800032455, w0=-0.31595503677112413, gamma=0.10015905209913427\n",
      "Gradient Descent(20/499): loss=0.3948647883042265, w0=-0.3158257277523058, gamma=0.04945216369106717\n",
      "Gradient Descent(21/499): loss=0.38578728795748396, w0=-0.3157682778015371, gamma=0.021689476814270775\n",
      "Gradient Descent(22/499): loss=0.38659312635622506, w0=-0.31574432659385104, gamma=0.021615328514339123\n",
      "Gradient Descent(23/499): loss=0.38219054263125, w0=-0.31572097497970986, gamma=0.12381520579564995\n",
      "Gradient Descent(24/499): loss=0.38190621499116706, w0=-0.31559010540557847, gamma=0.16038870966647284\n",
      "Gradient Descent(25/499): loss=0.38038467551219624, w0=-0.31544156855521277, gamma=0.47090875054265974\n",
      "Gradient Descent(26/499): loss=0.3785695368518068, w0=-0.31507540472032547, gamma=0.5518429488031557\n",
      "Gradient Descent(27/499): loss=0.3737235278967372, w0=-0.31484837392854587, gamma=0.10064994847124645\n",
      "Gradient Descent(28/499): loss=0.36965428157072955, w0=-0.31482981670254684, gamma=0.05429340691599585\n",
      "Gradient Descent(29/499): loss=0.36837299049291294, w0=-0.3148208139490626, gamma=0.021825029224944496\n",
      "Gradient Descent(30/499): loss=0.3681502671822686, w0=-0.31481739148012955, gamma=0.021625245136857818\n",
      "Gradient Descent(31/499): loss=0.36765732332066575, w0=-0.31481407435185754, gamma=0.12939720920515593\n",
      "Gradient Descent(32/499): loss=0.36750452826710256, w0=-0.3147946551500785, gamma=0.38076803593426917\n",
      "Gradient Descent(33/499): loss=0.3666083988776058, w0=-0.3147449058467425, gamma=0.9370855266630499\n",
      "Gradient Descent(34/499): loss=0.36413067174313235, w0=-0.3146690901525376, gamma=0.15388451869955005\n",
      "Gradient Descent(35/499): loss=0.3592939071132798, w0=-0.3146683068574897, gamma=0.07816122921620271\n",
      "Gradient Descent(36/499): loss=0.35852111080846394, w0=-0.3146679702285308, gamma=0.030204286328829787\n",
      "Gradient Descent(37/499): loss=0.3580940625812567, w0=-0.3146678503107344, gamma=0.02162019699039452\n",
      "Gradient Descent(38/499): loss=0.35781105850960665, w0=-0.3146677670663455, gamma=0.02170746454401299\n",
      "Gradient Descent(39/499): loss=0.3576911189423488, w0=-0.3146676852929745, gamma=0.55085597599999\n",
      "Gradient Descent(40/499): loss=0.35760066141933916, w0=-0.3146656552295523, gamma=0.8751713752736422\n",
      "Gradient Descent(41/499): loss=0.3554008869620697, w0=-0.3146642066235834, gamma=0.10869544976042812\n",
      "Gradient Descent(42/499): loss=0.3528447275293311, w0=-0.3146641841649812, gamma=0.08444495802024976\n",
      "Gradient Descent(43/499): loss=0.3521225505945983, w0=-0.31466416861352053, gamma=0.08538377064879327\n",
      "Gradient Descent(44/499): loss=0.35184478775945555, w0=-0.31466415421700944, gamma=0.03608224546386042\n",
      "Gradient Descent(45/499): loss=0.3516065573327576, w0=-0.3146641486526602, gamma=0.023218401558286406\n",
      "Gradient Descent(46/499): loss=0.35150657988002476, w0=-0.3146641452012774, gamma=0.025159978158812362\n",
      "Gradient Descent(47/499): loss=0.3514425895930008, w0=-0.3146641415481187, gamma=0.5951125815922373\n",
      "Gradient Descent(48/499): loss=0.35137399411776443, w0=-0.3146640573134721, gamma=5.1059175287858665\n",
      "Gradient Descent(49/499): loss=0.34980594889983413, w0=-0.31466376469638235, gamma=0.04847678771806265\n",
      "Gradient Descent(50/499): loss=0.34278794784693245, w0=-0.3146637761033421, gamma=0.02159841439557655\n",
      "Gradient Descent(51/499): loss=0.3440190898620401, w0=-0.31466378093924424, gamma=0.02159836599948471\n",
      "Gradient Descent(52/499): loss=0.3403935496718295, w0=-0.3146637856706869, gamma=2.511913511336461\n",
      "Gradient Descent(53/499): loss=0.34037313904617794, w0=-0.3146643240576514, gamma=2.955003855742922\n",
      "Gradient Descent(54/499): loss=0.3381668047136253, w0=-0.3146633664781436, gamma=0.17612917344504664\n",
      "Gradient Descent(55/499): loss=0.33740919735797936, w0=-0.3146634780605481, gamma=0.11212329861318403\n",
      "Gradient Descent(56/499): loss=0.33639156483097227, w0=-0.3146635365825809, gamma=0.09661028855999126\n",
      "Gradient Descent(57/499): loss=0.33599750206031226, w0=-0.314663581353868, gamma=0.10518052852327012\n",
      "Gradient Descent(58/499): loss=0.3359219198252423, w0=-0.3146636253877211, gamma=0.22909585473312352\n",
      "Gradient Descent(59/499): loss=0.33585901884033903, w0=-0.31466371121078146, gamma=0.2902157214176261\n",
      "Gradient Descent(60/499): loss=0.3357307314203705, w0=-0.3146637950231439, gamma=0.042015484155346006\n",
      "Gradient Descent(61/499): loss=0.33557431819263567, w0=-0.31466380363551827, gamma=0.021809709577748398\n",
      "Gradient Descent(62/499): loss=0.3355517684774052, w0=-0.3146638079182601, gamma=0.021754173777762634\n",
      "Gradient Descent(63/499): loss=0.3355394586739929, w0=-0.31466381209692895, gamma=0.4441776724454785\n",
      "Gradient Descent(64/499): loss=0.33552789474207584, w0=-0.31466389556110097, gamma=1.01657191195933\n",
      "Gradient Descent(65/499): loss=0.3352939745814667, w0=-0.3146640017349258, gamma=0.13550537579226354\n",
      "Gradient Descent(66/499): loss=0.3347909151614228, w0=-0.31466400150039076, gamma=0.0837743325123602\n",
      "Gradient Descent(67/499): loss=0.3347182903421477, w0=-0.31466400137504064, gamma=0.05268127991480682\n",
      "Gradient Descent(68/499): loss=0.33467446857443917, w0=-0.31466400130281813, gamma=0.02171420820167493\n",
      "Gradient Descent(69/499): loss=0.3346501632054078, w0=-0.3146640012746177, gamma=0.021653877269221317\n",
      "Gradient Descent(70/499): loss=0.3346385399290941, w0=-0.31466400124710625, gamma=1.7740933157948513\n",
      "Gradient Descent(71/499): loss=0.33462830620584266, w0=-0.31466399904191134, gamma=19.991870714913\n",
      "Gradient Descent(72/499): loss=0.33381403311977764, w0=-0.31466401827808205, gamma=1.4323055120124606\n",
      "Gradient Descent(73/499): loss=0.32800739601498624, w0=-0.3146639921042099, gamma=0.06025220292666694\n",
      "Gradient Descent(74/499): loss=0.36844401232666096, w0=-0.31466399258019717, gamma=0.026964238194652025\n",
      "Gradient Descent(75/499): loss=0.35036257405721033, w0=-0.31466399278038154, gamma=0.021749033054610793\n",
      "Gradient Descent(76/499): loss=0.33026885681734014, w0=-0.31466399293749087, gamma=0.02271920684151327\n",
      "Gradient Descent(77/499): loss=0.3284396675149081, w0=-0.31466399309803966, gamma=0.08508231981684768\n",
      "Gradient Descent(78/499): loss=0.32808395647236727, w0=-0.314663993685627, gamma=0.08585406801307058\n",
      "Gradient Descent(79/499): loss=0.3276425648258495, w0=-0.3146639942280969, gamma=0.20580193011397818\n",
      "Gradient Descent(80/499): loss=0.327612840596829, w0=-0.31466399541681733, gamma=0.11214182231418318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(81/499): loss=0.32757249977375724, w0=-0.31466399593124794, gamma=0.02167438012311435\n",
      "Gradient Descent(82/499): loss=0.3275709920026505, w0=-0.31466399601952527, gamma=0.021599950804965927\n",
      "Gradient Descent(83/499): loss=0.3275522410080299, w0=-0.3146639961055927, gamma=0.11558321780885689\n",
      "Gradient Descent(84/499): loss=0.3275491517037961, w0=-0.31466399655619914, gamma=0.4871295468820986\n",
      "Gradient Descent(85/499): loss=0.3275326689734085, w0=-0.3146639982357921, gamma=1.071871726318387\n",
      "Gradient Descent(86/499): loss=0.32746385294773067, w0=-0.3146640001312321, gamma=1.0760252409609317\n",
      "Gradient Descent(87/499): loss=0.3273155085557642, w0=-0.31466399999447486, gamma=0.24092304281535\n",
      "Gradient Descent(88/499): loss=0.32717331726927096, w0=-0.31466399999680267, gamma=0.03130284050906714\n",
      "Gradient Descent(89/499): loss=0.3271457622777653, w0=-0.3146639999970323, gamma=0.021707970658145427\n",
      "Gradient Descent(90/499): loss=0.3271366003712709, w0=-0.3146639999971865, gamma=0.021841099201071836\n",
      "Gradient Descent(91/499): loss=0.3271324550463146, w0=-0.31466399999733835, gamma=0.0965645954393636\n",
      "Gradient Descent(92/499): loss=0.3271295291553264, w0=-0.3146639999979948, gamma=0.10068515836571397\n",
      "Gradient Descent(93/499): loss=0.3271168371149115, w0=-0.3146639999986132, gamma=0.7003650448959885\n",
      "Gradient Descent(94/499): loss=0.3271037540331465, w0=-0.3146640000024817, gamma=4.832562616682532\n",
      "Gradient Descent(95/499): loss=0.32701333911046904, w0=-0.3146640000104781, gamma=0.03664584131264955\n",
      "Gradient Descent(96/499): loss=0.32645129153365177, w0=-0.31466400001024547, gamma=0.02173658105657051\n",
      "Gradient Descent(97/499): loss=0.3264283685043036, w0=-0.3146640000101127, gamma=0.021701741162033047\n",
      "Gradient Descent(98/499): loss=0.32641033115013535, w0=-0.31466400000998296, gamma=0.09328276515820629\n",
      "Gradient Descent(99/499): loss=0.326407397212479, w0=-0.31466400000943734, gamma=0.09996543198470643\n",
      "Gradient Descent(100/499): loss=0.3263960125465948, w0=-0.3146640000089072, gamma=0.15660083316222553\n",
      "Gradient Descent(101/499): loss=0.3263846051960241, w0=-0.3146640000081598, gamma=0.35948415025647096\n",
      "Gradient Descent(102/499): loss=0.32636687428011174, w0=-0.31466400000671274, gamma=0.09300889602852529\n",
      "Gradient Descent(103/499): loss=0.3263263891192366, w0=-0.31466400000647293, gamma=0.02193734547832882\n",
      "Gradient Descent(104/499): loss=0.32631615862887897, w0=-0.31466400000642164, gamma=0.02162381380067052\n",
      "Gradient Descent(105/499): loss=0.3263134931396772, w0=-0.3146640000063722, gamma=0.12416477774773424\n",
      "Gradient Descent(106/499): loss=0.32631107474909754, w0=-0.3146640000060943, gamma=19.643699703476923\n",
      "Gradient Descent(107/499): loss=0.32629720106094245, w0=-0.31466399996758804, gamma=37.60492587854769\n",
      "Gradient Descent(108/499): loss=0.3243097626520797, w0=-0.3146640013416278, gamma=0.05408200566766527\n",
      "Gradient Descent(109/499): loss=0.3374193919729437, w0=-0.3146640012692935, gamma=0.02368938354017483\n",
      "Gradient Descent(110/499): loss=0.33428402337282487, w0=-0.31466400123932, gamma=0.021750502695340623\n",
      "Gradient Descent(111/499): loss=0.3242061174021221, w0=-0.3146640012124535, gamma=0.030206131135466146\n",
      "Gradient Descent(112/499): loss=0.32357237375618914, w0=-0.314664001175954, gamma=0.11271795040916781\n",
      "Gradient Descent(113/499): loss=0.32305013800074217, w0=-0.31466400104386577, gamma=0.13002575569996708\n",
      "Gradient Descent(114/499): loss=0.32211586899598454, w0=-0.31466400090867086, gamma=0.16174844827648877\n",
      "Gradient Descent(115/499): loss=0.3218373171927654, w0=-0.3146640007623595, gamma=0.05832300821111004\n",
      "Gradient Descent(116/499): loss=0.32177415793907727, w0=-0.31466400071813594, gamma=0.02197591020847229\n",
      "Gradient Descent(117/499): loss=0.3217888290549522, w0=-0.31466400070244466, gamma=0.021628940851005182\n",
      "Gradient Descent(118/499): loss=0.3217492808357449, w0=-0.3146640006873404, gamma=0.07123165506579798\n",
      "Gradient Descent(119/499): loss=0.32174735720273145, w0=-0.3146640006386728, gamma=0.1849300711157904\n",
      "Gradient Descent(120/499): loss=0.32174185216570445, w0=-0.31466400052132293, gamma=0.24718768606292432\n",
      "Gradient Descent(121/499): loss=0.3217308421931245, w0=-0.3146640003934741, gamma=0.20259810804822626\n",
      "Gradient Descent(122/499): loss=0.3217184879415889, w0=-0.31466400031458946, gamma=0.03878134874852662\n",
      "Gradient Descent(123/499): loss=0.3217085965013164, w0=-0.3146640003025486, gamma=0.022137189301196535\n",
      "Gradient Descent(124/499): loss=0.32170666335278775, w0=-0.314664000295942, gamma=0.021921643958252992\n",
      "Gradient Descent(125/499): loss=0.32170554494972453, w0=-0.31466400028954455, gamma=0.11018252244810163\n",
      "Gradient Descent(126/499): loss=0.32170447290228943, w0=-0.3146640002580945, gamma=0.18366916018598237\n",
      "Gradient Descent(127/499): loss=0.32169909826507426, w0=-0.3146640002114451, gamma=1.0159548883342764\n",
      "Gradient Descent(128/499): loss=0.3216901750386853, w0=-0.31466400000080064, gamma=0.4114273853083928\n",
      "Gradient Descent(129/499): loss=0.3216413948867995, w0=-0.3146640000021612, gamma=0.025422599036274564\n",
      "Gradient Descent(130/499): loss=0.3216256062929665, w0=-0.3146640000022106, gamma=0.02177249650704356\n",
      "Gradient Descent(131/499): loss=0.32162118101796805, w0=-0.3146640000022519, gamma=0.02421571732433149\n",
      "Gradient Descent(132/499): loss=0.321619958654362, w0=-0.31466400000229683, gamma=0.08521675772934868\n",
      "Gradient Descent(133/499): loss=0.3216187637389925, w0=-0.3146640000024511, gamma=0.08649132200066914\n",
      "Gradient Descent(134/499): loss=0.321614728610592, w0=-0.3146640000025943, gamma=2.3106039887201\n",
      "Gradient Descent(135/499): loss=0.32161071079041736, w0=-0.31466400000608896, gamma=0.368562084521857\n",
      "Gradient Descent(136/499): loss=0.32150497306345877, w0=-0.3146640000053575, gamma=0.02162117971473802\n",
      "Gradient Descent(137/499): loss=0.32150954093196193, w0=-0.31466400000533046, gamma=0.02159837963801947\n",
      "Gradient Descent(138/499): loss=0.3214876690609645, w0=-0.314664000005304, gamma=0.09675952066674808\n",
      "Gradient Descent(139/499): loss=0.3214867201602676, w0=-0.3146640000051879, gamma=5.064099544416496\n",
      "Gradient Descent(140/499): loss=0.3214824720442015, w0=-0.3146639999996992, gamma=19.45351351265286\n",
      "Gradient Descent(141/499): loss=0.321264878784384, w0=-0.31466400008529705, gamma=0.13926945798208845\n",
      "Gradient Descent(142/499): loss=0.3205803076848507, w0=-0.3146640000739867, gamma=0.08423828330747195\n",
      "Gradient Descent(143/499): loss=0.3205383266694157, w0=-0.314664000068098, gamma=0.08343129261939498\n",
      "Gradient Descent(144/499): loss=0.32050780611195845, w0=-0.31466400006275713, gamma=0.021620349598203234\n",
      "Gradient Descent(145/499): loss=0.3205053989106943, w0=-0.3146640000614886, gamma=0.021600679537657253\n",
      "Gradient Descent(146/499): loss=0.3205042544087936, w0=-0.3146640000602486, gamma=1.191869203596799\n",
      "Gradient Descent(147/499): loss=0.32050353546197025, w0=-0.31466399999330597, gamma=2.834535547711454\n",
      "Gradient Descent(148/499): loss=0.3204640127732883, w0=-0.31466400002385025, gamma=0.4795740587133119\n",
      "Gradient Descent(149/499): loss=0.32037223117285685, w0=-0.31466400001436895, gamma=0.2584706086957299\n",
      "Gradient Descent(150/499): loss=0.3203567499817048, w0=-0.31466400001170947, gamma=0.18361654675988492\n",
      "Gradient Descent(151/499): loss=0.3203480795569718, w0=-0.3146640000103085, gamma=0.021852503461938247\n",
      "Gradient Descent(152/499): loss=0.3203430599373732, w0=-0.31466400001017236, gamma=0.021599689013922764\n",
      "Gradient Descent(153/499): loss=0.3203415478189841, w0=-0.31466400001004075, gamma=0.030422343212368235\n",
      "Gradient Descent(154/499): loss=0.32034086413504076, w0=-0.3146640000098594, gamma=0.5749644817603112\n",
      "Gradient Descent(155/499): loss=0.32033990164633014, w0=-0.31466400000653594, gamma=0.8939116473072125\n",
      "Gradient Descent(156/499): loss=0.32032174527185914, w0=-0.3146640000043397, gamma=0.180269011783954\n",
      "Gradient Descent(157/499): loss=0.32029377053200325, w0=-0.3146640000042926, gamma=0.10625457830655366\n",
      "Gradient Descent(158/499): loss=0.32028809900137245, w0=-0.31466400000426986, gamma=0.08752513539690945\n",
      "Gradient Descent(159/499): loss=0.3202847576600733, w0=-0.3146640000042531, gamma=0.04987420275352212\n",
      "Gradient Descent(160/499): loss=0.3202820335264002, w0=-0.3146640000042444, gamma=0.022730378985449894\n",
      "Gradient Descent(161/499): loss=0.32028048376508167, w0=-0.3146640000042406, gamma=0.02224565569149457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(162/499): loss=0.3202797764062405, w0=-0.314664000004237, gamma=0.25174117921971667\n",
      "Gradient Descent(163/499): loss=0.3202790853727462, w0=-0.314664000004197, gamma=15.316847629557337\n",
      "Gradient Descent(164/499): loss=0.3202712711506812, w0=-0.314664000002374, gamma=2.050391999704925\n",
      "Gradient Descent(165/499): loss=0.31981355838315556, w0=-0.3146640000058473, gamma=0.021925899579602136\n",
      "Gradient Descent(166/499): loss=0.3211779756802788, w0=-0.31466400000580763, gamma=0.02161044543048165\n",
      "Gradient Descent(167/499): loss=0.31977325770889364, w0=-0.31466400000577005, gamma=0.04421813837787551\n",
      "Gradient Descent(168/499): loss=0.3197670705812357, w0=-0.3146640000056948, gamma=0.10202376764392677\n",
      "Gradient Descent(169/499): loss=0.31975981772624085, w0=-0.3146640000055287, gamma=0.11116747563507993\n",
      "Gradient Descent(170/499): loss=0.3197535257999806, w0=-0.31466400000536615, gamma=0.11694087101052208\n",
      "Gradient Descent(171/499): loss=0.3197503565343394, w0=-0.3146640000052141, gamma=0.0901603108825617\n",
      "Gradient Descent(172/499): loss=0.31974725747538946, w0=-0.31466400000511063, gamma=0.023964435552087136\n",
      "Gradient Descent(173/499): loss=0.3197448825658749, w0=-0.3146640000050856, gamma=0.02171772062508904\n",
      "Gradient Descent(174/499): loss=0.3197442455217037, w0=-0.31466400000506345, gamma=0.031319885293893894\n",
      "Gradient Descent(175/499): loss=0.3197436729411671, w0=-0.3146640000050322, gamma=3.1749177184881234\n",
      "Gradient Descent(176/499): loss=0.31974284747643955, w0=-0.3146640000019644, gamma=4.173069521870874\n",
      "Gradient Descent(177/499): loss=0.31965978372242126, w0=-0.3146640000107253, gamma=0.16304197729622916\n",
      "Gradient Descent(178/499): loss=0.3195612436035144, w0=-0.31466400000963884, gamma=0.02864684550257551\n",
      "Gradient Descent(179/499): loss=0.3195645229207305, w0=-0.31466400000947914, gamma=0.021667125651093085\n",
      "Gradient Descent(180/499): loss=0.3195514789723262, w0=-0.31466400000936173, gamma=0.021909725234296504\n",
      "Gradient Descent(181/499): loss=0.3195492784094613, w0=-0.3146640000092456, gamma=0.10195033727153421\n",
      "Gradient Descent(182/499): loss=0.31954854943807137, w0=-0.314664000008717, gamma=0.11092744309787425\n",
      "Gradient Descent(183/499): loss=0.319545571234274, w0=-0.31466400000820055, gamma=0.21729243680444857\n",
      "Gradient Descent(184/499): loss=0.31954265248220504, w0=-0.31466400000730105, gamma=0.29395164471208046\n",
      "Gradient Descent(185/499): loss=0.31953717119908315, w0=-0.31466400000634853, gamma=0.05855448227023988\n",
      "Gradient Descent(186/499): loss=0.31952992843534284, w0=-0.31466400000621453, gamma=0.022412398805040434\n",
      "Gradient Descent(187/499): loss=0.3195284999357611, w0=-0.31466400000616623, gamma=0.021641756221454086\n",
      "Gradient Descent(188/499): loss=0.31952792202586977, w0=-0.31466400000612066, gamma=0.041298689178003466\n",
      "Gradient Descent(189/499): loss=0.31952738990823903, w0=-0.31466400000603556, gamma=0.24516361131106856\n",
      "Gradient Descent(190/499): loss=0.3195263749162712, w0=-0.3146640000055514, gamma=0.6881329449641769\n",
      "Gradient Descent(191/499): loss=0.3195203550271078, w0=-0.31466400000452516, gamma=0.25217194503905205\n",
      "Gradient Descent(192/499): loss=0.3195035150511264, w0=-0.3146640000044078, gamma=0.08228377215847818\n",
      "Gradient Descent(193/499): loss=0.3194973973272857, w0=-0.31466400000437916, gamma=0.03980140148778106\n",
      "Gradient Descent(194/499): loss=0.3194953701423246, w0=-0.31466400000436645, gamma=0.021641862283417697\n",
      "Gradient Descent(195/499): loss=0.3194943989064372, w0=-0.3146640000043598, gamma=0.021657783289408424\n",
      "Gradient Descent(196/499): loss=0.31949386446997285, w0=-0.3146640000043533, gamma=5.497191593974873\n",
      "Gradient Descent(197/499): loss=0.31949333807078795, w0=-0.314664000002739, gamma=14.984678682066477\n",
      "Gradient Descent(198/499): loss=0.319361220122197, w0=-0.31466400002247413, gamma=0.08988621159418472\n",
      "Gradient Descent(199/499): loss=0.3192445202766159, w0=-0.31466400002081824, gamma=0.08476183980352095\n",
      "Gradient Descent(200/499): loss=0.3190235551355795, w0=-0.31466400001939665, gamma=0.03762955315824299\n",
      "Gradient Descent(201/499): loss=0.3190202887908941, w0=-0.31466400001881906, gamma=0.021733090529211376\n",
      "Gradient Descent(202/499): loss=0.3190191111037991, w0=-0.314664000018498, gamma=0.021728430816143653\n",
      "Gradient Descent(203/499): loss=0.31901825744923573, w0=-0.31466400001818395, gamma=0.13447839830882624\n",
      "Gradient Descent(204/499): loss=0.31901778685267784, w0=-0.3146640000162828, gamma=0.14093579222605257\n",
      "Gradient Descent(205/499): loss=0.31901495910106825, w0=-0.3146640000145583, gamma=0.7795569033820994\n",
      "Gradient Descent(206/499): loss=0.3190120570135698, w0=-0.3146640000063639, gamma=0.6934304060982736\n",
      "Gradient Descent(207/499): loss=0.3189960499844546, w0=-0.31466400000475675, gamma=0.02258892080864126\n",
      "Gradient Descent(208/499): loss=0.31898241634289204, w0=-0.31466400000474065, gamma=0.021603129992748212\n",
      "Gradient Descent(209/499): loss=0.31898143152896963, w0=-0.31466400000472566, gamma=0.023011385458203793\n",
      "Gradient Descent(210/499): loss=0.31898099038073313, w0=-0.31466400000471, gamma=0.1314308050293667\n",
      "Gradient Descent(211/499): loss=0.31898052197418914, w0=-0.3146640000046227, gamma=0.16898179977756503\n",
      "Gradient Descent(212/499): loss=0.3189778492322603, w0=-0.31466400000452516, gamma=0.32127473663120404\n",
      "Gradient Descent(213/499): loss=0.3189744172914534, w0=-0.314664000004371, gamma=0.31775848172961235\n",
      "Gradient Descent(214/499): loss=0.318967903356966, w0=-0.31466400000426753, gamma=0.11325468788745832\n",
      "Gradient Descent(215/499): loss=0.3189614802157453, w0=-0.31466400000424233, gamma=0.08165035331487248\n",
      "Gradient Descent(216/499): loss=0.3189591886590213, w0=-0.31466400000422623, gamma=0.02710806029877248\n",
      "Gradient Descent(217/499): loss=0.3189575419382585, w0=-0.31466400000422134, gamma=0.021855109064297756\n",
      "Gradient Descent(218/499): loss=0.31895699395754723, w0=-0.3146640000042175, gamma=0.02553927579902054\n",
      "Gradient Descent(219/499): loss=0.31895655323829386, w0=-0.31466400000421313, gamma=20.486783212591966\n",
      "Gradient Descent(220/499): loss=0.31895603842078923, w0=-0.31466400000076933, gamma=96.75297023247013\n",
      "Gradient Descent(221/499): loss=0.3185585012502392, w0=-0.31466400031653874, gamma=0.03254565183355707\n",
      "Gradient Descent(222/499): loss=0.3312139637031838, w0=-0.31466400030636327, gamma=0.022400247965140244\n",
      "Gradient Descent(223/499): loss=0.32149687521203946, w0=-0.31466400029959035, gamma=0.022766875652553992\n",
      "Gradient Descent(224/499): loss=0.3183962114298926, w0=-0.31466400029285996, gamma=0.07681244793957394\n",
      "Gradient Descent(225/499): loss=0.3178130906998067, w0=-0.3146640002706699, gamma=0.08419728543281199\n",
      "Gradient Descent(226/499): loss=0.3171567291496221, w0=-0.3146640002482153, gamma=0.0602413275586982\n",
      "Gradient Descent(227/499): loss=0.3171512276320023, w0=-0.3146640002335023, gamma=0.021599109127626127\n",
      "Gradient Descent(228/499): loss=0.31715168250466924, w0=-0.3146640002285448, gamma=0.021598466124235816\n",
      "Gradient Descent(229/499): loss=0.31715004082549547, w0=-0.31466400022369456, gamma=0.4789899416903231\n",
      "Gradient Descent(230/499): loss=0.31714984714226035, w0=-0.31466400011845364, gamma=0.5249148544631226\n",
      "Gradient Descent(231/499): loss=0.3171455575475925, w0=-0.3146640000583647, gamma=0.27282761701566716\n",
      "Gradient Descent(232/499): loss=0.31714086805611563, w0=-0.314664000043527, gamma=0.12034341789962244\n",
      "Gradient Descent(233/499): loss=0.3171384357651444, w0=-0.31466400003876777, gamma=0.10406572132509534\n",
      "Gradient Descent(234/499): loss=0.31713736123080016, w0=-0.31466400003514755, gamma=0.1549053396425072\n",
      "Gradient Descent(235/499): loss=0.31713643422362453, w0=-0.31466400003031947, gamma=2.964613506836651\n",
      "Gradient Descent(236/499): loss=0.31713505516313406, w0=-0.3146639999522324, gamma=1.32736503110007\n",
      "Gradient Descent(237/499): loss=0.317108753738881, w0=-0.31466400002091843, gamma=0.02288279077469571\n",
      "Gradient Descent(238/499): loss=0.3171008384767346, w0=-0.31466400002053074, gamma=0.02166811705581341\n",
      "Gradient Descent(239/499): loss=0.31709703267770467, w0=-0.3146640000201721, gamma=0.0306950125668007\n",
      "Gradient Descent(240/499): loss=0.317096774037068, w0=-0.314664000019675, gamma=0.08571582655472838\n",
      "Gradient Descent(241/499): loss=0.31709646336654335, w0=-0.3146640000183295, gamma=0.08602052949848142\n",
      "Gradient Descent(242/499): loss=0.3170956838906624, w0=-0.314664000017095, gamma=0.1607443053567706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(243/499): loss=0.31709493161568963, w0=-0.3146640000149865, gamma=0.06958220688524229\n",
      "Gradient Descent(244/499): loss=0.317093526661121, w0=-0.3146640000142205, gamma=0.022736686043295908\n",
      "Gradient Descent(245/499): loss=0.3170929190808609, w0=-0.31466400001398764, gamma=0.02182773596670874\n",
      "Gradient Descent(246/499): loss=0.31709272012765527, w0=-0.31466400001376915, gamma=0.10404818532486203\n",
      "Gradient Descent(247/499): loss=0.3170925294972449, w0=-0.3146640000127504, gamma=78.81658674970566\n",
      "Gradient Descent(248/499): loss=0.3170916209557187, w0=-0.31466399932132494, gamma=37.52064936601825\n",
      "Gradient Descent(249/499): loss=0.31645574144014615, w0=-0.31466402493378814, gamma=0.02179162678339469\n",
      "Gradient Descent(250/499): loss=0.3845696738610107, w0=-0.31466402439052155, gamma=0.021599181732016325\n",
      "Gradient Descent(251/499): loss=0.31677499565161665, w0=-0.3146640238637913, gamma=0.03054728967088833\n",
      "Gradient Descent(252/499): loss=0.3166999689351844, w0=-0.31466402313493735, gamma=0.2751721852874214\n",
      "Gradient Descent(253/499): loss=0.3166160398468132, w0=-0.3146640167699291, gamma=0.2932923411740878\n",
      "Gradient Descent(254/499): loss=0.31624416328358534, w0=-0.3146640118525914, gamma=0.12334238239122564\n",
      "Gradient Descent(255/499): loss=0.31623775058755593, w0=-0.3146640103911499, gamma=0.098448190384891\n",
      "Gradient Descent(256/499): loss=0.31621957925588867, w0=-0.3146640093685473, gamma=0.0917321644021768\n",
      "Gradient Descent(257/499): loss=0.3162175306033845, w0=-0.31466400850951115, gamma=0.02330347375538313\n",
      "Gradient Descent(258/499): loss=0.3162169900342418, w0=-0.31466400831130165, gamma=0.021647595508821504\n",
      "Gradient Descent(259/499): loss=0.3162167350770607, w0=-0.3146640081314671, gamma=0.028331720716692297\n",
      "Gradient Descent(260/499): loss=0.31621659292227344, w0=-0.31466400790120014, gamma=0.361973495681965\n",
      "Gradient Descent(261/499): loss=0.3162164088527651, w0=-0.3146640050425994, gamma=0.3800164564092657\n",
      "Gradient Descent(262/499): loss=0.31621411455528703, w0=-0.3146640031278239, gamma=0.29362150109698154\n",
      "Gradient Descent(263/499): loss=0.3162117713769461, w0=-0.3146640022105829, gamma=0.06491319722874338\n",
      "Gradient Descent(264/499): loss=0.31620998654047827, w0=-0.3146640020673423, gamma=0.023197428182213327\n",
      "Gradient Descent(265/499): loss=0.31620959803157794, w0=-0.31466400201947653, gamma=0.021650242190120256\n",
      "Gradient Descent(266/499): loss=0.3162094432293963, w0=-0.31466400197583955, gamma=0.028600058270425967\n",
      "Gradient Descent(267/499): loss=0.31620931188354, w0=-0.31466400191944294, gamma=0.27257714428478763\n",
      "Gradient Descent(268/499): loss=0.3162091386261138, w0=-0.31466400139731915, gamma=0.42909936809690435\n",
      "Gradient Descent(269/499): loss=0.3162074915536719, w0=-0.31466400079941875, gamma=0.18248702259080526\n",
      "Gradient Descent(270/499): loss=0.3162049171385652, w0=-0.31466400065425315, gamma=0.08804535858040982\n",
      "Gradient Descent(271/499): loss=0.3162038288089855, w0=-0.31466400059699556, gamma=0.042997448110581545\n",
      "Gradient Descent(272/499): loss=0.31620330218991904, w0=-0.31466400057149546, gamma=0.02179002187997858\n",
      "Gradient Descent(273/499): loss=0.3162030464859893, w0=-0.3146640005591283, gamma=0.021785052214687704\n",
      "Gradient Descent(274/499): loss=0.316202915823439, w0=-0.31466400054703336, gamma=1.480395959729284\n",
      "Gradient Descent(275/499): loss=0.3162027863938489, w0=-0.3146639997430316, gamma=4.6706958619552\n",
      "Gradient Descent(276/499): loss=0.316194039555775, w0=-0.3146640009616281, gamma=0.13880584072943444\n",
      "Gradient Descent(277/499): loss=0.31616768004326595, w0=-0.31466400082869433, gamma=0.07754403121348856\n",
      "Gradient Descent(278/499): loss=0.31616655322375986, w0=-0.314664000764739, gamma=0.02570377174052649\n",
      "Gradient Descent(279/499): loss=0.3161661836312092, w0=-0.3146640007451834, gamma=0.02160285263783365\n",
      "Gradient Descent(280/499): loss=0.3161657626199552, w0=-0.3146640007291703, gamma=0.021662220860980443\n",
      "Gradient Descent(281/499): loss=0.316165630494575, w0=-0.31466400071346, gamma=0.09470286589297708\n",
      "Gradient Descent(282/499): loss=0.31616550849796043, w0=-0.3146640006462657, gamma=0.10244853963807238\n",
      "Gradient Descent(283/499): loss=0.31616497568649543, w0=-0.31466400058045957, gamma=1.207638618762701\n",
      "Gradient Descent(284/499): loss=0.3161643996911567, w0=-0.3146639998842231, gamma=12.572278707623033\n",
      "Gradient Descent(285/499): loss=0.31615761924474706, w0=-0.3146640013892392, gamma=0.2137698866982155\n",
      "Gradient Descent(286/499): loss=0.316088452000358, w0=-0.31466400109310155, gamma=0.07955666279542617\n",
      "Gradient Descent(287/499): loss=0.3160879647647429, w0=-0.3146640010064506, gamma=0.03785616129969727\n",
      "Gradient Descent(288/499): loss=0.3160867753392559, w0=-0.31466400096849895, gamma=0.021608016028414227\n",
      "Gradient Descent(289/499): loss=0.3160864054065633, w0=-0.3146640009476565, gamma=0.02160370028036488\n",
      "Gradient Descent(290/499): loss=0.31608607501274383, w0=-0.3146640009272685, gamma=0.08931300973857685\n",
      "Gradient Descent(291/499): loss=0.3160859581127792, w0=-0.3146640008448022, gamma=0.09323590010651182\n",
      "Gradient Descent(292/499): loss=0.3160854756503375, w0=-0.3146640007664026, gamma=1.0935509557454985\n",
      "Gradient Descent(293/499): loss=0.3160849724871339, w0=-0.3146639999325983, gamma=11.280330377519462\n",
      "Gradient Descent(294/499): loss=0.3160790769664589, w0=-0.31466400073721873, gamma=0.40799731996387345\n",
      "Gradient Descent(295/499): loss=0.31601910211401324, w0=-0.31466400043803666, gamma=0.10157341097381897\n",
      "Gradient Descent(296/499): loss=0.3160179420463587, w0=-0.31466400039394243, gamma=0.032383666754970675\n",
      "Gradient Descent(297/499): loss=0.31601715637799943, w0=-0.31466400038131226, gamma=0.02164679797256549\n",
      "Gradient Descent(298/499): loss=0.3160163093213494, w0=-0.314664000373143, gamma=0.021692196692656037\n",
      "Gradient Descent(299/499): loss=0.3160159703208385, w0=-0.31466400036513387, gamma=0.1123639549413433\n",
      "Gradient Descent(300/499): loss=0.316015849572548, w0=-0.31466400032454694, gamma=0.11713388968221494\n",
      "Gradient Descent(301/499): loss=0.31601524773590056, w0=-0.3146640002869912, gamma=0.11430950372268285\n",
      "Gradient Descent(302/499): loss=0.3160146364388625, w0=-0.314664000254634, gamma=0.1448272223012487\n",
      "Gradient Descent(303/499): loss=0.31601404100608643, w0=-0.3146640002183244, gamma=0.2059227816289791\n",
      "Gradient Descent(304/499): loss=0.316013286994863, w0=-0.31466400017417456, gamma=0.03037947863095226\n",
      "Gradient Descent(305/499): loss=0.31601221568551047, w0=-0.3146640001690024, gamma=0.021728200836861878\n",
      "Gradient Descent(306/499): loss=0.3160120573979174, w0=-0.31466400016541557, gamma=0.022316412052723028\n",
      "Gradient Descent(307/499): loss=0.31601194432955065, w0=-0.3146640001618117, gamma=1.5692430599408984\n",
      "Gradient Descent(308/499): loss=0.31601182825985313, w0=-0.3146639999140481, gamma=3.040227947663603\n",
      "Gradient Descent(309/499): loss=0.31600367547529795, w0=-0.31466400018729157, gamma=0.09362856360691413\n",
      "Gradient Descent(310/499): loss=0.3159882144300975, w0=-0.314664000170123, gamma=0.08140404524526056\n",
      "Gradient Descent(311/499): loss=0.3159874831078394, w0=-0.3146640001565937, gamma=0.023047685576112118\n",
      "Gradient Descent(312/499): loss=0.31598707689300926, w0=-0.314664000153075, gamma=0.021600406874156278\n",
      "Gradient Descent(313/499): loss=0.3159869411024326, w0=-0.31466400014985324, gamma=0.0220443008329596\n",
      "Gradient Descent(314/499): loss=0.31598683006882305, w0=-0.3146640001466363, gamma=1.8622388971334511\n",
      "Gradient Descent(315/499): loss=0.3159867168404241, w0=-0.3146639998808699, gamma=3.268281697495148\n",
      "Gradient Descent(316/499): loss=0.31597716370578377, w0=-0.3146640002830396, gamma=0.09282225468859757\n",
      "Gradient Descent(317/499): loss=0.31596086258712175, w0=-0.3146640002571312, gamma=0.08441475754985622\n",
      "Gradient Descent(318/499): loss=0.3159600263164884, w0=-0.3146640002357566, gamma=0.08844596458537278\n",
      "Gradient Descent(319/499): loss=0.315959593332735, w0=-0.3146640002152517, gamma=0.05426575405981707\n",
      "Gradient Descent(320/499): loss=0.3159591446764525, w0=-0.3146640002037837, gamma=0.023430724279085248\n",
      "Gradient Descent(321/499): loss=0.31595886968731507, w0=-0.3146640001991008, gamma=0.022188147806286425\n",
      "Gradient Descent(322/499): loss=0.31595875085627695, w0=-0.31466400019477014, gamma=0.08564811784725053\n",
      "Gradient Descent(323/499): loss=0.3159586384391641, w0=-0.3146640001784243, gamma=0.5511951077801989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(324/499): loss=0.3159582045747834, w0=-0.314664000082239, gamma=0.9607663601209009\n",
      "Gradient Descent(325/499): loss=0.3159554143241113, w0=-0.3146640000069936, gamma=0.0369624086092225\n",
      "Gradient Descent(326/499): loss=0.3159505652286714, w0=-0.31466400000688005, gamma=0.021635491193343567\n",
      "Gradient Descent(327/499): loss=0.31595037565123313, w0=-0.31466400000681605, gamma=0.02165200251090433\n",
      "Gradient Descent(328/499): loss=0.3159502635030297, w0=-0.3146640000067534, gamma=0.2723346714876858\n",
      "Gradient Descent(329/499): loss=0.31595015419152295, w0=-0.31466400000598194, gamma=0.29213442990500016\n",
      "Gradient Descent(330/499): loss=0.31594878042571456, w0=-0.31466400000537986, gamma=0.8494444260277962\n",
      "Gradient Descent(331/499): loss=0.31594730821318456, w0=-0.31466400000414063, gamma=1.13429675812584\n",
      "Gradient Descent(332/499): loss=0.31594303232508303, w0=-0.3146640000038908, gamma=0.07234300903583371\n",
      "Gradient Descent(333/499): loss=0.31593734998047057, w0=-0.3146640000038929, gamma=0.022627842615817424\n",
      "Gradient Descent(334/499): loss=0.31593699926355945, w0=-0.3146640000038935, gamma=0.02164274147357352\n",
      "Gradient Descent(335/499): loss=0.31593686107715263, w0=-0.31466400000389405, gamma=0.03412919626082375\n",
      "Gradient Descent(336/499): loss=0.31593675209369027, w0=-0.31466400000389494, gamma=0.14800478235674303\n",
      "Gradient Descent(337/499): loss=0.3159365805735554, w0=-0.31466400000389866, gamma=0.15409769224814476\n",
      "Gradient Descent(338/499): loss=0.3159358384311687, w0=-0.314664000003902, gamma=1.2545885701702244\n",
      "Gradient Descent(339/499): loss=0.3159350667687401, w0=-0.31466400000392475, gamma=0.33174728887337307\n",
      "Gradient Descent(340/499): loss=0.31592879144241875, w0=-0.3146640000039231, gamma=0.021709231914082784\n",
      "Gradient Descent(341/499): loss=0.3159272036185299, w0=-0.31466400000392303, gamma=0.021598819291340544\n",
      "Gradient Descent(342/499): loss=0.3159270276257816, w0=-0.314664000003923, gamma=0.04063861420815076\n",
      "Gradient Descent(343/499): loss=0.31592691992963645, w0=-0.3146640000039228, gamma=1.825072000105543\n",
      "Gradient Descent(344/499): loss=0.3159267173151807, w0=-0.3146640000039168, gamma=3.316761177883722\n",
      "Gradient Descent(345/499): loss=0.31591762950500124, w0=-0.3146640000039239, gamma=0.17031560537825075\n",
      "Gradient Descent(346/499): loss=0.3159013833886962, w0=-0.3146640000039229, gamma=0.1370479570449999\n",
      "Gradient Descent(347/499): loss=0.3159003771610766, w0=-0.31466400000392225, gamma=0.09893224471754374\n",
      "Gradient Descent(348/499): loss=0.3158996919011085, w0=-0.31466400000392186, gamma=0.029030015939405334\n",
      "Gradient Descent(349/499): loss=0.31589920605312005, w0=-0.31466400000392175, gamma=0.021633554702033434\n",
      "Gradient Descent(350/499): loss=0.315899061217524, w0=-0.31466400000392164, gamma=0.02179695958959335\n",
      "Gradient Descent(351/499): loss=0.3158989545603704, w0=-0.31466400000392153, gamma=0.2567217057782485\n",
      "Gradient Descent(352/499): loss=0.31589884738655144, w0=-0.31466400000392064, gamma=0.37854786381386346\n",
      "Gradient Descent(353/499): loss=0.3158975854609186, w0=-0.31466400000391953, gamma=0.16219453981187554\n",
      "Gradient Descent(354/499): loss=0.31589572626590495, w0=-0.3146640000039192, gamma=0.09060011383282256\n",
      "Gradient Descent(355/499): loss=0.31589493014884257, w0=-0.31466400000391903, gamma=0.0902406743125692\n",
      "Gradient Descent(356/499): loss=0.3158944853869538, w0=-0.3146640000039189, gamma=0.05709485933988906\n",
      "Gradient Descent(357/499): loss=0.31589404273384125, w0=-0.31466400000391886, gamma=0.025040631086495922\n",
      "Gradient Descent(358/499): loss=0.3158937627364746, w0=-0.3146640000039188, gamma=0.02287216443894967\n",
      "Gradient Descent(359/499): loss=0.3158936399204074, w0=-0.3146640000039188, gamma=0.07173510684057705\n",
      "Gradient Descent(360/499): loss=0.31589352776730295, w0=-0.31466400000391875, gamma=13.988486384717186\n",
      "Gradient Descent(361/499): loss=0.31589317604815603, w0=-0.3146640000039006, gamma=2.680580125411958\n",
      "Gradient Descent(362/499): loss=0.3158251928499408, w0=-0.31466400000393613, gamma=0.021604412914520015\n",
      "Gradient Descent(363/499): loss=0.31588438554845655, w0=-0.3146640000039358, gamma=0.021598294433376717\n",
      "Gradient Descent(364/499): loss=0.3158124106921642, w0=-0.3146640000039353, gamma=0.039909052726595895\n",
      "Gradient Descent(365/499): loss=0.31581230898904855, w0=-0.3146640000039344, gamma=0.15026823772437342\n",
      "Gradient Descent(366/499): loss=0.31581212112608054, w0=-0.3146640000039312, gamma=0.24232185336416467\n",
      "Gradient Descent(367/499): loss=0.31581141406994717, w0=-0.31466400000392675, gamma=0.36838242500599633\n",
      "Gradient Descent(368/499): loss=0.3158102744914235, w0=-0.31466400000392164, gamma=0.16846157585511085\n",
      "Gradient Descent(369/499): loss=0.31580854340744563, w0=-0.31466400000392014, gamma=0.09163869053065152\n",
      "Gradient Descent(370/499): loss=0.31580775228753927, w0=-0.3146640000039195, gamma=0.09163335044926489\n",
      "Gradient Descent(371/499): loss=0.31580732186777166, w0=-0.31466400000391886, gamma=1.0399135629816452\n",
      "Gradient Descent(372/499): loss=0.3158068917896232, w0=-0.31466400000391254, gamma=48.45960764289057\n",
      "Gradient Descent(373/499): loss=0.3158020148312062, w0=-0.3146640000039065, gamma=0.06475907640919662\n",
      "Gradient Descent(374/499): loss=0.31558560655326773, w0=-0.3146640000039059, gamma=0.021778140919264104\n",
      "Gradient Descent(375/499): loss=0.3155947717932564, w0=-0.31466400000390576, gamma=0.021600711542435206\n",
      "Gradient Descent(376/499): loss=0.3155813620862434, w0=-0.3146640000039056, gamma=0.03630600592533621\n",
      "Gradient Descent(377/499): loss=0.3155812636380503, w0=-0.3146640000039054, gamma=0.08447643497538428\n",
      "Gradient Descent(378/499): loss=0.31558110605682377, w0=-0.3146640000039048, gamma=0.08487591190109398\n",
      "Gradient Descent(379/499): loss=0.31558075103127353, w0=-0.3146640000039043, gamma=1.0591054111182046\n",
      "Gradient Descent(380/499): loss=0.3155803974311198, w0=-0.31466400000389877, gamma=2.664605045668805\n",
      "Gradient Descent(381/499): loss=0.3155759885729296, w0=-0.3146640000038991, gamma=0.07005789143328216\n",
      "Gradient Descent(382/499): loss=0.3155649638645201, w0=-0.31466400000389905, gamma=0.02191247110760509\n",
      "Gradient Descent(383/499): loss=0.31556471787533824, w0=-0.31466400000389905, gamma=0.021625156295581642\n",
      "Gradient Descent(384/499): loss=0.3155645577332359, w0=-0.31466400000389905, gamma=0.08686530083421655\n",
      "Gradient Descent(385/499): loss=0.31556446706893504, w0=-0.314664000003899, gamma=0.20592453791242035\n",
      "Gradient Descent(386/499): loss=0.3155641047627765, w0=-0.31466400000389877, gamma=0.21358388822927335\n",
      "Gradient Descent(387/499): loss=0.31556325271218066, w0=-0.3146640000038986, gamma=1.3535130885596305\n",
      "Gradient Descent(388/499): loss=0.3155623711330023, w0=-0.31466400000389794, gamma=0.04087975826153187\n",
      "Gradient Descent(389/499): loss=0.3155567949202956, w0=-0.31466400000389794, gamma=0.021603878931343617\n",
      "Gradient Descent(390/499): loss=0.3155566258378266, w0=-0.31466400000389794, gamma=0.021604954332829048\n",
      "Gradient Descent(391/499): loss=0.3155565334236409, w0=-0.31466400000389794, gamma=3.0637353162233993\n",
      "Gradient Descent(392/499): loss=0.3155564445525716, w0=-0.314664000003897, gamma=4.377496854620325\n",
      "Gradient Descent(393/499): loss=0.3155438640067683, w0=-0.31466400000389594, gamma=0.14715751655474196\n",
      "Gradient Descent(394/499): loss=0.3155265979990585, w0=-0.31466400000389594, gamma=0.13741876596122152\n",
      "Gradient Descent(395/499): loss=0.31552542648568427, w0=-0.3146640000038959, gamma=0.12415032881244366\n",
      "Gradient Descent(396/499): loss=0.3155248670339462, w0=-0.3146640000038959, gamma=0.028259089941315707\n",
      "Gradient Descent(397/499): loss=0.3155243650950925, w0=-0.3146640000038959, gamma=0.021711385280499384\n",
      "Gradient Descent(398/499): loss=0.31552425043434196, w0=-0.3146640000038959, gamma=0.022460476119003907\n",
      "Gradient Descent(399/499): loss=0.3155241625902825, w0=-0.3146640000038959, gamma=0.33620033883529543\n",
      "Gradient Descent(400/499): loss=0.3155240717610987, w0=-0.31466400000389577, gamma=0.5365978357861946\n",
      "Gradient Descent(401/499): loss=0.31552271254695863, w0=-0.31466400000389566, gamma=0.13975056258104623\n",
      "Gradient Descent(402/499): loss=0.31552054556635745, w0=-0.31466400000389566, gamma=0.08509401839663024\n",
      "Gradient Descent(403/499): loss=0.3155199811809165, w0=-0.3146640000038956, gamma=0.0518383421605788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(404/499): loss=0.31551963760347224, w0=-0.3146640000038956, gamma=0.022018372317571\n",
      "Gradient Descent(405/499): loss=0.31551942853356807, w0=-0.3146640000038956, gamma=0.021812125783324875\n",
      "Gradient Descent(406/499): loss=0.3155193396169704, w0=-0.3146640000038956, gamma=0.584294422967372\n",
      "Gradient Descent(407/499): loss=0.31551925163773026, w0=-0.31466400000389544, gamma=166.23950971536303\n",
      "Gradient Descent(408/499): loss=0.31551689570280145, w0=-0.3146640000038568, gamma=5.992701507352341\n",
      "Gradient Descent(409/499): loss=0.314906943434087, w0=-0.31466400000384126, gamma=0.023086211389372367\n",
      "Gradient Descent(410/499): loss=0.32748726915575954, w0=-0.3146640000038394, gamma=0.021686960686082026\n",
      "Gradient Descent(411/499): loss=0.3155205965206903, w0=-0.3146640000038397, gamma=0.03025142566075423\n",
      "Gradient Descent(412/499): loss=0.315224544794527, w0=-0.31466400000384004, gamma=0.08639393844671021\n",
      "Gradient Descent(413/499): loss=0.3150441669647464, w0=-0.314664000003841, gamma=0.08750051923914506\n",
      "Gradient Descent(414/499): loss=0.31490184311439934, w0=-0.31466400000384204, gamma=0.1784269767000085\n",
      "Gradient Descent(415/499): loss=0.3148933951066893, w0=-0.314664000003844, gamma=0.07221297414809674\n",
      "Gradient Descent(416/499): loss=0.31488925102460746, w0=-0.3146640000038446, gamma=0.021644062364374887\n",
      "Gradient Descent(417/499): loss=0.31489190410628204, w0=-0.3146640000038448, gamma=0.021602027755395294\n",
      "Gradient Descent(418/499): loss=0.31488823215740697, w0=-0.314664000003845, gamma=0.1776696898186999\n",
      "Gradient Descent(419/499): loss=0.3148881606921238, w0=-0.31466400000384637, gamma=0.22467317479472404\n",
      "Gradient Descent(420/499): loss=0.314887617093118, w0=-0.3146640000038478, gamma=0.2218664279688806\n",
      "Gradient Descent(421/499): loss=0.3148869878167638, w0=-0.31466400000384886, gamma=0.12598919265955716\n",
      "Gradient Descent(422/499): loss=0.3148863689433802, w0=-0.3146640000038493, gamma=0.0903105497079847\n",
      "Gradient Descent(423/499): loss=0.3148860175628189, w0=-0.3146640000038496, gamma=0.025177983313185073\n",
      "Gradient Descent(424/499): loss=0.3148857657996565, w0=-0.31466400000384964, gamma=0.021837523520082735\n",
      "Gradient Descent(425/499): loss=0.3148856955639242, w0=-0.3146640000038497, gamma=0.030295669603467554\n",
      "Gradient Descent(426/499): loss=0.31488563468594605, w0=-0.31466400000384975, gamma=38.52649241085031\n",
      "Gradient Descent(427/499): loss=0.3148855502334592, w0=-0.31466400000395084, gamma=114.72827359354359\n",
      "Gradient Descent(428/499): loss=0.3147797527229318, w0=-0.3146639999917991, gamma=0.02850701519766883\n",
      "Gradient Descent(429/499): loss=0.3177200217495174, w0=-0.3146639999921426, gamma=0.02200710580696118\n",
      "Gradient Descent(430/499): loss=0.315203292028029, w0=-0.31466399999239925, gamma=0.023435903713566842\n",
      "Gradient Descent(431/499): loss=0.31477753993530294, w0=-0.31466399999266687, gamma=0.08953722698123312\n",
      "Gradient Descent(432/499): loss=0.3146563085851835, w0=-0.31466399999366546, gamma=0.09146340560101122\n",
      "Gradient Descent(433/499): loss=0.31450298628965345, w0=-0.3146639999945947, gamma=0.08582760974484036\n",
      "Gradient Descent(434/499): loss=0.314498766658406, w0=-0.3146639999953869, gamma=0.025481484162711298\n",
      "Gradient Descent(435/499): loss=0.3144968783179989, w0=-0.31466399999560196, gamma=0.021898044075222632\n",
      "Gradient Descent(436/499): loss=0.3144958066681235, w0=-0.31466399999578204, gamma=0.029887299513815472\n",
      "Gradient Descent(437/499): loss=0.31449535176639654, w0=-0.3146639999960224, gamma=0.5445341718391652\n",
      "Gradient Descent(438/499): loss=0.31449479625723287, w0=-0.31466400000027095, gamma=0.560784515427799\n",
      "Gradient Descent(439/499): loss=0.3144890500121626, w0=-0.3146640000022633, gamma=0.06723826262462371\n",
      "Gradient Descent(440/499): loss=0.3144879415465322, w0=-0.3146640000023682, gamma=0.021786035740051002\n",
      "Gradient Descent(441/499): loss=0.31448833156790723, w0=-0.3146640000023999, gamma=0.021617020159370913\n",
      "Gradient Descent(442/499): loss=0.31448746069138367, w0=-0.31466400000243067, gamma=0.1178815366138\n",
      "Gradient Descent(443/499): loss=0.3144874008612716, w0=-0.31466400000259487, gamma=0.20942264497180504\n",
      "Gradient Descent(444/499): loss=0.3144870994831081, w0=-0.31466400000285216, gamma=0.2200975627276809\n",
      "Gradient Descent(445/499): loss=0.31448662335601774, w0=-0.314664000003066, gamma=0.26968739487150584\n",
      "Gradient Descent(446/499): loss=0.314486134108442, w0=-0.31466400000327027, gamma=0.025483831078877753\n",
      "Gradient Descent(447/499): loss=0.3144855363284832, w0=-0.31466400000328437, gamma=0.02163139498434926\n",
      "Gradient Descent(448/499): loss=0.3144854790451356, w0=-0.31466400000329603, gamma=0.022301847194507403\n",
      "Gradient Descent(449/499): loss=0.31448543103321863, w0=-0.3146640000033078, gamma=0.20825418948688426\n",
      "Gradient Descent(450/499): loss=0.31448538156227057, w0=-0.3146640000034152, gamma=0.2827621427408897\n",
      "Gradient Descent(451/499): loss=0.31448491967761943, w0=-0.3146640000035306, gamma=0.21361963347841048\n",
      "Gradient Descent(452/499): loss=0.3144842927607201, w0=-0.3146640000035931, gamma=0.11959786293409305\n",
      "Gradient Descent(453/499): loss=0.3144838193082919, w0=-0.31466400000362066, gamma=0.09729101374986995\n",
      "Gradient Descent(454/499): loss=0.31448355426486313, w0=-0.31466400000364037, gamma=0.030441245507088044\n",
      "Gradient Descent(455/499): loss=0.3144833387199013, w0=-0.3146640000036459, gamma=0.022108060794927036\n",
      "Gradient Descent(456/499): loss=0.3144832712708927, w0=-0.31466400000364986, gamma=0.024621767982014597\n",
      "Gradient Descent(457/499): loss=0.3144832222934456, w0=-0.31466400000365413, gamma=4.42048707702141\n",
      "Gradient Descent(458/499): loss=0.3144831677512662, w0=-0.3146640000044021, gamma=31.784619395461107\n",
      "Gradient Descent(459/499): loss=0.31447338897067123, w0=-0.3146639999859839, gamma=0.040341582120475984\n",
      "Gradient Descent(460/499): loss=0.31440854001223306, w0=-0.31466399998670325, gamma=0.02260833261164747\n",
      "Gradient Descent(461/499): loss=0.31440631740373715, w0=-0.3146639999870902, gamma=0.02196755042472052\n",
      "Gradient Descent(462/499): loss=0.314404319047689, w0=-0.31466399998745764, gamma=0.05889470521248054\n",
      "Gradient Descent(463/499): loss=0.31440411901529375, w0=-0.31466399998842115, gamma=0.08497078743995573\n",
      "Gradient Descent(464/499): loss=0.3144038199869668, w0=-0.3146639999897294, gamma=0.09006143713033536\n",
      "Gradient Descent(465/499): loss=0.3144036030142968, w0=-0.3146639999909982, gamma=0.07128014786309554\n",
      "Gradient Descent(466/499): loss=0.31440339516945487, w0=-0.31466399999191197, gamma=0.025587017232055857\n",
      "Gradient Descent(467/499): loss=0.31440323489008876, w0=-0.3146639999922166, gamma=0.022204261474329207\n",
      "Gradient Descent(468/499): loss=0.3144031761571973, w0=-0.3146639999924742, gamma=0.03765633841273418\n",
      "Gradient Descent(469/499): loss=0.3144031265554414, w0=-0.31466399999290134, gamma=0.6551321505552732\n",
      "Gradient Descent(470/499): loss=0.31440304283757264, w0=-0.3146640000000529, gamma=0.664020858373419\n",
      "Gradient Descent(471/499): loss=0.3144016220539532, w0=-0.31466400000255257, gamma=0.02211075295855852\n",
      "Gradient Descent(472/499): loss=0.3144002313739696, w0=-0.31466400000258055, gamma=0.021598717674355002\n",
      "Gradient Descent(473/499): loss=0.3144001680646243, w0=-0.31466400000260725, gamma=0.022340654513746304\n",
      "Gradient Descent(474/499): loss=0.3144001223152928, w0=-0.3146640000026343, gamma=3.0115406992844274\n",
      "Gradient Descent(475/499): loss=0.3144000750053639, w0=-0.3146640000061964, gamma=3.5421669895527423\n",
      "Gradient Descent(476/499): loss=0.31439370379055803, w0=-0.3146639999977674, gamma=0.20772089011626427\n",
      "Gradient Descent(477/499): loss=0.31438628932179374, w0=-0.31466399999902384, gamma=0.09860385325427665\n",
      "Gradient Descent(478/499): loss=0.3143858357294913, w0=-0.3146639999994964, gamma=0.08960228584000472\n",
      "Gradient Descent(479/499): loss=0.31438559549215433, w0=-0.3146639999998834, gamma=0.09010518525472536\n",
      "Gradient Descent(480/499): loss=0.31438540544896987, w0=-0.31466400000023775, gamma=0.028063904568299568\n",
      "Gradient Descent(481/499): loss=0.31438521559509375, w0=-0.3146640000003382, gamma=0.021966624985595366\n",
      "Gradient Descent(482/499): loss=0.3143851563153638, w0=-0.31466400000041456, gamma=0.02509416907669206\n",
      "Gradient Descent(483/499): loss=0.31438511005962083, w0=-0.31466400000049993, gamma=0.37538008356321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(484/499): loss=0.3143850572536727, w0=-0.3146640000017448, gamma=0.39317635357397807\n",
      "Gradient Descent(485/499): loss=0.3143842682464535, w0=-0.3146640000025592, gamma=0.18386220942623552\n",
      "Gradient Descent(486/499): loss=0.3143834427944204, w0=-0.31466400000279027, gamma=0.022149031770086564\n",
      "Gradient Descent(487/499): loss=0.31438305754496126, w0=-0.31466400000281297, gamma=0.02160814232752351\n",
      "Gradient Descent(488/499): loss=0.3143830104435971, w0=-0.3146640000028346, gamma=0.036365306314950094\n",
      "Gradient Descent(489/499): loss=0.31438296510337244, w0=-0.3146640000028703, gamma=4.656936066170878\n",
      "Gradient Descent(490/499): loss=0.31438288880117726, w0=-0.3146640000072747, gamma=13.746705922642011\n",
      "Gradient Descent(491/499): loss=0.31437313062233174, w0=-0.3146639999597195, gamma=0.089304398008611\n",
      "Gradient Descent(492/499): loss=0.3143464336034141, w0=-0.3146639999636573, gamma=0.0828821727446893\n",
      "Gradient Descent(493/499): loss=0.31434438086719835, w0=-0.3146639999669855, gamma=0.022612337637939087\n",
      "Gradient Descent(494/499): loss=0.3143442533243517, w0=-0.3146639999678183, gamma=0.02160041614961623\n",
      "Gradient Descent(495/499): loss=0.31434414850669345, w0=-0.3146639999685958, gamma=0.022428637420919762\n",
      "Gradient Descent(496/499): loss=0.3143441037732751, w0=-0.31466399996938565, gamma=0.3583396222499096\n",
      "Gradient Descent(497/499): loss=0.3143440574786984, w0=-0.3146639999817223, gamma=0.38802605334678747\n",
      "Gradient Descent(498/499): loss=0.31434331919013314, w0=-0.31466399999029415, gamma=0.177453512135364\n",
      "Gradient Descent(499/499): loss=0.3143425218094218, w0=-0.3146639999926931, gamma=0.11282405575659442\n"
     ]
    }
   ],
   "source": [
    "w = np.array(np.random.rand((tX.shape[1])))\n",
    "max_iters = 500 #choosing number of iterations\n",
    "gamma = 5 *10**(-2)\n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)\n",
    "(w, loss) =adaptative_step_gradient_descent(y, tX, w, max_iters, gamma)\n",
    "#gamma = 5/max_iters #choosing gamma \n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = build_poly(tX_test, degree)\n",
    "tX_test = tX_test[:,1:]\n",
    "tX_test = (tX_test-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = np.c_[(np.ones(tX_test.shape[0]) , tX_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/output.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
