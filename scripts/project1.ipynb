{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from extend import *\n",
    "from gradient_descent import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 15\n",
    "tX = build_poly(tX, degree)\n",
    "tX = tX[:,1:]\n",
    "#tX = extend(tX,[np.cos,np.sin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 450)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = (tX - np.mean(tX,axis = 0))/(np.std(tX,axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[(np.ones(tX.shape[0]) , tX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 451)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=986.4215558950696, w0=0.04565292360638737, gamma=0.012215464217372076\n",
      "Gradient Descent(1/499): loss=2518.4041778607757, w0=0.041251485118233795, gamma=0.008344484423803684\n",
      "Gradient Descent(2/499): loss=653.9301952574614, w0=0.03828155389634167, gamma=0.008365219142401684\n",
      "Gradient Descent(3/499): loss=101.5487171357724, w0=0.03532908699235974, gamma=0.025107715638461113\n",
      "Gradient Descent(4/499): loss=60.79684939637473, w0=0.026541560087828715, gamma=0.027091316327138428\n",
      "Gradient Descent(5/499): loss=18.97519610611461, w0=0.01729785232594001, gamma=0.055813018593930126\n",
      "Gradient Descent(6/499): loss=12.060288636029108, w0=-0.00122994071238677, gamma=0.07469009574989682\n",
      "Gradient Descent(7/499): loss=5.2391010166140966, w0=-0.024640360614497248, gamma=0.00983838745577261\n",
      "Gradient Descent(8/499): loss=5.977168987682307, w0=-0.027493725550430077, gamma=0.008199026298287605\n",
      "Gradient Descent(9/499): loss=2.3017676033766072, w0=-0.029848242183016634, gamma=0.008329794550479256\n",
      "Gradient Descent(10/499): loss=2.051535240771615, w0=-0.032220698930660804, gamma=0.11197703781853847\n",
      "Gradient Descent(11/499): loss=1.9588548597336042, w0=-0.06384786313998803, gamma=0.12845974629502194\n",
      "Gradient Descent(12/499): loss=1.1765019122106661, w0=-0.09606764045217384, gamma=0.03387957308995849\n",
      "Gradient Descent(13/499): loss=1.090097548360122, w0=-0.10347359179384488, gamma=0.027321107212520693\n",
      "Gradient Descent(14/499): loss=0.9347387587086746, w0=-0.10924354757964698, gamma=0.024851798250868088\n",
      "Gradient Descent(15/499): loss=0.9042194151725985, w0=-0.11434861522065995, gamma=0.013372119938280717\n",
      "Gradient Descent(16/499): loss=0.8854989979203568, w0=-0.11702725657187474, gamma=0.009114252226003608\n",
      "Gradient Descent(17/499): loss=0.8757792825729248, w0=-0.11882856770092014, gamma=0.01037303623863031\n",
      "Gradient Descent(18/499): loss=0.8693426421855206, w0=-0.12085997573732529, gamma=0.13476225485213655\n",
      "Gradient Descent(19/499): loss=0.8622802863424431, w0=-0.14697744305104518, gamma=0.4440003649370696\n",
      "Gradient Descent(20/499): loss=0.7788992803034105, w0=-0.22143033554677988, gamma=0.013969796765898494\n",
      "Gradient Descent(21/499): loss=0.6643929307677905, w0=-0.2227327908914174, gamma=0.008193120108996938\n",
      "Gradient Descent(22/499): loss=0.6314113510578104, w0=-0.22348599432969127, gamma=0.008193465297653286\n",
      "Gradient Descent(23/499): loss=0.599967012967335, w0=-0.22423305815534342, gamma=0.16476902889670172\n",
      "Gradient Descent(24/499): loss=0.5983631067852017, w0=-0.23913327663099732, gamma=0.2544773457656601\n",
      "Gradient Descent(25/499): loss=0.5684318990253626, w0=-0.25835413464649704, gamma=0.03827086068157384\n",
      "Gradient Descent(26/499): loss=0.5389288409717125, w0=-0.26050916165976185, gamma=0.02782531669419519\n",
      "Gradient Descent(27/499): loss=0.5289988618499837, w0=-0.26201603718806343, gamma=0.025905146991121897\n",
      "Gradient Descent(28/499): loss=0.525112665394225, w0=-0.2633798904043849, gamma=0.04769001507968948\n",
      "Gradient Descent(29/499): loss=0.5224175605225786, w0=-0.26582563036599616, gamma=0.8077552560548241\n",
      "Gradient Descent(30/499): loss=0.517643534525465, w0=-0.305275080162921, gamma=0.16374659677246944\n",
      "Gradient Descent(31/499): loss=0.45611954050499304, w0=-0.3068124838392703, gamma=0.008274082421303243\n",
      "Gradient Descent(32/499): loss=0.7004098779833966, w0=-0.30687744793139826, gamma=0.00819868544549031\n",
      "Gradient Descent(33/499): loss=0.44986802807216664, w0=-0.3069412874227968, gamma=0.018730619579261067\n",
      "Gradient Descent(34/499): loss=0.4488699470498794, w0=-0.3070859386148471, gamma=0.024691339332851243\n",
      "Gradient Descent(35/499): loss=0.44769023692928045, w0=-0.30727305110084563, gamma=0.028042737471943156\n",
      "Gradient Descent(36/499): loss=0.44678837499117247, w0=-0.3074803135414614, gamma=0.336417842987405\n",
      "Gradient Descent(37/499): loss=0.44581840564365705, w0=-0.3098970338561559, gamma=1.1033910868621675\n",
      "Gradient Descent(38/499): loss=0.43497380515465045, w0=-0.31515686184873304, gamma=0.010859530781073273\n",
      "Gradient Descent(39/499): loss=0.4310827688205847, w0=-0.31515150960069194, gamma=0.008207472668785062\n",
      "Gradient Descent(40/499): loss=0.41281560451581667, w0=-0.31514750837925126, gamma=0.008268225132731852\n",
      "Gradient Descent(41/499): loss=0.41034689450395234, w0=-0.3151435106234033, gamma=0.04640766418202242\n",
      "Gradient Descent(42/499): loss=0.4100326277244621, w0=-0.3151212576570214, gamma=0.05181647280218142\n",
      "Gradient Descent(43/499): loss=0.40885368587823495, w0=-0.31509756417986007, gamma=0.03995125987611732\n",
      "Gradient Descent(44/499): loss=0.4080976575286227, w0=-0.3150802427460155, gamma=0.029088770591325305\n",
      "Gradient Descent(45/499): loss=0.40757364481974734, w0=-0.31506813475726964, gamma=0.02999619252786443\n",
      "Gradient Descent(46/499): loss=0.40720357519370654, w0=-0.31505601225431795, gamma=0.025941493967262932\n",
      "Gradient Descent(47/499): loss=0.4068304214552787, w0=-0.3150458428716822, gamma=0.010612261304225728\n",
      "Gradient Descent(48/499): loss=0.40651250271560146, w0=-0.31504179065571675, gamma=0.008685966290238559\n",
      "Gradient Descent(49/499): loss=0.40638196751750694, w0=-0.315038509179116, gamma=0.01377212716707965\n",
      "Gradient Descent(50/499): loss=0.4062764091475194, w0=-0.315033351391551, gamma=0.888202900697371\n",
      "Gradient Descent(51/499): loss=0.4061098609796528, w0=-0.3147052924448315, gamma=1.3632402034257634\n",
      "Gradient Descent(52/499): loss=0.3963187115767043, w0=-0.3146490009709303, gamma=0.008581929803961478\n",
      "Gradient Descent(53/499): loss=0.44869554116637794, w0=-0.3146491296918425, gamma=0.008194081326893287\n",
      "Gradient Descent(54/499): loss=0.38729141721470944, w0=-0.3146492515406391, gamma=0.008953103245396792\n",
      "Gradient Descent(55/499): loss=0.3868349714983464, w0=-0.3146493835854269, gamma=0.07909502905183198\n",
      "Gradient Descent(56/499): loss=0.38654601241739106, w0=-0.3146505396738869, gamma=0.07998267311102886\n",
      "Gradient Descent(57/499): loss=0.3851904354325621, w0=-0.31465161626950544, gamma=0.036957240851437874\n",
      "Gradient Descent(58/499): loss=0.3847322160141975, w0=-0.3146520739392889, gamma=0.02468239756961568\n",
      "Gradient Descent(59/499): loss=0.38452079773499176, w0=-0.3146523683039109, gamma=0.02473590780964563\n",
      "Gradient Descent(60/499): loss=0.3843820231500835, w0=-0.314652656025325, gamma=0.01406171375250193\n",
      "Gradient Descent(61/499): loss=0.38424571692166243, w0=-0.314652815541534, gamma=0.009605647930449098\n",
      "Gradient Descent(62/499): loss=0.3841684670190458, w0=-0.31465292297583514, gamma=0.01092400022523954\n",
      "Gradient Descent(63/499): loss=0.3841158104873257, w0=-0.31465304398162586, gamma=0.09762533786470463\n",
      "Gradient Descent(64/499): loss=0.3840560962488718, w0=-0.3146541135699837, gamma=0.7001009790010141\n",
      "Gradient Descent(65/499): loss=0.38352576623548024, w0=-0.31466103509342785, gamma=0.014788165704630072\n",
      "Gradient Descent(66/499): loss=0.3800438492557356, w0=-0.3146610789394665, gamma=0.008192611837147585\n",
      "Gradient Descent(67/499): loss=0.37992251097268587, w0=-0.3146611028708637, gamma=0.008193001806176837\n",
      "Gradient Descent(68/499): loss=0.3797885146174483, w0=-0.31466112660733, gamma=4.021505695364712\n",
      "Gradient Descent(69/499): loss=0.3797501423368001, w0=-0.31467268211076643, gamma=4.510726453923089\n",
      "Gradient Descent(70/499): loss=0.36408720468170996, w0=-0.314633519639048, gamma=0.06936372588620292\n",
      "Gradient Descent(71/499): loss=0.41435222023728374, w0=-0.3146356338728277, gamma=0.04704652258609809\n",
      "Gradient Descent(72/499): loss=0.3634635665510066, w0=-0.3146369684020834, gamma=0.024424121466443308\n",
      "Gradient Descent(73/499): loss=0.3616376740393806, w0=-0.31463762862595196, gamma=0.024154493420657473\n",
      "Gradient Descent(74/499): loss=0.3544625517141835, w0=-0.3146382656139612, gamma=0.01018660832380959\n",
      "Gradient Descent(75/499): loss=0.35431393721818866, w0=-0.3146385277604216, gamma=0.008492169515991759\n",
      "Gradient Descent(76/499): loss=0.35424015789027863, w0=-0.3146387440752893, gamma=0.012488165647382199\n",
      "Gradient Descent(77/499): loss=0.35419669571593904, w0=-0.31463905947588894, gamma=0.15470418460796836\n",
      "Gradient Descent(78/499): loss=0.35413899943059257, w0=-0.31464291788464055, gamma=0.16578297383611532\n",
      "Gradient Descent(79/499): loss=0.35365755138536337, w0=-0.3146464129461046, gamma=0.011675916796271927\n",
      "Gradient Descent(80/499): loss=0.35345294190633014, w0=-0.3146466182914829, gamma=0.008257345672869731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(81/499): loss=0.3533874111467468, w0=-0.3146467618185417, gamma=0.008346616626907789\n",
      "Gradient Descent(82/499): loss=0.353363733930163, w0=-0.31464690569931986, gamma=0.034587555211965944\n",
      "Gradient Descent(83/499): loss=0.35335000416802015, w0=-0.3146474969505744, gamma=0.03640103540521781\n",
      "Gradient Descent(84/499): loss=0.35329598454933486, w0=-0.3146480976799089, gamma=0.1206056163755961\n",
      "Gradient Descent(85/499): loss=0.35324087681027816, w0=-0.3146500155931605, gamma=0.19643281832061094\n",
      "Gradient Descent(86/499): loss=0.3530599599857901, w0=-0.3146527625963435, gamma=0.011847869621793614\n",
      "Gradient Descent(87/499): loss=0.3527715729002009, w0=-0.3146528957360431, gamma=0.008204206426240541\n",
      "Gradient Descent(88/499): loss=0.3527521089260743, w0=-0.3146529868379981, gamma=0.008235349692238724\n",
      "Gradient Descent(89/499): loss=0.35273957903722775, w0=-0.3146530775355207, gamma=0.14055885343257515\n",
      "Gradient Descent(90/499): loss=0.35272753487755326, w0=-0.3146546127894232, gamma=0.22229137059265888\n",
      "Gradient Descent(91/499): loss=0.35252277353083644, w0=-0.31465669949294867, gamma=0.048044285676302395\n",
      "Gradient Descent(92/499): loss=0.35220467367359887, w0=-0.3146570502422419, gamma=0.03105102028532764\n",
      "Gradient Descent(93/499): loss=0.3521352256968224, w0=-0.31465726604037536, gamma=0.031529521363023505\n",
      "Gradient Descent(94/499): loss=0.3520907013824938, w0=-0.3146574783599799, gamma=0.02153276121368184\n",
      "Gradient Descent(95/499): loss=0.35204619988028235, w0=-0.3146576187896353, gamma=0.009817547982222135\n",
      "Gradient Descent(96/499): loss=0.3520158834820633, w0=-0.3146576814378107, gamma=0.00880213490196191\n",
      "Gradient Descent(97/499): loss=0.35200205104380206, w0=-0.3146577370549492, gamma=0.023639281843083795\n",
      "Gradient Descent(98/499): loss=0.35198967736323855, w0=-0.31465788510728265, gamma=2.2494912179510145\n",
      "Gradient Descent(99/499): loss=0.3519564795005059, w0=-0.31467164058184954, gamma=0.7415942791465525\n",
      "Gradient Descent(100/499): loss=0.3489339967434426, w0=-0.31466597439545657, gamma=0.0082025070831012\n",
      "Gradient Descent(101/499): loss=0.35901799367086773, w0=-0.31466595820074544, gamma=0.008191903450407892\n",
      "Gradient Descent(102/499): loss=0.3480547146024181, w0=-0.31466594215963445, gamma=0.0197645942427816\n",
      "Gradient Descent(103/499): loss=0.3480448271865134, w0=-0.31466590377431397, gamma=0.07680936053086428\n",
      "Gradient Descent(104/499): loss=0.34802141672282133, w0=-0.31466575754925596, gamma=0.08067664314200666\n",
      "Gradient Descent(105/499): loss=0.3479333895226305, w0=-0.3146656157588438, gamma=0.3529584353473042\n",
      "Gradient Descent(106/499): loss=0.3478426515700232, w0=-0.3146650454752134, gamma=1.1495097216784262\n",
      "Gradient Descent(107/499): loss=0.34744948763606254, w0=-0.31466384373063827, gamma=0.044888018237398714\n",
      "Gradient Descent(108/499): loss=0.34626636109435316, w0=-0.3146638507467959, gamma=0.0295536320410401\n",
      "Gradient Descent(109/499): loss=0.34617750031946787, w0=-0.3146638551587814, gamma=0.010165711633670008\n",
      "Gradient Descent(110/499): loss=0.3461485771009668, w0=-0.3146638566315433, gamma=0.00819549611925872\n",
      "Gradient Descent(111/499): loss=0.3461265042349591, w0=-0.31466385780679934, gamma=0.008229021061636246\n",
      "Gradient Descent(112/499): loss=0.34611743231450254, w0=-0.31466385897719173, gamma=0.062145898972416506\n",
      "Gradient Descent(113/499): loss=0.3461090595564011, w0=-0.3146638677433071, gamma=0.08167970840446961\n",
      "Gradient Descent(114/499): loss=0.3460460088651023, w0=-0.3146638785487897, gamma=0.0729387064269134\n",
      "Gradient Descent(115/499): loss=0.3459635397053462, w0=-0.3146638874097792, gamma=0.036890210531653395\n",
      "Gradient Descent(116/499): loss=0.34589027414968604, w0=-0.31466389156451813, gamma=0.028086001137820865\n",
      "Gradient Descent(117/499): loss=0.3458532708582381, w0=-0.31466389461099803, gamma=0.03604211828834518\n",
      "Gradient Descent(118/499): loss=0.3458251848282957, w0=-0.3146638984106739, gamma=0.13832003710405805\n",
      "Gradient Descent(119/499): loss=0.34578922703965426, w0=-0.31466391246724523, gamma=0.02324232088493333\n",
      "Gradient Descent(120/499): loss=0.34565185878382715, w0=-0.3146639145025047, gamma=0.008261977553317907\n",
      "Gradient Descent(121/499): loss=0.345629214566421, w0=-0.3146639152091657, gamma=0.008209299009374834\n",
      "Gradient Descent(122/499): loss=0.34562061289331597, w0=-0.31466391590551984, gamma=0.24302998491290861\n",
      "Gradient Descent(123/499): loss=0.34561251684586664, w0=-0.31466393635131334, gamma=21.022969491890535\n",
      "Gradient Descent(124/499): loss=0.34537376696219935, w0=-0.3146652751547927, gamma=3.0580121112087935\n",
      "Gradient Descent(125/499): loss=0.3311374605751348, w0=-0.3146613758198116, gamma=0.018720167238910214\n",
      "Gradient Descent(126/499): loss=1.0715506508134993, w0=-0.3146614249455329, gamma=0.011066143070722713\n",
      "Gradient Descent(127/499): loss=0.5467971868919155, w0=-0.31466145344183377, gamma=0.008299085935777306\n",
      "Gradient Descent(128/499): loss=0.36229990151888236, w0=-0.3146614745762192, gamma=0.008446125783711272\n",
      "Gradient Descent(129/499): loss=0.334199013361518, w0=-0.3146614959065526, gamma=0.023847740830229544\n",
      "Gradient Descent(130/499): loss=0.33183260257310226, w0=-0.3146615556243331, gamma=0.024254830969187565\n",
      "Gradient Descent(131/499): loss=0.33007990936149467, w0=-0.3146616149130745, gamma=0.027179153870990735\n",
      "Gradient Descent(132/499): loss=0.33006935557060796, w0=-0.31466167973864095, gamma=0.009798466786675479\n",
      "Gradient Descent(133/499): loss=0.3300615199790022, w0=-0.3146617024739772, gamma=0.008214248310341829\n",
      "Gradient Descent(134/499): loss=0.3300584218680145, w0=-0.31466172134670506, gamma=0.008621761323378727\n",
      "Gradient Descent(135/499): loss=0.3300560452964214, w0=-0.31466174099300237, gamma=0.09757550911651833\n",
      "Gradient Descent(136/499): loss=0.3300535647066921, w0=-0.31466196142007, gamma=0.12555320504880635\n",
      "Gradient Descent(137/499): loss=0.3300255714555178, w0=-0.3146622173745729, gamma=0.06608281855740152\n",
      "Gradient Descent(138/499): loss=0.3299897714032023, w0=-0.314662335177727, gamma=0.03654861766350745\n",
      "Gradient Descent(139/499): loss=0.3299710120075308, w0=-0.31466239602591944, gamma=0.02583180419588911\n",
      "Gradient Descent(140/499): loss=0.32996063868201575, w0=-0.31466243746034, gamma=0.008777433777968014\n",
      "Gradient Descent(141/499): loss=0.32995334739011456, w0=-0.3146624511757261, gamma=0.008307004966860989\n",
      "Gradient Descent(142/499): loss=0.3299508532877096, w0=-0.3146624640420988, gamma=0.030591723695485618\n",
      "Gradient Descent(143/499): loss=0.3299485079625218, w0=-0.31466251103073617, gamma=8.811176084197482\n",
      "Gradient Descent(144/499): loss=0.3299398766282332, w0=-0.3146756308999646, gamma=7.998557447110556\n",
      "Gradient Descent(145/499): loss=0.3277007549409718, w0=-0.314582600749157, gamma=0.008226020094538248\n",
      "Gradient Descent(146/499): loss=0.45229667757552633, w0=-0.3145832703413054, gamma=0.008192152701331301\n",
      "Gradient Descent(147/499): loss=0.3266069748613243, w0=-0.31458393169127374, gamma=0.014989556057508825\n",
      "Gradient Descent(148/499): loss=0.3265708922528225, w0=-0.3145851318801823, gamma=0.03474993789170407\n",
      "Gradient Descent(149/499): loss=0.3265300347314171, w0=-0.31458787254362175, gamma=0.03796994740773928\n",
      "Gradient Descent(150/499): loss=0.3264775145002626, w0=-0.31459076310041967, gamma=0.3043316102053264\n",
      "Gradient Descent(151/499): loss=0.32643591371383307, w0=-0.31461305141427837, gamma=0.4275950882680765\n",
      "Gradient Descent(152/499): loss=0.32621450192664164, w0=-0.314634836793733, gamma=0.04211698656350323\n",
      "Gradient Descent(153/499): loss=0.3261731089694353, w0=-0.31463606506152225, gamma=0.01904927254445282\n",
      "Gradient Descent(154/499): loss=0.32614836846860434, w0=-0.31463659720242226, gamma=0.009056408290979005\n",
      "Gradient Descent(155/499): loss=0.326135021895508, w0=-0.3146368453736514, gamma=0.008221896935426718\n",
      "Gradient Descent(156/499): loss=0.32610786907638756, w0=-0.3146370686364683, gamma=0.009439086249942639\n",
      "Gradient Descent(157/499): loss=0.3261057528481908, w0=-0.31463732284425033, gamma=0.03636071977995675\n",
      "Gradient Descent(158/499): loss=0.32610386079940296, w0=-0.31463829284606337, gamma=0.04562088678071883\n",
      "Gradient Descent(159/499): loss=0.3260973782682087, w0=-0.3146394656307636, gamma=0.052092636770776544\n",
      "Gradient Descent(160/499): loss=0.3260900986938782, w0=-0.3146407436925083, gamma=0.037116965645765804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(161/499): loss=0.32608215699903426, w0=-0.3146416068973283, gamma=0.018231850440039384\n",
      "Gradient Descent(162/499): loss=0.3260765689035276, w0=-0.31464201516564294, gamma=0.008771766024620096\n",
      "Gradient Descent(163/499): loss=0.3260738359291976, w0=-0.3146422080117623, gamma=0.008341149036237124\n",
      "Gradient Descent(164/499): loss=0.32607251134468745, w0=-0.3146423897822657, gamma=0.03143549463901728\n",
      "Gradient Descent(165/499): loss=0.32607126350783533, w0=-0.3146430691112113, gamma=0.2623743471491722\n",
      "Gradient Descent(166/499): loss=0.326066565867263, w0=-0.31464856084835485, gamma=0.4851363600584652\n",
      "Gradient Descent(167/499): loss=0.3260275729039354, w0=-0.31465605095857246, gamma=0.01568358546670745\n",
      "Gradient Descent(168/499): loss=0.32595715130980846, w0=-0.31465617562857273, gamma=0.008387272505065078\n",
      "Gradient Descent(169/499): loss=0.32595463216461334, w0=-0.31465624125399133, gamma=0.008229712889199261\n",
      "Gradient Descent(170/499): loss=0.3259529734610703, w0=-0.3146563051065213, gamma=0.02305542826033642\n",
      "Gradient Descent(171/499): loss=0.32595176601294923, w0=-0.3146564825163645, gamma=0.040630633896550354\n",
      "Gradient Descent(172/499): loss=0.32594840047180806, w0=-0.3146567879578621, gamma=0.11912358151228623\n",
      "Gradient Descent(173/499): loss=0.32594249732190034, w0=-0.31465764708617455, gamma=0.1736647975938878\n",
      "Gradient Descent(174/499): loss=0.3259252621667351, w0=-0.31465875036953284, gamma=0.019892122222320743\n",
      "Gradient Descent(175/499): loss=0.3259002556232788, w0=-0.31465885479649547, gamma=0.008280338590029867\n",
      "Gradient Descent(176/499): loss=0.3258974173783475, w0=-0.3146588974008022, gamma=0.008206104001258584\n",
      "Gradient Descent(177/499): loss=0.3258961884983009, w0=-0.31465893927353905, gamma=0.10028670007709216\n",
      "Gradient Descent(178/499): loss=0.3258950107503565, w0=-0.3146594468004825, gamma=0.8426436997078353\n",
      "Gradient Descent(179/499): loss=0.3258806299047201, w0=-0.31466328355382434, gamma=0.1398960868960023\n",
      "Gradient Descent(180/499): loss=0.325760820374718, w0=-0.31466338378656417, gamma=0.02419123804355728\n",
      "Gradient Descent(181/499): loss=0.3257440794926982, w0=-0.3146633986943468, gamma=0.023517290619769406\n",
      "Gradient Descent(182/499): loss=0.32573776093227985, w0=-0.31466341283622057, gamma=0.008218983675319699\n",
      "Gradient Descent(183/499): loss=0.3257345273419997, w0=-0.3146634176623876, gamma=0.008199442248230123\n",
      "Gradient Descent(184/499): loss=0.3257333178178756, w0=-0.31466342243750806, gamma=0.6410974372953828\n",
      "Gradient Descent(185/499): loss=0.3257321752772585, w0=-0.31466379273298606, gamma=9.457124262170806\n",
      "Gradient Descent(186/499): loss=0.32564321743706653, w0=-0.3146657532021311, gamma=0.2333531203386708\n",
      "Gradient Descent(187/499): loss=0.32445317394326784, w0=-0.31466534409480523, gamma=0.057216050771295726\n",
      "Gradient Descent(188/499): loss=0.32459720550534354, w0=-0.3146652671929362, gamma=0.014834976583275162\n",
      "Gradient Descent(189/499): loss=0.32455064877628675, w0=-0.3146652483946586, gamma=0.008303670408017001\n",
      "Gradient Descent(190/499): loss=0.32448254619249395, w0=-0.3146652380286805, gamma=0.008226050905175888\n",
      "Gradient Descent(191/499): loss=0.3243938350501048, w0=-0.3146652278448707, gamma=0.025358381360653805\n",
      "Gradient Descent(192/499): loss=0.32439117956763597, w0=-0.31466519670956655, gamma=0.030875320744648972\n",
      "Gradient Descent(193/499): loss=0.3243861765563697, w0=-0.314665159761815, gamma=0.05826024438315508\n",
      "Gradient Descent(194/499): loss=0.32438231387067046, w0=-0.31466509219577105, gamma=0.07389111132811114\n",
      "Gradient Descent(195/499): loss=0.32437581456173264, w0=-0.3146650114947011, gamma=0.013515385504951617\n",
      "Gradient Descent(196/499): loss=0.324367923377463, w0=-0.3146649978244156, gamma=0.008230642889098699\n",
      "Gradient Descent(197/499): loss=0.32436646824395027, w0=-0.31466498961195644, gamma=0.00824106826685209\n",
      "Gradient Descent(198/499): loss=0.324365581099373, w0=-0.31466498145677435, gamma=0.13311139728037324\n",
      "Gradient Descent(199/499): loss=0.32436470243952265, w0=-0.3146648508181761, gamma=0.26134485335037977\n",
      "Gradient Descent(200/499): loss=0.3243505249483338, w0=-0.3146646284700287, gamma=0.0393309532878893\n",
      "Gradient Descent(201/499): loss=0.3243228480998178, w0=-0.31466460375302824, gamma=0.02459012128058378\n",
      "Gradient Descent(202/499): loss=0.32431865741962695, w0=-0.3146645889074964, gamma=0.024101492196599062\n",
      "Gradient Descent(203/499): loss=0.3243160412693022, w0=-0.31466457471475884, gamma=0.011580269569245866\n",
      "Gradient Descent(204/499): loss=0.32431349901651046, w0=-0.3146645680597971, gamma=0.009067530219482261\n",
      "Gradient Descent(205/499): loss=0.32431227766007753, w0=-0.31466456290920314, gamma=0.013295806716918707\n",
      "Gradient Descent(206/499): loss=0.3243113217273449, w0=-0.31466455542531907, gamma=0.44006978504474076\n",
      "Gradient Descent(207/499): loss=0.32430992043851986, w0=-0.31466431101424197, gamma=1.0837173707149799\n",
      "Gradient Descent(208/499): loss=0.3242636636774413, w0=-0.3146639739992076, gamma=0.008613461973635557\n",
      "Gradient Descent(209/499): loss=0.32416461081695425, w0=-0.31466397422345455, gamma=0.008192279168012118\n",
      "Gradient Descent(210/499): loss=0.32415041814907924, w0=-0.3146639744348991, gamma=0.008290490424176771\n",
      "Gradient Descent(211/499): loss=0.3241495423876543, w0=-0.3146639746471255, gamma=0.05012409648784243\n",
      "Gradient Descent(212/499): loss=0.324148693638922, w0=-0.3146639759196034, gamma=0.0570229998084074\n",
      "Gradient Descent(213/499): loss=0.3241435772676655, w0=-0.31466397729466006, gamma=0.08011542441168049\n",
      "Gradient Descent(214/499): loss=0.32413777381773723, w0=-0.3146639791164059, gamma=0.11557270930271109\n",
      "Gradient Descent(215/499): loss=0.32412963398274347, w0=-0.3146639815338712, gamma=0.07353483427447789\n",
      "Gradient Descent(216/499): loss=0.32411791631229897, w0=-0.3146639828942512, gamma=0.03896700764942281\n",
      "Gradient Descent(217/499): loss=0.32411047737831483, w0=-0.31466398356212316, gamma=0.033181195807481176\n",
      "Gradient Descent(218/499): loss=0.3241065356652227, w0=-0.31466398410866886, gamma=0.010772892340200978\n",
      "Gradient Descent(219/499): loss=0.32410318647684166, w0=-0.3146639842802272, gamma=0.008418808026315259\n",
      "Gradient Descent(220/499): loss=0.3241020982529315, w0=-0.3146639844128524, gamma=0.010472594172598396\n",
      "Gradient Descent(221/499): loss=0.3241012487909634, w0=-0.31466398457644296, gamma=2.654759530150911\n",
      "Gradient Descent(222/499): loss=0.3241001924593654, w0=-0.3146640256116688, gamma=19.312510350576716\n",
      "Gradient Descent(223/499): loss=0.32383620771420113, w0=-0.3146635316360226, gamma=0.009063109656440317\n",
      "Gradient Descent(224/499): loss=0.3270419992427282, w0=-0.3146635358811601, gamma=0.008248032559340633\n",
      "Gradient Descent(225/499): loss=0.32241393024169496, w0=-0.31466353970950406, gamma=0.010045045330431678\n",
      "Gradient Descent(226/499): loss=0.3222637618557712, w0=-0.3146635443334798, gamma=0.02585427422074724\n",
      "Gradient Descent(227/499): loss=0.3222088105380391, w0=-0.3146635561152741, gamma=0.02623244203979369\n",
      "Gradient Descent(228/499): loss=0.3221732804173388, w0=-0.31466356776033416, gamma=0.02757648973021672\n",
      "Gradient Descent(229/499): loss=0.3221711414207494, w0=-0.3146635796809117, gamma=0.009578494293520187\n",
      "Gradient Descent(230/499): loss=0.3221693896500071, w0=-0.3146635837072569, gamma=0.008209957307515902\n",
      "Gradient Descent(231/499): loss=0.3221687653669223, w0=-0.31466358712527787, gamma=0.008726601863739953\n",
      "Gradient Descent(232/499): loss=0.3221682426376713, w0=-0.31466359072856387, gamma=0.18021005036436583\n",
      "Gradient Descent(233/499): loss=0.32216768759998154, w0=-0.3146636644894337, gamma=0.23265335225629555\n",
      "Gradient Descent(234/499): loss=0.32215623947189853, w0=-0.314663742554894, gamma=0.05763901220266125\n",
      "Gradient Descent(235/499): loss=0.3221415167583874, w0=-0.3146637573957085, gamma=0.02882476133319373\n",
      "Gradient Descent(236/499): loss=0.32213787529689863, w0=-0.3146637643896859, gamma=0.022586353098805505\n",
      "Gradient Descent(237/499): loss=0.32213604468426904, w0=-0.31466376971202104, gamma=0.008572259610378788\n",
      "Gradient Descent(238/499): loss=0.3221346211587844, w0=-0.31466377168639686, gamma=0.008256400888117993\n",
      "Gradient Descent(239/499): loss=0.32213407844966235, w0=-0.31466377357172237, gamma=0.03732708826658178\n",
      "Gradient Descent(240/499): loss=0.3221335578545404, w0=-0.3146637820248824, gamma=2.525724033275854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(241/499): loss=0.32213120484472896, w0=-0.3146643326545723, gamma=6.1802368731821105\n",
      "Gradient Descent(242/499): loss=0.321973602230977, w0=-0.3146622769776794, gamma=0.011476868523381588\n",
      "Gradient Descent(243/499): loss=0.3218916765257956, w0=-0.31466229675296453, gamma=0.00831511334433918\n",
      "Gradient Descent(244/499): loss=0.3216906216020314, w0=-0.3146623109159351, gamma=0.008463312686552635\n",
      "Gradient Descent(245/499): loss=0.321645913288953, w0=-0.31466232521146514, gamma=0.033442045901258446\n",
      "Gradient Descent(246/499): loss=0.32163659166549285, w0=-0.3146623812209401, gamma=0.04085514345873869\n",
      "Gradient Descent(247/499): loss=0.3216154101502627, w0=-0.314662447357759, gamma=0.05709963061938562\n",
      "Gradient Descent(248/499): loss=0.3216049744396812, w0=-0.3146625360149696, gamma=0.0436293977465749\n",
      "Gradient Descent(249/499): loss=0.32159895293200946, w0=-0.31466259988921574, gamma=0.01120381828042341\n",
      "Gradient Descent(250/499): loss=0.32159686418107014, w0=-0.3146626155761777, gamma=0.008301548603406254\n",
      "Gradient Descent(251/499): loss=0.3215954513475505, w0=-0.31466262706931736, gamma=0.008458632527044848\n",
      "Gradient Descent(252/499): loss=0.3215948493190697, w0=-0.3146626386827167, gamma=0.029608481636259634\n",
      "Gradient Descent(253/499): loss=0.3215943583047826, w0=-0.31466267899024597, gamma=0.03509589327484046\n",
      "Gradient Descent(254/499): loss=0.32159267907833866, w0=-0.31466272535343853, gamma=0.06852861922710884\n",
      "Gradient Descent(255/499): loss=0.32159072061504657, w0=-0.3146628127055022, gamma=0.07473183203030181\n",
      "Gradient Descent(256/499): loss=0.321586925336802, w0=-0.31466290143669773, gamma=0.0114086800669231\n",
      "Gradient Descent(257/499): loss=0.3215828070000498, w0=-0.314662913970237, gamma=0.00833074197168716\n",
      "Gradient Descent(258/499): loss=0.32158217276414053, w0=-0.3146629230179497, gamma=0.008531671116671994\n",
      "Gradient Descent(259/499): loss=0.32158171253075774, w0=-0.314662932206692, gamma=0.04409291112352488\n",
      "Gradient Descent(260/499): loss=0.32158124218051637, w0=-0.3146629792902839, gamma=0.07581470280917867\n",
      "Gradient Descent(261/499): loss=0.3215788122729506, w0=-0.3146630566776264, gamma=0.09909288483362168\n",
      "Gradient Descent(262/499): loss=0.32157463732707775, w0=-0.3146631501574799, gamma=0.03806624773921724\n",
      "Gradient Descent(263/499): loss=0.3215691878094619, w0=-0.3146631825090705, gamma=0.016655714053579376\n",
      "Gradient Descent(264/499): loss=0.32156709598617544, w0=-0.31466319612552335, gamma=0.008665327727243335\n",
      "Gradient Descent(265/499): loss=0.3215661808183454, w0=-0.3146632030916493, gamma=0.008334752765355441\n",
      "Gradient Descent(266/499): loss=0.32156570404651114, w0=-0.31466320973396245, gamma=0.03767010086020589\n",
      "Gradient Descent(267/499): loss=0.3215652464130767, w0=-0.31466323950462516, gamma=0.3223923073365856\n",
      "Gradient Descent(268/499): loss=0.3215631785694384, w0=-0.31466348469327865, gamma=0.868310987479052\n",
      "Gradient Descent(269/499): loss=0.32154550768247725, w0=-0.31466393216883964, gamma=0.01871000029723267\n",
      "Gradient Descent(270/499): loss=0.32149890504092193, w0=-0.31466393343858706, gamma=0.009643590462044837\n",
      "Gradient Descent(271/499): loss=0.3214975434832104, w0=-0.3146639340808009, gamma=0.008241157415337494\n",
      "Gradient Descent(272/499): loss=0.32149669223621113, w0=-0.3146639346243273, gamma=0.008746341314212282\n",
      "Gradient Descent(273/499): loss=0.3214962319176233, w0=-0.31466393519641805, gamma=0.02473401888412996\n",
      "Gradient Descent(274/499): loss=0.3214957574150156, w0=-0.3146639368000991, gamma=0.02568082926737187\n",
      "Gradient Descent(275/499): loss=0.3214944208727982, w0=-0.31466393842398466, gamma=2.1670806920948604\n",
      "Gradient Descent(276/499): loss=0.3214930353133952, w0=-0.31466407193672785, gamma=2.9747017053116944\n",
      "Gradient Descent(277/499): loss=0.3213769781122732, w0=-0.31466385804597763, gamma=0.008242797221850832\n",
      "Gradient Descent(278/499): loss=0.3215089145085767, w0=-0.3146638592163515, gamma=0.008192855704416148\n",
      "Gradient Descent(279/499): loss=0.3212223003691917, w0=-0.3146638603700457, gamma=0.014716768392619792\n",
      "Gradient Descent(280/499): loss=0.3212217656403919, w0=-0.3146638624254397, gamma=0.02418396192208856\n",
      "Gradient Descent(281/499): loss=0.321220950613692, w0=-0.31466386575334665, gamma=0.02435759184840566\n",
      "Gradient Descent(282/499): loss=0.32121971747169087, w0=-0.31466386902408666, gamma=0.6473418842491133\n",
      "Gradient Descent(283/499): loss=0.32121848904736766, w0=-0.314663953831936, gamma=1.1041203974646383\n",
      "Gradient Descent(284/499): loss=0.3211859218709775, w0=-0.314664004843969, gamma=0.03577376653615742\n",
      "Gradient Descent(285/499): loss=0.3211315607803384, w0=-0.3146640046718784, gamma=0.008384294203117976\n",
      "Gradient Descent(286/499): loss=0.3211311537584184, w0=-0.3146640046329884, gamma=0.008198118976601993\n",
      "Gradient Descent(287/499): loss=0.3211288619614943, w0=-0.31466400459528077, gamma=0.016583545951762632\n",
      "Gradient Descent(288/499): loss=0.32112842993416546, w0=-0.31466400451962934, gamma=0.09142774494957832\n",
      "Gradient Descent(289/499): loss=0.32112756968458206, w0=-0.31466400410946743, gamma=0.0918869181324391\n",
      "Gradient Descent(290/499): loss=0.3211229781685284, w0=-0.314664003734934, gamma=0.8343897854212289\n",
      "Gradient Descent(291/499): loss=0.32111845447945897, w0=-0.3146640006464465, gamma=0.020608404736954817\n",
      "Gradient Descent(292/499): loss=0.32107758704854694, w0=-0.3146640006338134, gamma=0.008212279735928994\n",
      "Gradient Descent(293/499): loss=0.32107666938989926, w0=-0.31466400062888294, gamma=0.008193050210896956\n",
      "Gradient Descent(294/499): loss=0.32107611302477923, w0=-0.31466400062400446, gamma=0.04875451278672909\n",
      "Gradient Descent(295/499): loss=0.32107571413480246, w0=-0.31466400059521166, gamma=0.0964939360821342\n",
      "Gradient Descent(296/499): loss=0.3210733413028853, w0=-0.3146640005410038, gamma=0.18060626173971706\n",
      "Gradient Descent(297/499): loss=0.32106864879737756, w0=-0.3146640004493341, gamma=0.05484151111780948\n",
      "Gradient Descent(298/499): loss=0.3210598817512579, w0=-0.3146640004265257, gamma=0.028149643137480935\n",
      "Gradient Descent(299/499): loss=0.3210572232199848, w0=-0.31466400041546033, gamma=0.026552632593036366\n",
      "Gradient Descent(300/499): loss=0.3210558560836604, w0=-0.31466400040531656, gamma=0.02457215864677074\n",
      "Gradient Descent(301/499): loss=0.32105457008662924, w0=-0.31466400039617864, gamma=0.011458464420361162\n",
      "Gradient Descent(302/499): loss=0.3210533804891698, w0=-0.3146640003920222, gamma=0.009005269738721722\n",
      "Gradient Descent(303/499): loss=0.32105282578238253, w0=-0.314664000388793, gamma=0.013305598951584199\n",
      "Gradient Descent(304/499): loss=0.32105238994092133, w0=-0.3146640003840647, gamma=0.5237227549894973\n",
      "Gradient Descent(305/499): loss=0.32105174607015524, w0=-0.3146640002004324, gamma=1.3189196377194052\n",
      "Gradient Descent(306/499): loss=0.32102644549185244, w0=-0.3146639999801732, gamma=0.008524055540213994\n",
      "Gradient Descent(307/499): loss=0.3209693250433773, w0=-0.31466399998062716, gamma=0.008191897913136532\n",
      "Gradient Descent(308/499): loss=0.32096288176330057, w0=-0.3146639999810597, gamma=0.008203124635233738\n",
      "Gradient Descent(309/499): loss=0.3209624840533752, w0=-0.31466399998148925, gamma=0.11856936393155511\n",
      "Gradient Descent(310/499): loss=0.32096209581477986, w0=-0.3146639999876474, gamma=0.1966170628405512\n",
      "Gradient Descent(311/499): loss=0.3209564869674873, w0=-0.3146639999966481, gamma=0.049514119984801826\n",
      "Gradient Descent(312/499): loss=0.3209472066090286, w0=-0.3146639999984691, gamma=0.03053617972190611\n",
      "Gradient Descent(313/499): loss=0.3209448690278478, w0=-0.3146639999995365, gamma=0.030508374215332718\n",
      "Gradient Descent(314/499): loss=0.3209434277700687, w0=-0.3146640000005704, gamma=0.11774460766078522\n",
      "Gradient Descent(315/499): loss=0.3209419908123226, w0=-0.3146640000044389, gamma=1.3550418248583522\n",
      "Gradient Descent(316/499): loss=0.3209364484664711, w0=-0.3146640000437154, gamma=0.1132948266831866\n",
      "Gradient Descent(317/499): loss=0.3208730474978759, w0=-0.31466400004254863, gamma=0.02354530731534744\n",
      "Gradient Descent(318/499): loss=0.32086864622720546, w0=-0.31466400004233364, gamma=0.01884441822027168\n",
      "Gradient Descent(319/499): loss=0.3208667128269465, w0=-0.3146640000421656, gamma=0.008198784040659397\n",
      "Gradient Descent(320/499): loss=0.320865883443268, w0=-0.3146640000420939, gamma=0.008192745817830803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(321/499): loss=0.32086540192568475, w0=-0.3146640000420228, gamma=0.1588321615636673\n",
      "Gradient Descent(322/499): loss=0.3208650234299401, w0=-0.3146640000406559, gamma=0.386728126799007\n",
      "Gradient Descent(323/499): loss=0.3208576897015605, w0=-0.3146640000378557, gamma=0.035813118759562355\n",
      "Gradient Descent(324/499): loss=0.3208399128483253, w0=-0.3146640000376967, gamma=0.024201191547299435\n",
      "Gradient Descent(325/499): loss=0.32083824141032885, w0=-0.31466400003759304, gamma=0.024369379287719435\n",
      "Gradient Descent(326/499): loss=0.32083712255994923, w0=-0.31466400003749123, gamma=2.5475036217240206\n",
      "Gradient Descent(327/499): loss=0.3208360046756067, w0=-0.3146640000271024, gamma=0.7117531522637395\n",
      "Gradient Descent(328/499): loss=0.32072002610817046, w0=-0.3146640000315864, gamma=0.008199167763892715\n",
      "Gradient Descent(329/499): loss=0.3207601375863624, w0=-0.31466400003160133, gamma=0.008191879212828122\n",
      "Gradient Descent(330/499): loss=0.3206880100359043, w0=-0.31466400003161604, gamma=0.013122001277956284\n",
      "Gradient Descent(331/499): loss=0.3206876474679002, w0=-0.3146640000316394, gamma=0.08162551756181187\n",
      "Gradient Descent(332/499): loss=0.32068706705672095, w0=-0.31466400003178296, gamma=0.09682912794012973\n",
      "Gradient Descent(333/499): loss=0.3206834603707969, w0=-0.3146640000319393, gamma=0.1903298478535415\n",
      "Gradient Descent(334/499): loss=0.3206791869854415, w0=-0.3146640000322169, gamma=0.05224293996013386\n",
      "Gradient Descent(335/499): loss=0.32067080120787506, w0=-0.3146640000322785, gamma=0.025224750951000793\n",
      "Gradient Descent(336/499): loss=0.32066850321415985, w0=-0.3146640000323067, gamma=0.024747824137297505\n",
      "Gradient Descent(337/499): loss=0.32066739013303164, w0=-0.31466400003233364, gamma=0.19469503300313226\n",
      "Gradient Descent(338/499): loss=0.32066630207046937, w0=-0.31466400003254047, gamma=2.3875432774509955\n",
      "Gradient Descent(339/499): loss=0.32065774855226936, w0=-0.3146640000345817, gamma=0.014486426032602515\n",
      "Gradient Descent(340/499): loss=0.32055467734088317, w0=-0.31466400003456435, gamma=0.008212700880748028\n",
      "Gradient Descent(341/499): loss=0.32055364809537457, w0=-0.31466400003455464, gamma=0.008202346792124055\n",
      "Gradient Descent(342/499): loss=0.32055274762283187, w0=-0.31466400003454503, gamma=0.0320640282406389\n",
      "Gradient Descent(343/499): loss=0.320552393648445, w0=-0.31466400003450784, gamma=0.033360154550932775\n",
      "Gradient Descent(344/499): loss=0.32055101948134257, w0=-0.3146640000344703, gamma=0.13408124005962865\n",
      "Gradient Descent(345/499): loss=0.3205495949686857, w0=-0.3146640000343246, gamma=0.615792615748335\n",
      "Gradient Descent(346/499): loss=0.3205438736757884, w0=-0.3146640000337444, gamma=0.08804187804358841\n",
      "Gradient Descent(347/499): loss=0.3205176848061966, w0=-0.3146640000337123, gamma=0.01569042754484647\n",
      "Gradient Descent(348/499): loss=0.32051408594816994, w0=-0.3146640000337071, gamma=0.00909538227777765\n",
      "Gradient Descent(349/499): loss=0.3205133362956814, w0=-0.3146640000337041, gamma=0.008332904920490289\n",
      "Gradient Descent(350/499): loss=0.32051289795373555, w0=-0.3146640000337014, gamma=0.011807625830937486\n",
      "Gradient Descent(351/499): loss=0.3205125425176058, w0=-0.3146640000336976, gamma=0.02518034650830705\n",
      "Gradient Descent(352/499): loss=0.320512041807984, w0=-0.3146640000336896, gamma=0.027879781219577068\n",
      "Gradient Descent(353/499): loss=0.32051097658918415, w0=-0.314664000033681, gamma=0.2766585469750962\n",
      "Gradient Descent(354/499): loss=0.32050979805710295, w0=-0.3146640000335974, gamma=0.08180224428178068\n",
      "Gradient Descent(355/499): loss=0.32049811552206176, w0=-0.3146640000335795, gamma=0.008275500531410605\n",
      "Gradient Descent(356/499): loss=0.32049474029841335, w0=-0.31466400003357786, gamma=0.008192845346103815\n",
      "Gradient Descent(357/499): loss=0.32049431989283633, w0=-0.3146640000335762, gamma=0.01662625224226261\n",
      "Gradient Descent(358/499): loss=0.32049397498602405, w0=-0.3146640000335729, gamma=0.614848011802813\n",
      "Gradient Descent(359/499): loss=0.32049327514143106, w0=-0.3146640000334531, gamma=4.163341948156593\n",
      "Gradient Descent(360/499): loss=0.3204674403709938, w0=-0.3146640000331292, gamma=0.03692645865265281\n",
      "Gradient Descent(361/499): loss=0.320300143367009, w0=-0.31466400003313766, gamma=0.024609525890862936\n",
      "Gradient Descent(362/499): loss=0.32029504060917946, w0=-0.31466400003314304, gamma=0.023725089146689513\n",
      "Gradient Descent(363/499): loss=0.320292847828184, w0=-0.3146640000331481, gamma=0.008938050150788322\n",
      "Gradient Descent(364/499): loss=0.32029188986733254, w0=-0.3146640000331499, gamma=0.008272234285712655\n",
      "Gradient Descent(365/499): loss=0.32029149969535997, w0=-0.31466400003315165, gamma=0.014647493667960516\n",
      "Gradient Descent(366/499): loss=0.3202911652844841, w0=-0.31466400003315464, gamma=0.06706281526973955\n",
      "Gradient Descent(367/499): loss=0.3202905753066053, w0=-0.3146640000331681, gamma=0.06881638797486349\n",
      "Gradient Descent(368/499): loss=0.3202878883085671, w0=-0.314664000033181, gamma=0.0826181132546657\n",
      "Gradient Descent(369/499): loss=0.32028513959200366, w0=-0.3146640000331955, gamma=0.009621467657610974\n",
      "Gradient Descent(370/499): loss=0.32028184517391, w0=-0.31466400003319706, gamma=0.008209825493751996\n",
      "Gradient Descent(371/499): loss=0.3202814582382438, w0=-0.31466400003319833, gamma=0.00876394212044673\n",
      "Gradient Descent(372/499): loss=0.3202811306926537, w0=-0.3146640000331997, gamma=1.5703033541745333\n",
      "Gradient Descent(373/499): loss=0.3202807811753115, w0=-0.3146640000334428, gamma=3.0852252023685227\n",
      "Gradient Descent(374/499): loss=0.32021840941553165, w0=-0.31466400003314654, gamma=0.03129005195751005\n",
      "Gradient Descent(375/499): loss=0.32012159220503966, w0=-0.3146640000331524, gamma=0.029455439464502506\n",
      "Gradient Descent(376/499): loss=0.32009722873273333, w0=-0.31466400003315775, gamma=0.008767321870782482\n",
      "Gradient Descent(377/499): loss=0.32009655795166414, w0=-0.3146640000331593, gamma=0.008195689917200741\n",
      "Gradient Descent(378/499): loss=0.3200955887838057, w0=-0.31466400003316075, gamma=0.00851066046202103\n",
      "Gradient Descent(379/499): loss=0.32009527297315205, w0=-0.31466400003316225, gamma=0.02626383711733174\n",
      "Gradient Descent(380/499): loss=0.3200949488292664, w0=-0.31466400003316675, gamma=0.029036123895350322\n",
      "Gradient Descent(381/499): loss=0.3200939495915982, w0=-0.3146640000331716, gamma=0.46803603333916527\n",
      "Gradient Descent(382/499): loss=0.32009284549605393, w0=-0.3146640000332478, gamma=3.8513235153224095\n",
      "Gradient Descent(383/499): loss=0.3200750727091859, w0=-0.314664000033571, gamma=0.06283813994102294\n",
      "Gradient Descent(384/499): loss=0.3199320508002236, w0=-0.3146640000335549, gamma=0.018314052398734827\n",
      "Gradient Descent(385/499): loss=0.31993130198228914, w0=-0.3146640000335505, gamma=0.009301600217546372\n",
      "Gradient Descent(386/499): loss=0.31992935337662415, w0=-0.3146640000335483, gamma=0.008237429133756595\n",
      "Gradient Descent(387/499): loss=0.31992744024324576, w0=-0.31466400003354633, gamma=0.009082182004126084\n",
      "Gradient Descent(388/499): loss=0.3199270878983269, w0=-0.3146640000335442, gamma=0.024219316087980274\n",
      "Gradient Descent(389/499): loss=0.3199267456911158, w0=-0.3146640000335386, gamma=0.02435700258110234\n",
      "Gradient Descent(390/499): loss=0.31992585612696167, w0=-0.3146640000335331, gamma=0.3557047909655746\n",
      "Gradient Descent(391/499): loss=0.3199249689726505, w0=-0.31466400003345546, gamma=0.2648302846803396\n",
      "Gradient Descent(392/499): loss=0.31991202705662625, w0=-0.3146640000334178, gamma=0.008510508268727908\n",
      "Gradient Descent(393/499): loss=0.3199026942503682, w0=-0.31466400003341694, gamma=0.008193973969236934\n",
      "Gradient Descent(394/499): loss=0.31990211512101085, w0=-0.31466400003341605, gamma=0.009169740178659267\n",
      "Gradient Descent(395/499): loss=0.3199018173923301, w0=-0.3146640000334151, gamma=0.06214653266032363\n",
      "Gradient Descent(396/499): loss=0.31990148489754855, w0=-0.31466400003340866, gamma=0.06866006361856572\n",
      "Gradient Descent(397/499): loss=0.31989923426399847, w0=-0.314664000033402, gamma=0.08892815373305986\n",
      "Gradient Descent(398/499): loss=0.3198967507466767, w0=-0.31466400003339384, gamma=0.07656512645412038\n",
      "Gradient Descent(399/499): loss=0.3198935364161659, w0=-0.31466400003338746, gamma=0.045041047217296105\n",
      "Gradient Descent(400/499): loss=0.31989077121690757, w0=-0.31466400003338396, gamma=0.023870192050698066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(401/499): loss=0.3198891452593498, w0=-0.3146640000333822, gamma=0.008775696977817514\n",
      "Gradient Descent(402/499): loss=0.31988828424340976, w0=-0.3146640000333816, gamma=0.008295880837973288\n",
      "Gradient Descent(403/499): loss=0.3198879672037258, w0=-0.31466400003338096, gamma=0.028046712689996246\n",
      "Gradient Descent(404/499): loss=0.31988766794796647, w0=-0.31466400003337897, gamma=2.3596083855318546\n",
      "Gradient Descent(405/499): loss=0.31988665635733315, w0=-0.3146640000332154, gamma=15.975654155314698\n",
      "Gradient Descent(406/499): loss=0.3198020291801106, w0=-0.31466400003454764, gamma=0.008628962592968867\n",
      "Gradient Descent(407/499): loss=0.31996450881886773, w0=-0.31466400003453654, gamma=0.008204305833523244\n",
      "Gradient Descent(408/499): loss=0.3192722103574722, w0=-0.3146640000345258, gamma=0.010340313685943897\n",
      "Gradient Descent(409/499): loss=0.31926480380186456, w0=-0.31466400003451245, gamma=0.032535599967414945\n",
      "Gradient Descent(410/499): loss=0.31926095204152444, w0=-0.3146640000344708, gamma=0.03258039269021845\n",
      "Gradient Descent(411/499): loss=0.3192568685954141, w0=-0.31466400003443046, gamma=0.03063909991542174\n",
      "Gradient Descent(412/499): loss=0.3192558706738594, w0=-0.31466400003439376, gamma=0.009864416057526729\n",
      "Gradient Descent(413/499): loss=0.3192549362190268, w0=-0.31466400003438233, gamma=0.00837628133263504\n",
      "Gradient Descent(414/499): loss=0.3192546352082078, w0=-0.31466400003437267, gamma=0.01260253894841697\n",
      "Gradient Descent(415/499): loss=0.3192543797668399, w0=-0.3146640000343583, gamma=6.248625027769667\n",
      "Gradient Descent(416/499): loss=0.31925399548258265, w0=-0.31466400002732875, gamma=19.578379910066477\n",
      "Gradient Descent(417/499): loss=0.31906583835530083, w0=-0.31466400014241536, gamma=0.0087261386381851\n",
      "Gradient Descent(418/499): loss=0.3225225937619144, w0=-0.3146640001414613, gamma=0.008241433006922165\n",
      "Gradient Descent(419/499): loss=0.3186817608782711, w0=-0.31466400014056856, gamma=0.011958121023187015\n",
      "Gradient Descent(420/499): loss=0.3185924915602692, w0=-0.31466400013928386, gamma=0.025124654728994934\n",
      "Gradient Descent(421/499): loss=0.31854659324699264, w0=-0.31466400013661683, gamma=0.02652234467540198\n",
      "Gradient Descent(422/499): loss=0.3185234704132623, w0=-0.3146640001338722, gamma=0.059082062009445144\n",
      "Gradient Descent(423/499): loss=0.31851912814581895, w0=-0.3146640001279203, gamma=0.042401708789554106\n",
      "Gradient Descent(424/499): loss=0.3185157709076362, w0=-0.3146640001239011, gamma=0.008225444198701358\n",
      "Gradient Descent(425/499): loss=0.3185163182405694, w0=-0.3146640001231545, gamma=0.008192435623446835\n",
      "Gradient Descent(426/499): loss=0.3185143680513461, w0=-0.31466400012241696, gamma=0.027457073155466732\n",
      "Gradient Descent(427/499): loss=0.31851416105180236, w0=-0.3146640001199655, gamma=0.0946013625219584\n",
      "Gradient Descent(428/499): loss=0.3185134693373926, w0=-0.31466400011175094, gamma=0.1354218263739337\n",
      "Gradient Descent(429/499): loss=0.3185111005633958, w0=-0.314664000101104, gamma=0.11286485540937671\n",
      "Gradient Descent(430/499): loss=0.31850772728698895, w0=-0.3146640000934322, gamma=0.029141183824049163\n",
      "Gradient Descent(431/499): loss=0.31850492166631017, w0=-0.31466400009167494, gamma=0.02398857936558666\n",
      "Gradient Descent(432/499): loss=0.31850419511095257, w0=-0.31466400009027057, gamma=0.011381785612860396\n",
      "Gradient Descent(433/499): loss=0.31850359882568957, w0=-0.3146640000896202, gamma=0.008485457828551637\n",
      "Gradient Descent(434/499): loss=0.31850331589620756, w0=-0.31466400008914086, gamma=0.010107924091256486\n",
      "Gradient Descent(435/499): loss=0.31850310501856577, w0=-0.3146640000885747, gamma=1.0639110061020365\n",
      "Gradient Descent(436/499): loss=0.3185028538499838, w0=-0.3146640000295868, gamma=4.328826874879671\n",
      "Gradient Descent(437/499): loss=0.3184764666168856, w0=-0.3146640000449104, gamma=0.013567481801254019\n",
      "Gradient Descent(438/499): loss=0.3183767713775871, w0=-0.3146640000447503, gamma=0.008271664585406163\n",
      "Gradient Descent(439/499): loss=0.31837324836259745, w0=-0.31466400004465406, gamma=0.008294881843937469\n",
      "Gradient Descent(440/499): loss=0.3183709848575342, w0=-0.3146640000445583, gamma=0.05472450821821451\n",
      "Gradient Descent(441/499): loss=0.31837052741104144, w0=-0.3146640000439318, gamma=0.0592336556438522\n",
      "Gradient Descent(442/499): loss=0.31836845474931097, w0=-0.31466400004329076, gamma=0.03864724940540481\n",
      "Gradient Descent(443/499): loss=0.318367013437655, w0=-0.3146640000428973, gamma=0.025896577406585308\n",
      "Gradient Descent(444/499): loss=0.3183660821309581, w0=-0.31466400004264383, gamma=0.008942208171074868\n",
      "Gradient Descent(445/499): loss=0.31836546326678095, w0=-0.31466400004255857, gamma=0.008210365180019616\n",
      "Gradient Descent(446/499): loss=0.3183652471744676, w0=-0.314664000042481, gamma=0.010177030021944696\n",
      "Gradient Descent(447/499): loss=0.31836505066859627, w0=-0.31466400004238565, gamma=0.29218302204031477\n",
      "Gradient Descent(448/499): loss=0.3183648071370822, w0=-0.31466400003967576, gamma=0.39655788841588985\n",
      "Gradient Descent(449/499): loss=0.31835782080783276, w0=-0.3146640000370716, gamma=0.04719575430566922\n",
      "Gradient Descent(450/499): loss=0.31834837114125203, w0=-0.31466400003688455, gamma=0.027490844991581605\n",
      "Gradient Descent(451/499): loss=0.3183472399982805, w0=-0.31466400003678074, gamma=0.013685038370998229\n",
      "Gradient Descent(452/499): loss=0.31834658240108493, w0=-0.31466400003673045, gamma=0.008238002535990867\n",
      "Gradient Descent(453/499): loss=0.3183462555829146, w0=-0.3146640000367006, gamma=0.008240762556020543\n",
      "Gradient Descent(454/499): loss=0.31834605869150834, w0=-0.314664000036671, gamma=0.09150396401789125\n",
      "Gradient Descent(455/499): loss=0.3183458625538512, w0=-0.3146640000363449, gamma=0.1583818408462377\n",
      "Gradient Descent(456/499): loss=0.3183436852670711, w0=-0.3146640000358321, gamma=0.051219869908257665\n",
      "Gradient Descent(457/499): loss=0.3183399201631376, w0=-0.31466400003569256, gamma=0.026120418198032878\n",
      "Gradient Descent(458/499): loss=0.3183387033848892, w0=-0.314664000035625, gamma=0.024691247774477117\n",
      "Gradient Descent(459/499): loss=0.3183380823351644, w0=-0.31466400003556283, gamma=0.013952147723860357\n",
      "Gradient Descent(460/499): loss=0.3183374960470878, w0=-0.3146640000355286, gamma=0.009466351862489853\n",
      "Gradient Descent(461/499): loss=0.3183371647923805, w0=-0.31466400003550565, gamma=0.010763980642298221\n",
      "Gradient Descent(462/499): loss=0.31833694005695584, w0=-0.31466400003547984, gamma=0.11410590715978536\n",
      "Gradient Descent(463/499): loss=0.3183366845407216, w0=-0.314664000035209, gamma=1.0676142034247365\n",
      "Gradient Descent(464/499): loss=0.3183339765410474, w0=-0.3146640000329634, gamma=0.014630855704802853\n",
      "Gradient Descent(465/499): loss=0.31830875365510347, w0=-0.31466400003296546, gamma=0.008192773235024996\n",
      "Gradient Descent(466/499): loss=0.3183083877746774, w0=-0.3146640000329666, gamma=0.008192749641131432\n",
      "Gradient Descent(467/499): loss=0.31830816050012517, w0=-0.3146640000329677, gamma=0.13694579878729177\n",
      "Gradient Descent(468/499): loss=0.3183079675569355, w0=-0.3146640000329861, gamma=0.21682418264017764\n",
      "Gradient Descent(469/499): loss=0.31830474343179105, w0=-0.3146640000330111, gamma=0.051633299389316135\n",
      "Gradient Descent(470/499): loss=0.3182996460573709, w0=-0.3146640000330157, gamma=0.033332712603049326\n",
      "Gradient Descent(471/499): loss=0.318298431516541, w0=-0.31466400003301853, gamma=0.033120766245205026\n",
      "Gradient Descent(472/499): loss=0.31829764794230625, w0=-0.31466400003302125, gamma=0.08333413058080152\n",
      "Gradient Descent(473/499): loss=0.31829687024879777, w0=-0.3146640000330277, gamma=0.43691457775694303\n",
      "Gradient Descent(474/499): loss=0.31829491412619115, w0=-0.31466400003305917, gamma=0.06525823088861442\n",
      "Gradient Descent(475/499): loss=0.3182846723601599, w0=-0.3146640000330617, gamma=0.010892358748276723\n",
      "Gradient Descent(476/499): loss=0.3182831701786298, w0=-0.3146640000330621, gamma=0.008381193431509651\n",
      "Gradient Descent(477/499): loss=0.3182828924786313, w0=-0.3146640000330624, gamma=0.00871237974807059\n",
      "Gradient Descent(478/499): loss=0.3182826931261604, w0=-0.3146640000330627, gamma=0.024198754992131814\n",
      "Gradient Descent(479/499): loss=0.3182824889116826, w0=-0.31466400003306355, gamma=0.025744364461022233\n",
      "Gradient Descent(480/499): loss=0.3182819227607057, w0=-0.31466400003306444, gamma=1.118835779099739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(481/499): loss=0.31828132090007694, w0=-0.31466400003310124, gamma=0.25212604476044653\n",
      "Gradient Descent(482/499): loss=0.3182552166887468, w0=-0.31466400003309936, gamma=0.00820501535623961\n",
      "Gradient Descent(483/499): loss=0.31825075772560696, w0=-0.3146640000330993, gamma=0.008191889614761631\n",
      "Gradient Descent(484/499): loss=0.31824917652237167, w0=-0.31466400003309924, gamma=0.014060450771319125\n",
      "Gradient Descent(485/499): loss=0.3182489867373013, w0=-0.31466400003309913, gamma=1.2006054920763405\n",
      "Gradient Descent(486/499): loss=0.3182486610198253, w0=-0.3146640000330912, gamma=4.899398322844672\n",
      "Gradient Descent(487/499): loss=0.31822090445255324, w0=-0.3146640000330741, gamma=0.030325882739230727\n",
      "Gradient Descent(488/499): loss=0.31811789816881086, w0=-0.314664000033074, gamma=0.02682973510357355\n",
      "Gradient Descent(489/499): loss=0.31810873070879403, w0=-0.31466400003307393, gamma=0.02415292404617938\n",
      "Gradient Descent(490/499): loss=0.31810790620984664, w0=-0.3146640000330739, gamma=0.024051260125836756\n",
      "Gradient Descent(491/499): loss=0.31810736566134185, w0=-0.3146640000330738, gamma=0.010835054757589018\n",
      "Gradient Descent(492/499): loss=0.31810683030590853, w0=-0.3146640000330738, gamma=0.008890172551726961\n",
      "Gradient Descent(493/499): loss=0.31810658912703393, w0=-0.3146640000330738, gamma=0.0148835019933969\n",
      "Gradient Descent(494/499): loss=0.3181063912699481, w0=-0.3146640000330738, gamma=0.8817147801497206\n",
      "Gradient Descent(495/499): loss=0.31810606005354125, w0=-0.3146640000330722, gamma=1.3197496218170897\n",
      "Gradient Descent(496/499): loss=0.3180864671815141, w0=-0.31466400003306777, gamma=0.008345822679038566\n",
      "Gradient Descent(497/499): loss=0.3180614098358914, w0=-0.31466400003306777, gamma=0.008192407323649765\n",
      "Gradient Descent(498/499): loss=0.31805715368412923, w0=-0.3146640000330677, gamma=0.009295728944212819\n",
      "Gradient Descent(499/499): loss=0.31805696921728704, w0=-0.31466400003306766, gamma=0.06283406952682678\n"
     ]
    }
   ],
   "source": [
    "w = np.array(np.random.rand((tX.shape[1])))\n",
    "max_iters = 500 #choosing number of iterations\n",
    "gamma = 5 *10**(-2)\n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)\n",
    "(w, loss) =adaptative_step_gradient_descent(y, tX, w, max_iters, gamma)\n",
    "#gamma = 5/max_iters #choosing gamma \n",
    "#(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1a5e51a4d3eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
