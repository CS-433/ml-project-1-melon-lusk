{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tX = (tX - np.mean(tX,axis = 0))/(np.std(tX,axis = 0))\n",
    "tX = np.c_[(np.ones(tX.shape[0]) , tX)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.46141372,  0.06833197, ...,  1.5668    ,\n",
       "         1.55858439,  0.4125105 ],\n",
       "       [ 1.        ,  0.51670419,  0.55250482, ..., -0.63936657,\n",
       "        -0.63936694, -0.27381996],\n",
       "       [ 1.        , -2.33785898,  3.19515553, ..., -0.63936657,\n",
       "        -0.63936694, -0.29396985],\n",
       "       ...,\n",
       "       [ 1.        ,  0.38016991,  0.31931645, ..., -0.63936657,\n",
       "        -0.63936694, -0.31701723],\n",
       "       [ 1.        ,  0.35431502, -0.84532397, ..., -0.63936657,\n",
       "        -0.63936694, -0.74543941],\n",
       "       [ 1.        , -2.33785898,  0.66533608, ..., -0.63936657,\n",
       "        -0.63936694, -0.74543941]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=44.88650198284493, w0=0.3735670128735564, w1=0.34413951299485884\n",
      "Gradient Descent(1/499): loss=35.3873544022071, w0=0.36668470274482046, w1=0.32358386308406195\n",
      "Gradient Descent(2/499): loss=28.0503324753744, w0=0.35987121571737185, w1=0.3053336141910768\n",
      "Gradient Descent(3/499): loss=22.37845502745467, w0=0.3531258635601977, w1=0.2891087900215332\n",
      "Gradient Descent(4/499): loss=17.98915659192931, w0=0.3464479649245953, w1=0.2746637331635473\n",
      "Gradient Descent(5/499): loss=14.587939376164995, w0=0.3398368452753489, w1=0.26178289484008976\n",
      "Gradient Descent(6/499): loss=11.948093029595032, w0=0.33329183682259494, w1=0.250277141168803\n",
      "Gradient Descent(7/499): loss=9.895084822026607, w0=0.3268122784543686, w1=0.23998051256693376\n",
      "Gradient Descent(8/499): loss=8.294544654385591, w0=0.32039751566982444, w1=0.23074738071190365\n",
      "Gradient Descent(9/499): loss=7.043017026151788, w0=0.3140469005131258, w1=0.2224499542873827\n",
      "Gradient Descent(10/499): loss=6.060842740730177, w0=0.3077597915079941, w1=0.2149760907275206\n",
      "Gradient Descent(11/499): loss=5.2866798793136764, w0=0.3015355535929137, w1=0.20822737642085865\n",
      "Gradient Descent(12/499): loss=4.673286527216887, w0=0.2953735580569841, w1=0.202117442440417\n",
      "Gradient Descent(13/499): loss=4.184274677281231, w0=0.2892731824764138, w1=0.19657048690652146\n",
      "Gradient Descent(14/499): loss=3.791611653433808, w0=0.2832338106516492, w1=0.191519978633393\n",
      "Gradient Descent(15/499): loss=3.4736969048586737, w0=0.2772548325451322, w1=0.1869075198201774\n",
      "Gradient Descent(16/499): loss=3.2138816666376013, w0=0.2713356442196805, w1=0.18268184827527245\n",
      "Gradient Descent(17/499): loss=2.9993294979125613, w0=0.2654756477774832, w1=0.1787979620563201\n",
      "Gradient Descent(18/499): loss=2.8201391962703557, w0=0.2596742512997079, w1=0.17521635150811696\n",
      "Gradient Descent(19/499): loss=2.6686696655720175, w0=0.2539308687867104, w1=0.17190232552298945\n",
      "Gradient Descent(20/499): loss=2.5390202295499087, w0=0.24824492009884286, w1=0.16882542046446763\n",
      "Gradient Descent(21/499): loss=2.4266305939962476, w0=0.24261583089785402, w1=0.16595888161310765\n",
      "Gradient Descent(22/499): loss=2.3279729042769213, w0=0.23704303258887505, w1=0.16327920823737552\n",
      "Gradient Descent(23/499): loss=2.240314690271525, w0=0.23152596226298586, w1=0.1607657544839539\n",
      "Gradient Descent(24/499): loss=2.1615363748927683, w0=0.2260640626403555, w1=0.15840037923939201\n",
      "Gradient Descent(25/499): loss=2.089990781611826, w0=0.2206567820139515, w1=0.1561671389551098\n",
      "Gradient Descent(26/499): loss=2.0243949699435015, w0=0.21530357419381155, w1=0.15405201816480157\n",
      "Gradient Descent(27/499): loss=1.9637469550046907, w0=0.210003898451873, w1=0.15204269306990473\n",
      "Gradient Descent(28/499): loss=1.90726158151392, w0=0.20475721946735387, w1=0.15012832413609542\n",
      "Gradient Descent(29/499): loss=1.8543211420688221, w0=0.1995630072726799, w1=0.14829937414147787\n",
      "Gradient Descent(30/499): loss=1.8044373451401152, w0=0.19442073719995268, w1=0.14654744855378282\n",
      "Gradient Descent(31/499): loss=1.7572220199344935, w0=0.1893298898279527, w1=0.14486515549697476\n",
      "Gradient Descent(32/499): loss=1.712364546968365, w0=0.18428995092967274, w1=0.14324598290375684\n",
      "Gradient Descent(33/499): loss=1.6696144663185084, w0=0.1793004114203756, w1=0.1416841907453222\n",
      "Gradient Descent(34/499): loss=1.6287680719855637, w0=0.17436076730617142, w1=0.140174716488389\n",
      "Gradient Descent(35/499): loss=1.5896580751838727, w0=0.1694705196331093, w1=0.13871309215651004\n",
      "Gradient Descent(36/499): loss=1.5521456305620691, w0=0.1646291744367778, w1=0.13729537157176042\n",
      "Gradient Descent(37/499): loss=1.516114181914595, w0=0.15983624269240962, w1=0.13591806652759283\n",
      "Gradient Descent(38/499): loss=1.4814647090654294, w0=0.15509124026548515, w1=0.13457809079690697\n",
      "Gradient Descent(39/499): loss=1.4481120539128152, w0=0.15039368786282992, w1=0.13327271101383523\n",
      "Gradient Descent(40/499): loss=1.4159820777532328, w0=0.14574311098420126, w1=0.1319995035857088\n",
      "Gradient Descent(41/499): loss=1.3850094590617998, w0=0.14113903987435886, w1=0.13075631689515788\n",
      "Gradient Descent(42/499): loss=1.3551359848259148, w0=0.13658100947561486, w1=0.12954123814309523\n",
      "Gradient Descent(43/499): loss=1.3263092223351458, w0=0.13206855938085832, w1=0.12835256426298822\n",
      "Gradient Descent(44/499): loss=1.2984814843522303, w0=0.12760123378704935, w1=0.12718877640670836\n",
      "Gradient Descent(45/499): loss=1.2716090206201227, w0=0.12317858144917848, w1=0.1260485175635595\n",
      "Gradient Descent(46/499): loss=1.2456513840783299, w0=0.1188001556346863, w1=0.12493057292787413\n",
      "Gradient Descent(47/499): loss=1.220570932030116, w0=0.11446551407833906, w1=0.12383385267775904\n",
      "Gradient Descent(48/499): loss=1.1963324316382753, w0=0.11017421893755529, w1=0.12275737686897233\n",
      "Gradient Descent(49/499): loss=1.1729027461600614, w0=0.10592583674817936, w1=0.12170026218423653\n",
      "Gradient Descent(50/499): loss=1.1502505837459698, w0=0.10171993838069719, w1=0.12066171031015853\n",
      "Gradient Descent(51/499): loss=1.1283462947950935, w0=0.09755609899688984, w1=0.11964099774188412\n",
      "Gradient Descent(52/499): loss=1.1071617070686754, w0=0.09343389800692056, w1=0.118637466840142\n",
      "Gradient Descent(53/499): loss=1.086669990234142, w0=0.089352919026851, w1=0.11765051798684996\n",
      "Gradient Descent(54/499): loss=1.0668455434142083, w0=0.08531274983658213, w1=0.11667960270433429\n",
      "Gradient Descent(55/499): loss=1.0476639007805795, w0=0.08131298233821593, w1=0.11572421761977518\n",
      "Gradient Descent(56/499): loss=1.0291016513599445, w0=0.07735321251483342, w1=0.11478389917102129\n",
      "Gradient Descent(57/499): loss=1.0111363700889224, w0=0.07343304038968472, w1=0.11385821896266368\n",
      "Gradient Descent(58/499): loss=0.9937465578240524, w0=0.0695520699857875, w1=0.11294677969244321\n",
      "Gradient Descent(59/499): loss=0.9769115885287416, w0=0.06570990928592928, w1=0.11204921157787621\n",
      "Gradient Descent(60/499): loss=0.9606116622566362, w0=0.06190617019306962, w1=0.11116516922159127\n",
      "Gradient Descent(61/499): loss=0.9448277628574001, w0=0.058140468491138554, w1=0.11029432886142115\n",
      "Gradient Descent(62/499): loss=0.9295416195672858, w0=0.05441242380622681, w1=0.10943638595791866\n",
      "Gradient Descent(63/499): loss=0.9147356718293207, w0=0.0507216595681642, w1=0.1085910530777773\n",
      "Gradient Descent(64/499): loss=0.900393036828794, w0=0.04706780297248221, w1=0.10775805803673658\n",
      "Gradient Descent(65/499): loss=0.8864974793385731, w0=0.04345048494275704, w1=0.10693714227002443\n",
      "Gradient Descent(66/499): loss=0.873033383552978, w0=0.03986934009332911, w1=0.10612805940231422\n",
      "Gradient Descent(67/499): loss=0.8599857266541242, w0=0.036324006692395494, w1=0.10533057399261563\n",
      "Gradient Descent(68/499): loss=0.847340053905209, w0=0.03281412662547117, w1=0.1045444604325396\n",
      "Gradient Descent(69/499): loss=0.8350824551044674, w0=0.029339345359216115, w1=0.1037695019790267\n",
      "Gradient Descent(70/499): loss=0.8231995422641118, w0=0.025899311905623598, w1=0.10300548990495255\n",
      "Gradient Descent(71/499): loss=0.8116784284023985, w0=0.02249367878656702, w1=0.1022522227530633\n",
      "Gradient Descent(72/499): loss=0.800506707355668, w0=0.01912210199870101, w1=0.10150950568048249\n",
      "Gradient Descent(73/499): loss=0.7896724345318563, w0=0.015784240978713643, w1=0.10077714988260045\n",
      "Gradient Descent(74/499): loss=0.7791641085385663, w0=0.012479758568926174, w1=0.1000549720865332\n",
      "Gradient Descent(75/499): loss=0.76897065362798, w0=0.009208320983236585, w1=0.099342794105546\n",
      "Gradient Descent(76/499): loss=0.7590814029082216, w0=0.00596959777340389, w1=0.09864044244689554\n",
      "Gradient Descent(77/499): loss=0.7494860822766874, w0=0.0027632617956695137, w1=0.09794774796647444\n",
      "Gradient Descent(78/499): loss=0.7401747950356277, w0=-0.00041101082228750833, w1=0.09726454556445632\n",
      "Gradient Descent(79/499): loss=0.7311380071541911, w0=-0.003553540714064964, w1=0.096590673916855\n",
      "Gradient Descent(80/499): loss=0.7223665331443637, w0=-0.0066646453069246405, w1=0.09592597523853863\n",
      "Gradient Descent(81/499): loss=0.7138515225209314, w0=-0.009744638853855726, w1=0.09527029507378941\n",
      "Gradient Descent(82/499): loss=0.705584446817877, w0=-0.012793832465317503, w1=0.09462348211098226\n",
      "Gradient Descent(83/499): loss=0.6975570871355747, w0=-0.015812534140664653, w1=0.09398538801837897\n",
      "Gradient Descent(84/499): loss=0.6897615221948178, w0=-0.018801048799258334, w1=0.09335586729840556\n",
      "Gradient Descent(85/499): loss=0.6821901168752026, w0=-0.021759678311266073, w1=0.09273477715810627\n",
      "Gradient Descent(86/499): loss=0.6748355112166835, w0=-0.024688721528153726, w1=0.09212197739375283\n",
      "Gradient Descent(87/499): loss=0.6676906098642934, w0=-0.027588474312872507, w1=0.0915173302878385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(88/499): loss=0.6607485719370627, w0=-0.03045922956974411, w1=0.09092070051690547\n",
      "Gradient Descent(89/499): loss=0.6540028013031466, w0=-0.033301277274047, w1=0.09033195506884745\n",
      "Gradient Descent(90/499): loss=0.647446937244036, w0=-0.03611490450130685, w1=0.08975096316849729\n",
      "Gradient Descent(91/499): loss=0.6410748454915532, w0=-0.03890039545629412, w1=0.08917759621045838\n",
      "Gradient Descent(92/499): loss=0.6348806096220898, w0=-0.041658031501731495, w1=0.08861172769826783\n",
      "Gradient Descent(93/499): loss=0.6288585227932465, w0=-0.04438809118671449, w1=0.08805323318909353\n",
      "Gradient Descent(94/499): loss=0.6230030798087118, w0=-0.047090850274847655, w1=0.08750199024326685\n",
      "Gradient Descent(95/499): loss=0.617308969497832, w0=-0.04976658177209949, w1=0.0869578783780403\n",
      "Gradient Descent(96/499): loss=0.6117710673969244, w0=-0.052415555954378794, w1=0.08642077902503602\n",
      "Gradient Descent(97/499): loss=0.6063844287199444, w0=-0.055038040394835314, w1=0.08589057549091833\n",
      "Gradient Descent(98/499): loss=0.6011442816066545, w0=-0.05763429999088729, w1=0.08536715292088225\n",
      "Gradient Descent(99/499): loss=0.5960460206369381, w0=-0.060204596990978736, w1=0.08485039826460171\n",
      "Gradient Descent(100/499): loss=0.5910852006004017, w0=-0.06274919102106927, w1=0.08434020024432624\n",
      "Gradient Descent(101/499): loss=0.5862575305108525, w0=-0.06526833911085889, w1=0.08383644932485455\n",
      "Gradient Descent(102/499): loss=0.5815588678556838, w0=-0.0677622957197506, w1=0.08333903768514807\n",
      "Gradient Descent(103/499): loss=0.5769852130706234, w0=-0.07023131276255341, w1=0.08284785919137808\n",
      "Gradient Descent(104/499): loss=0.5725327042306939, w0=-0.0726756396349282, w1=0.08236280937122638\n",
      "Gradient Descent(105/499): loss=0.5681976119486186, w0=-0.07509552323857922, w1=0.08188378538928287\n",
      "Gradient Descent(106/499): loss=0.5639763344722735, w0=-0.07749120800619375, w1=0.08141068602340376\n",
      "Gradient Descent(107/499): loss=0.5598653929731363, w0=-0.07986293592613211, w1=0.08094341164191207\n",
      "Gradient Descent(108/499): loss=0.555861427018017, w0=-0.0822109465668711, w1=0.08048186418153737\n",
      "Gradient Descent(109/499): loss=0.5519611902166717, w0=-0.0845354771012027, w1=0.08002594712600569\n",
      "Gradient Descent(110/499): loss=0.5481615460382158, w0=-0.08683676233019097, w1=0.0795755654852022\n",
      "Gradient Descent(111/499): loss=0.5444594637895398, w0=-0.08911503470688936, w1=0.07913062577483973\n",
      "Gradient Descent(112/499): loss=0.5408520147492158, w0=-0.09137052435982077, w1=0.07869103599657541\n",
      "Gradient Descent(113/499): loss=0.5373363684506449, w0=-0.09360345911622286, w1=0.07825670561852541\n",
      "Gradient Descent(114/499): loss=0.533909789108467, w0=-0.09581406452506093, w1=0.07782754555613486\n",
      "Gradient Descent(115/499): loss=0.5305696321824873, w0=-0.09800256387981063, w1=0.07740346815336616\n",
      "Gradient Descent(116/499): loss=0.527313341073618, w0=-0.10016917824101282, w1=0.0769843871641737\n",
      "Gradient Descent(117/499): loss=0.5241384439465584, w0=-0.102314126458603, w1=0.07657021773423828\n",
      "Gradient Descent(118/499): loss=0.5210425506741542, w0=-0.10443762519401727, w1=0.0761608763829377\n",
      "Gradient Descent(119/499): loss=0.5180233498985827, w0=-0.10653988894207739, w1=0.07575628098553426\n",
      "Gradient Descent(120/499): loss=0.5150786062047118, w0=-0.1086211300526569, w1=0.07535635075556227\n",
      "Gradient Descent(121/499): loss=0.512206157401171, w0=-0.11068155875213063, w1=0.07496100622740155\n",
      "Gradient Descent(122/499): loss=0.5094039119048545, w0=-0.11272138316460961, w1=0.07457016923902528\n",
      "Gradient Descent(123/499): loss=0.5066698462247516, w0=-0.11474080933296381, w1=0.0741837629149122\n",
      "Gradient Descent(124/499): loss=0.5040020025411694, w0=-0.11674004123963448, w1=0.07380171164911509\n",
      "Gradient Descent(125/499): loss=0.501398486376571, w0=-0.11871928082723844, w1=0.07342394108847877\n",
      "Gradient Descent(126/499): loss=0.4988574643544092, w0=-0.12067872801896636, w1=0.0730503781160023\n",
      "Gradient Descent(127/499): loss=0.4963771620424774, w0=-0.12261858073877699, w1=0.07268095083434076\n",
      "Gradient Descent(128/499): loss=0.4939558618774531, w0=-0.1245390349313895, w1=0.07231558854944332\n",
      "Gradient Descent(129/499): loss=0.4915919011674269, w0=-0.1264402845820759, w1=0.07195422175432496\n",
      "Gradient Descent(130/499): loss=0.4892836701693601, w0=-0.12832252173625544, w1=0.07159678211296948\n",
      "Gradient Descent(131/499): loss=0.48702961023852054, w0=-0.13018593651889318, w1=0.07124320244436272\n",
      "Gradient Descent(132/499): loss=0.48482821204707965, w0=-0.13203071715370454, w1=0.07089341670665476\n",
      "Gradient Descent(133/499): loss=0.4826780138691563, w0=-0.1338570499821678, w1=0.07054735998145047\n",
      "Gradient Descent(134/499): loss=0.4805775999297137, w0=-0.1356651194823464, w1=0.07020496845822823\n",
      "Gradient Descent(135/499): loss=0.4785255988148129, w0=-0.13745510828752325, w1=0.06986617941888675\n",
      "Gradient Descent(136/499): loss=0.4765206819408283, w0=-0.13922719720464832, w1=0.06953093122242014\n",
      "Gradient Descent(137/499): loss=0.47456156208033246, w0=-0.14098156523260214, w1=0.06919916328972167\n",
      "Gradient Descent(138/499): loss=0.4726469919424419, w0=-0.1427183895802764, w1=0.06887081608851658\n",
      "Gradient Descent(139/499): loss=0.47077576280551364, w0=-0.14443784568447393, w1=0.0685458311184248\n",
      "Gradient Descent(140/499): loss=0.46894670320015586, w0=-0.1461401072276295, w1=0.0682241508961539\n",
      "Gradient Descent(141/499): loss=0.46715867764061086, w0=-0.14782534615535348, w1=0.06790571894082333\n",
      "Gradient Descent(142/499): loss=0.46541058540263497, w0=-0.14949373269380023, w1=0.06759047975942048\n",
      "Gradient Descent(143/499): loss=0.4637013593460835, w0=-0.1511454353668625, w1=0.06727837883238949\n",
      "Gradient Descent(144/499): loss=0.462029964780474, w0=-0.15278062101319417, w1=0.06696936259935338\n",
      "Gradient Descent(145/499): loss=0.4603953983718774, w0=-0.15439945480306252, w1=0.06666337844497046\n",
      "Gradient Descent(146/499): loss=0.45879668708954363, w0=-0.1560021002550322, w1=0.06636037468492566\n",
      "Gradient Descent(147/499): loss=0.4572328871907424, w0=-0.15758871925248216, w1=0.06606030055205742\n",
      "Gradient Descent(148/499): loss=0.45570308324234954, w0=-0.15915947205995762, w1=0.0657631061826209\n",
      "Gradient Descent(149/499): loss=0.4542063871777784, w0=-0.16071451733935832, w1=0.0654687426026881\n",
      "Gradient Descent(150/499): loss=0.4527419373879005, w0=-0.16225401216596502, w1=0.06517716171468535\n",
      "Gradient Descent(151/499): loss=0.4513088978446656, w0=-0.16377811204430565, w1=0.06488831628406881\n",
      "Gradient Descent(152/499): loss=0.4499064572561697, w0=-0.16528697092386288, w1=0.06460215992613845\n",
      "Gradient Descent(153/499): loss=0.44853382825198146, w0=-0.16678074121462452, w1=0.06431864709299068\n",
      "Gradient Descent(154/499): loss=0.44719024659757545, w0=-0.16825957380247855, w1=0.0640377330606103\n",
      "Gradient Descent(155/499): loss=0.4458749704367694, w0=-0.16972361806445405, w1=0.0637593739161017\n",
      "Gradient Descent(156/499): loss=0.4445872795611079, w0=-0.17117302188380978, w1=0.06348352654505988\n",
      "Gradient Descent(157/499): loss=0.44332647470517406, w0=-0.17260793166497196, w1=0.06321014861908107\n",
      "Gradient Descent(158/499): loss=0.44209187686685275, w0=-0.17402849234832252, w1=0.06293919858341333\n",
      "Gradient Descent(159/499): loss=0.4408828266516032, w0=-0.17543484742483956, w1=0.06267063564474712\n",
      "Gradient Descent(160/499): loss=0.43969868363984554, w0=-0.17682713895059146, w1=0.062404419759145624\n",
      "Gradient Descent(161/499): loss=0.4385388257765868, w0=-0.17820550756108583, w1=0.06214051162011501\n",
      "Gradient Descent(162/499): loss=0.43740264878245955, w0=-0.17957009248547526, w1=0.061878872646814284\n",
      "Gradient Descent(163/499): loss=0.43628956558537096, w0=-0.18092103156062078, w1=0.06161946497240466\n",
      "Gradient Descent(164/499): loss=0.4351990057719931, w0=-0.18225846124501485, w1=0.06136225143253816\n",
      "Gradient Descent(165/499): loss=0.4341304150583562, w0=-0.18358251663256497, w1=0.06110719555398511\n",
      "Gradient Descent(166/499): loss=0.43308325477883625, w0=-0.1848933314662396, w1=0.0608542615434003\n",
      "Gradient Descent(167/499): loss=0.43205700139285086, w0=-0.18619103815157748, w1=0.06060341427622722\n",
      "Gradient Descent(168/499): loss=0.43105114600861255, w0=-0.18747576777006197, w1=0.060354619285740166\n",
      "Gradient Descent(169/499): loss=0.43006519392330433, w0=-0.18874765009236164, w1=0.06010784275222347\n",
      "Gradient Descent(170/499): loss=0.4290986641790757, w0=-0.19000681359143828, w1=0.05986305149228756\n",
      "Gradient Descent(171/499): loss=0.42815108913427435, w0=-0.19125338545552417, w1=0.05962021294832107\n",
      "Gradient Descent(172/499): loss=0.4272220140493526, w0=-0.1924874916009692, w1=0.05937929517807859\n",
      "Gradient Descent(173/499): loss=0.42631099668691336, w0=-0.19370925668495978, w1=0.059140266844403155\n",
      "Gradient Descent(174/499): loss=0.42541760692537645, w0=-0.19491880411811047, w1=0.05890309720508304\n",
      "Gradient Descent(175/499): loss=0.4245414263857666, w0=-0.19611625607692965, w1=0.05866775610284198\n",
      "Gradient Descent(176/499): loss=0.42368204807114557, w0=-0.19730173351616062, w1=0.05843421395546206\n",
      "Gradient Descent(177/499): loss=0.4228390760182298, w0=-0.1984753561809993, w1=0.05820244174603857\n",
      "Gradient Descent(178/499): loss=0.42201212496074797, w0=-0.19963724261918958, w1=0.05797241101336593\n",
      "Gradient Descent(179/499): loss=0.42120082000411485, w0=-0.20078751019299795, w1=0.05774409384245388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(180/499): loss=0.4204047963110115, w0=-0.20192627509106825, w1=0.05751746285517305\n",
      "Gradient Descent(181/499): loss=0.41962369879748, w0=-0.20305365234015785, w1=0.05729249120102897\n",
      "Gradient Descent(182/499): loss=0.41885718183914905, w0=-0.20416975581675656, w1=0.057069152548063704\n",
      "Gradient Descent(183/499): loss=0.4181049089872328, w0=-0.20527469825858927, w1=0.056847421073883984\n",
      "Gradient Descent(184/499): loss=0.4173665526939469, w0=-0.20636859127600365, w1=0.05662727145681511\n",
      "Gradient Descent(185/499): loss=0.41664179404700696, w0=-0.2074515453632439, w1=0.05640867886717942\n",
      "Gradient Descent(186/499): loss=0.41593032251288553, w0=-0.20852366990961174, w1=0.0561916189586984\n",
      "Gradient Descent(187/499): loss=0.41523183568851374, w0=-0.2095850732105159, w1=0.05597606786001747\n",
      "Gradient Descent(188/499): loss=0.41454603906112886, w0=-0.210635862478411, w1=0.05576200216635235\n",
      "Gradient Descent(189/499): loss=0.41387264577597677, w0=-0.21167614385362718, w1=0.05554939893125589\n",
      "Gradient Descent(190/499): loss=0.4132113764115933, w0=-0.21270602241509118, w1=0.05533823565850446\n",
      "Gradient Descent(191/499): loss=0.41256195876239554, w0=-0.21372560219094053, w1=0.05512849029410264\n",
      "Gradient Descent(192/499): loss=0.4119241276283248, w0=-0.2147349861690314, w1=0.05492014121840525\n",
      "Gradient Descent(193/499): loss=0.4112976246112945, w0=-0.21573427630734135, w1=0.054713167238355565\n",
      "Gradient Descent(194/499): loss=0.4106821979182025, w0=-0.21672357354426822, w1=0.05450754757983857\n",
      "Gradient Descent(195/499): loss=0.4100776021702809, w0=-0.2177029778088258, w1=0.05430326188014822\n",
      "Gradient Descent(196/499): loss=0.40948359821855757, w0=-0.2186725880307378, w1=0.05410029018056749\n",
      "Gradient Descent(197/499): loss=0.4088999529652202, w0=-0.2196325021504307, w1=0.05389861291906013\n",
      "Gradient Descent(198/499): loss=0.4083264391906756, w0=-0.22058281712892666, w1=0.05369821092307297\n",
      "Gradient Descent(199/499): loss=0.40776283538610764, w0=-0.22152362895763766, w1=0.053499065402447595\n",
      "Gradient Descent(200/499): loss=0.40720892559134186, w0=-0.22245503266806158, w1=0.05330115794244033\n",
      "Gradient Descent(201/499): loss=0.40666449923783576, w0=-0.22337712234138124, w1=0.05310447049684927\n",
      "Gradient Descent(202/499): loss=0.4061293509966165, w0=-0.2242899911179677, w1=0.05290898538124726\n",
      "Gradient Descent(203/499): loss=0.40560328063099693, w0=-0.22519373120678832, w1=0.05271468526631965\n",
      "Gradient Descent(204/499): loss=0.40508609285390385, w0=-0.2260884338947207, w1=0.052521553171305656\n",
      "Gradient Descent(205/499): loss=0.4045775971896655, w0=-0.22697418955577375, w1=0.05232957245754219\n",
      "Gradient Descent(206/499): loss=0.4040776078401009, w0=-0.22785108766021628, w1=0.052138726822108954\n",
      "Gradient Descent(207/499): loss=0.40358594355476757, w0=-0.2287192167836144, w1=0.05194900029157367\n",
      "Gradient Descent(208/499): loss=0.40310242750522535, w0=-0.22957866461577853, w1=0.05176037721583626\n",
      "Gradient Descent(209/499): loss=0.4026268871631817, w0=-0.23042951796962102, w1=0.05157284226207082\n",
      "Gradient Descent(210/499): loss=0.40215915418238457, w0=-0.2312718627899251, w1=0.05138638040876423\n",
      "Gradient Descent(211/499): loss=0.4016990642841406, w0=-0.23210578416202612, w1=0.05120097693985028\n",
      "Gradient Descent(212/499): loss=0.40124645714633145, w0=-0.23293136632040612, w1=0.05101661743893806\n",
      "Gradient Descent(213/499): loss=0.40080117629581713, w0=-0.23374869265720233, w1=0.05083328778363355\n",
      "Gradient Descent(214/499): loss=0.40036306900410695, w0=-0.23455784573063057, w1=0.05065097413995327\n",
      "Gradient Descent(215/499): loss=0.3999319861861946, w0=-0.23535890727332454, w1=0.05046966295682874\n",
      "Gradient Descent(216/499): loss=0.3995077823024466, w0=-0.23615195820059157, w1=0.05028934096070074\n",
      "Gradient Descent(217/499): loss=0.3990903152634485, w0=-0.23693707861858593, w1=0.05010999515020216\n",
      "Gradient Descent(218/499): loss=0.3986794463377063, w0=-0.23771434783240034, w1=0.04993161279092829\n",
      "Gradient Descent(219/499): loss=0.3982750400621108, w0=-0.2384838443540766, w1=0.049754181410293516\n",
      "Gradient Descent(220/499): loss=0.3978769641550736, w0=-0.2392456459105361, w1=0.04957768879247322\n",
      "Gradient Descent(221/499): loss=0.39748508943224764, w0=-0.239999829451431, w1=0.04940212297342979\n",
      "Gradient Descent(222/499): loss=0.3970992897247467, w0=-0.24074647115691697, w1=0.04922747223602167\n",
      "Gradient Descent(223/499): loss=0.3967194417997821, w0=-0.24148564644534806, w1=0.0490537251051943\n",
      "Gradient Descent(224/499): loss=0.39634542528363903, w0=-0.24221742998089485, w1=0.048880870343251885\n",
      "Gradient Descent(225/499): loss=0.3959771225869148, w0=-0.24294189568108618, w1=0.04870889694520893\n",
      "Gradient Descent(226/499): loss=0.39561441883194787, w0=-0.2436591167242756, w1=0.04853779413422034\n",
      "Gradient Descent(227/499): loss=0.39525720178236495, w0=-0.2443691655570331, w1=0.04836755135708924\n",
      "Gradient Descent(228/499): loss=0.3949053617746781, w0=-0.24507211390146305, w1=0.0481981582798512\n",
      "Gradient Descent(229/499): loss=0.39455879165186675, w0=-0.24576803276244868, w1=0.04802960478343398\n",
      "Gradient Descent(230/499): loss=0.39421738669888196, w0=-0.24645699243482447, w1=0.04786188095939169\n",
      "Gradient Descent(231/499): loss=0.39388104458000833, w0=-0.2471390625104765, w1=0.04769497710571232\n",
      "Gradient Descent(232/499): loss=0.3935496652780267, w0=-0.24781431188537198, w1=0.04752888372269765\n",
      "Gradient Descent(233/499): loss=0.3932231510351205, w0=-0.24848280876651851, w1=0.04736359150891445\n",
      "Gradient Descent(234/499): loss=0.3929014062954696, w0=-0.2491446206788536, w1=0.047199091357215985\n",
      "Gradient Descent(235/499): loss=0.39258433764947803, w0=-0.24979981447206534, w1=0.04703537435083291\n",
      "Gradient Descent(236/499): loss=0.3922718537795858, w0=-0.25044845632734497, w1=0.046872431759532374\n",
      "Gradient Descent(237/499): loss=0.3919638654076119, w0=-0.25109061176407177, w1=0.04671025503584454\n",
      "Gradient Descent(238/499): loss=0.3916602852435853, w0=-0.2517263456464313, w1=0.046548835811355396\n",
      "Gradient Descent(239/499): loss=0.3913610279360112, w0=-0.25235572218996727, w1=0.04638816589306497\n",
      "Gradient Descent(240/499): loss=0.3910660100235332, w0=-0.25297880496806785, w1=0.046228237259809984\n",
      "Gradient Descent(241/499): loss=0.3907751498879464, w0=-0.2535956569183874, w1=0.04606904205874993\n",
      "Gradient Descent(242/499): loss=0.39048836770851747, w0=-0.2542063403492038, w1=0.04591057260191577\n",
      "Gradient Descent(243/499): loss=0.3902055854175737, w0=-0.254810916945712, w1=0.04575282136282011\n",
      "Gradient Descent(244/499): loss=0.389926726657322, w0=-0.2554094477762551, w1=0.04559578097312819\n",
      "Gradient Descent(245/499): loss=0.3896517167378572, w0=-0.2560019932984928, w1=0.04543944421938853\n",
      "Gradient Descent(246/499): loss=0.3893804825963276, w0=-0.25658861336550814, w1=0.0452838040398225\n",
      "Gradient Descent(247/499): loss=0.38911295275721863, w0=-0.25716936723185335, w1=0.04512885352117188\n",
      "Gradient Descent(248/499): loss=0.3888490572937216, w0=-0.2577443135595351, w1=0.044974585895603436\n",
      "Gradient Descent(249/499): loss=0.388588727790157, w0=-0.25831351042394, w1=0.044820994537669794\n",
      "Gradient Descent(250/499): loss=0.38833189730541595, w0=-0.25887701531970086, w1=0.04466807296132561\n",
      "Gradient Descent(251/499): loss=0.3880785003373936, w0=-0.25943488516650415, w1=0.04451581481699828\n",
      "Gradient Descent(252/499): loss=0.3878284727883809, w0=-0.25998717631483936, w1=0.04436421388871227\n",
      "Gradient Descent(253/499): loss=0.387581751931389, w0=-0.2605339445516912, w1=0.04421326409126628\n",
      "Gradient Descent(254/499): loss=0.3873382763773756, w0=-0.2610752451061746, w1=0.044062959467462415\n",
      "Gradient Descent(255/499): loss=0.3870979860433489, w0=-0.2616111326551131, w1=0.0439132941853865\n",
      "Gradient Descent(256/499): loss=0.38686082212132133, w0=-0.26214166132856226, w1=0.04376426253573883\n",
      "Gradient Descent(257/499): loss=0.38662672704808815, w0=-0.2626668847152769, w1=0.043615858929214414\n",
      "Gradient Descent(258/499): loss=0.3863956444758073, w0=-0.2631868558681244, w1=0.04346807789393206\n",
      "Gradient Descent(259/499): loss=0.3861675192433563, w0=-0.2637016273094434, w1=0.04332091407291149\n",
      "Gradient Descent(260/499): loss=0.38594229734844204, w0=-0.26421125103634924, w1=0.043174362221597665\n",
      "Gradient Descent(261/499): loss=0.38571992592044557, w0=-0.264715778525986, w1=0.04302841720543158\n",
      "Gradient Descent(262/499): loss=0.385500353193974, w0=-0.2652152607407264, w1=0.04288307399746686\n",
      "Gradient Descent(263/499): loss=0.38528352848310593, w0=-0.2657097481333194, w1=0.04273832767603129\n",
      "Gradient Descent(264/499): loss=0.385069402156304, w0=-0.26619929065198644, w1=0.042594173422432685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(265/499): loss=0.3848579256119797, w0=-0.26668393774546684, w1=0.04245060651870821\n",
      "Gradient Descent(266/499): loss=0.3846490512546899, w0=-0.26716373836801244, w1=0.042307622345416636\n",
      "Gradient Descent(267/499): loss=0.38444273247194555, w0=-0.2676387409843326, w1=0.042165216379472674\n",
      "Gradient Descent(268/499): loss=0.3842389236116188, w0=-0.26810899357448953, w1=0.04202338419202278\n",
      "Gradient Descent(269/499): loss=0.38403757995992616, w0=-0.2685745436387449, w1=0.04188212144636167\n",
      "Gradient Descent(270/499): loss=0.3838386577199767, w0=-0.2690354382023577, w1=0.041741423895889\n",
      "Gradient Descent(271/499): loss=0.38364211399086484, w0=-0.2694917238203344, w1=0.041601287382105376\n",
      "Gradient Descent(272/499): loss=0.3834479067472952, w0=-0.2699434465821313, w1=0.04146170783264718\n",
      "Gradient Descent(273/499): loss=0.38325599481972245, w0=-0.27039065211631025, w1=0.04132268125935951\n",
      "Gradient Descent(274/499): loss=0.38306633787499295, w0=-0.2708333855951474, w1=0.04118420375640656\n",
      "Gradient Descent(275/499): loss=0.38287889639747386, w0=-0.2712716917391962, w1=0.04104627149841884\n",
      "Gradient Descent(276/499): loss=0.3826936316706558, w0=-0.2717056148218045, w1=0.0409088807386767\n",
      "Gradient Descent(277/499): loss=0.38251050575921514, w0=-0.2721351986735867, w1=0.04077202780732933\n",
      "Gradient Descent(278/499): loss=0.3823294814915248, w0=-0.2725604866868511, w1=0.040635709109648886\n",
      "Gradient Descent(279/499): loss=0.38215052244259967, w0=-0.27298152181998286, w1=0.04049992112431896\n",
      "Gradient Descent(280/499): loss=0.38197359291746513, w0=-0.2733983466017833, w1=0.040364660401756854\n",
      "Gradient Descent(281/499): loss=0.3817986579349369, w0=-0.27381100313576573, w1=0.04022992356246916\n",
      "Gradient Descent(282/499): loss=0.3816256832118019, w0=-0.27421953310440833, w1=0.04009570729543995\n",
      "Gradient Descent(283/499): loss=0.38145463514738587, w0=-0.2746239777733645, w1=0.0399620083565511\n",
      "Gradient Descent(284/499): loss=0.38128548080850244, w0=-0.2750243779956311, w1=0.039828823567034155\n",
      "Gradient Descent(285/499): loss=0.3811181879147674, w0=-0.27542077421567507, w1=0.0396961498119532\n",
      "Gradient Descent(286/499): loss=0.38095272482427367, w0=-0.2758132064735186, w1=0.03956398403871819\n",
      "Gradient Descent(287/499): loss=0.3807890605196127, w0=-0.27620171440878366, w1=0.03943232325562826\n",
      "Gradient Descent(288/499): loss=0.3806271645942362, w0=-0.27658633726469606, w1=0.03930116453044436\n",
      "Gradient Descent(289/499): loss=0.3804670072391458, w0=-0.27696711389204937, w1=0.039170504988990856\n",
      "Gradient Descent(290/499): loss=0.38030855922990653, w0=-0.2773440827531291, w1=0.0390403418137855\n",
      "Gradient Descent(291/499): loss=0.3801517919139699, w0=-0.2777172819255981, w1=0.03891067224269725\n",
      "Gradient Descent(292/499): loss=0.3799966771983031, w0=-0.27808674910634235, w1=0.03878149356763158\n",
      "Gradient Descent(293/499): loss=0.37984318753731217, w0=-0.27845252161527917, w1=0.038652803133242576\n",
      "Gradient Descent(294/499): loss=0.3796912959210551, w0=-0.2788146363991266, w1=0.03852459833567153\n",
      "Gradient Descent(295/499): loss=0.3795409758637335, w0=-0.2791731300351356, w1=0.038396876621311506\n",
      "Gradient Descent(296/499): loss=0.37939220139245955, w0=-0.2795280387347845, w1=0.038269635485597364\n",
      "Gradient Descent(297/499): loss=0.3792449470362863, w0=-0.2798793983474369, w1=0.038142872471820834\n",
      "Gradient Descent(298/499): loss=0.3790991878154988, w0=-0.2802272443639628, w1=0.038016585169970196\n",
      "Gradient Descent(299/499): loss=0.3789548992311557, w0=-0.2805716119203234, w1=0.03789077121559408\n",
      "Gradient Descent(300/499): loss=0.37881205725487804, w0=-0.28091253580112047, w1=0.03776542828868899\n",
      "Gradient Descent(301/499): loss=0.3786706383188752, w0=-0.2812500504431095, w1=0.037640554112610115\n",
      "Gradient Descent(302/499): loss=0.3785306193062061, w0=-0.2815841899386787, w1=0.03751614645300497\n",
      "Gradient Descent(303/499): loss=0.37839197754126436, w0=-0.28191498803929216, w1=0.037392203116769517\n",
      "Gradient Descent(304/499): loss=0.37825469078048723, w0=-0.2822424781588995, w1=0.0372687219510263\n",
      "Gradient Descent(305/499): loss=0.378118737203278, w0=-0.2825666933773108, w1=0.03714570084212419\n",
      "Gradient Descent(306/499): loss=0.377984095403139, w0=-0.2828876664435379, w1=0.0370231377146594\n",
      "Gradient Descent(307/499): loss=0.37785074437900934, w0=-0.2832054297791028, w1=0.036901030530517326\n",
      "Gradient Descent(308/499): loss=0.37771866352680067, w0=-0.283520015481312, w1=0.03677937728793483\n",
      "Gradient Descent(309/499): loss=0.3775878326311278, w0=-0.28383145532649917, w1=0.03665817602058264\n",
      "Gradient Descent(310/499): loss=0.3774582318572292, w0=-0.28413978077323443, w1=0.03653742479666741\n",
      "Gradient Descent(311/499): loss=0.3773298417430711, w0=-0.28444502296550234, w1=0.03641712171805317\n",
      "Gradient Descent(312/499): loss=0.3772026431916307, w0=-0.2847472127358476, w1=0.036297264919401724\n",
      "Gradient Descent(313/499): loss=0.37707661746335674, w0=-0.28504638060848936, w1=0.036177852567331706\n",
      "Gradient Descent(314/499): loss=0.37695174616879856, w0=-0.2853425568024047, w1=0.03605888285959587\n",
      "Gradient Descent(315/499): loss=0.3768280112614035, w0=-0.28563577123438094, w1=0.03594035402427637\n",
      "Gradient Descent(316/499): loss=0.37670539503047556, w0=-0.28592605352203737, w1=0.03582226431899762\n",
      "Gradient Descent(317/499): loss=0.3765838800942935, w0=-0.2862134329868173, w1=0.035704612030156374\n",
      "Gradient Descent(318/499): loss=0.37646344939338233, w0=-0.2864979386569494, w1=0.035587395472168826\n",
      "Gradient Descent(319/499): loss=0.37634408618393594, w0=-0.28677959927038016, w1=0.03547061298673429\n",
      "Gradient Descent(320/499): loss=0.3762257740313873, w0=-0.2870584432776766, w1=0.03535426294211516\n",
      "Gradient Descent(321/499): loss=0.3761084968041199, w0=-0.2873344988449001, w1=0.03523834373243293\n",
      "Gradient Descent(322/499): loss=0.37599223866732195, w0=-0.28760779385645135, w1=0.03512285377697983\n",
      "Gradient Descent(323/499): loss=0.37587698407697406, w0=-0.2878783559178871, w1=0.035007791519545896\n",
      "Gradient Descent(324/499): loss=0.3757627177739719, w0=-0.28814621235870846, w1=0.03489315542776114\n",
      "Gradient Descent(325/499): loss=0.375649424778378, w0=-0.28841139023512163, w1=0.03477894399245248\n",
      "Gradient Descent(326/499): loss=0.37553709038379884, w0=-0.2886739163327707, w1=0.03466515572701523\n",
      "Gradient Descent(327/499): loss=0.37542570015188925, w0=-0.2889338171694432, w1=0.03455178916679881\n",
      "Gradient Descent(328/499): loss=0.37531523990697235, w0=-0.289191118997749, w1=0.03443884286850642\n",
      "Gradient Descent(329/499): loss=0.3752056957307814, w0=-0.28944584780777177, w1=0.03432631540960839\n",
      "Gradient Descent(330/499): loss=0.3750970539573146, w0=-0.2896980293296943, w1=0.034214205387769016\n",
      "Gradient Descent(331/499): loss=0.374989301167801, w0=-0.2899476890363976, w1=0.034102511420286453\n",
      "Gradient Descent(332/499): loss=0.37488242418577905, w0=-0.2901948521460339, w1=0.033991232143545616\n",
      "Gradient Descent(333/499): loss=0.3747764100722787, w0=-0.2904395436245738, w1=0.033880366212483665\n",
      "Gradient Descent(334/499): loss=0.3746712461211083, w0=-0.29068178818832835, w1=0.033769912300067935\n",
      "Gradient Descent(335/499): loss=0.3745669198542459, w0=-0.2909216103064453, w1=0.033659869096786005\n",
      "Gradient Descent(336/499): loss=0.3744634190173263, w0=-0.29115903420338113, w1=0.03355023531014769\n",
      "Gradient Descent(337/499): loss=0.3743607315752281, w0=-0.2913940838613476, w1=0.03344100966419871\n",
      "Gradient Descent(338/499): loss=0.37425884570775325, w0=-0.29162678302273437, w1=0.03333219089904582\n",
      "Gradient Descent(339/499): loss=0.3741577498054004, w0=-0.29185715519250727, w1=0.03322377777039317\n",
      "Gradient Descent(340/499): loss=0.3740574324652289, w0=-0.2920852236405825, w1=0.03311576904908963\n",
      "Gradient Descent(341/499): loss=0.3739578824868094, w0=-0.2923110114041769, w1=0.03300816352068695\n",
      "Gradient Descent(342/499): loss=0.37385908886826247, w0=-0.2925345412901354, w1=0.032900959985008436\n",
      "Gradient Descent(343/499): loss=0.37376104080238015, w0=-0.2927558358772343, w1=0.032794157255727985\n",
      "Gradient Descent(344/499): loss=0.37366372767283057, w0=-0.2929749175184622, w1=0.03268775415995929\n",
      "Gradient Descent(345/499): loss=0.3735671390504419, w0=-0.29319180834327785, w1=0.03258174953785495\n",
      "Gradient Descent(346/499): loss=0.37347126468956615, w0=-0.2934065302598453, w1=0.03247614224221534\n",
      "Gradient Descent(347/499): loss=0.37337609452451775, w0=-0.29361910495724713, w1=0.032370931138107004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(348/499): loss=0.3732816186660891, w0=-0.2938295539076749, w1=0.03226611510249042\n",
      "Gradient Descent(349/499): loss=0.37318782739813744, w0=-0.2940378983685984, w1=0.03216169302385686\n",
      "Gradient Descent(350/499): loss=0.37309471117424453, w0=-0.29424415938491266, w1=0.032057663801874296\n",
      "Gradient Descent(351/499): loss=0.3730022606144462, w0=-0.2944483577910638, w1=0.031954026347042024\n",
      "Gradient Descent(352/499): loss=0.3729104665020278, w0=-0.2946505142131534, w1=0.03185077958035392\n",
      "Gradient Descent(353/499): loss=0.37281931978039107, w0=-0.29485064907102215, w1=0.03174792243297012\n",
      "Gradient Descent(354/499): loss=0.3727288115499808, w0=-0.2950487825803122, w1=0.03164545384589697\n",
      "Gradient Descent(355/499): loss=0.3726389330652796, w0=-0.29524493475450936, w1=0.03154337276967497\n",
      "Gradient Descent(356/499): loss=0.3725496757318621, w0=-0.29543912540696454, w1=0.031441678164074775\n",
      "Gradient Descent(357/499): loss=0.37246103110351203, w0=-0.29563137415289514, w1=0.0313403689978008\n",
      "Gradient Descent(358/499): loss=0.3723729908793969, w0=-0.29582170041136646, w1=0.03123944424820251\n",
      "Gradient Descent(359/499): loss=0.37228554690130344, w0=-0.2960101234072531, w1=0.031138902900993068\n",
      "Gradient Descent(360/499): loss=0.3721986911509269, w0=-0.2961966621731808, w1=0.031038743949975325\n",
      "Gradient Descent(361/499): loss=0.3721124157472179, w0=-0.29638133555144924, w1=0.030938966396774854\n",
      "Gradient Descent(362/499): loss=0.372026712943784, w0=-0.296564162195935, w1=0.030839569250580002\n",
      "Gradient Descent(363/499): loss=0.37194157512634246, w0=-0.29674516057397593, w1=0.030740551527888738\n",
      "Gradient Descent(364/499): loss=0.3718569948102276, w0=-0.29692434896823644, w1=0.030641912252262194\n",
      "Gradient Descent(365/499): loss=0.37177296463794596, w0=-0.29710174547855434, w1=0.03054365045408472\n",
      "Gradient Descent(366/499): loss=0.37168947737678454, w0=-0.29727736802376903, w1=0.030445765170330347\n",
      "Gradient Descent(367/499): loss=0.3716065259164645, w0=-0.2974512343435316, w1=0.030348255444335497\n",
      "Gradient Descent(368/499): loss=0.3715241032668447, w0=-0.29762336200009654, w1=0.030251120325577825\n",
      "Gradient Descent(369/499): loss=0.3714422025556713, w0=-0.2977937683800958, w1=0.03015435886946104\n",
      "Gradient Descent(370/499): loss=0.37136081702637064, w0=-0.2979624706962951, w1=0.030057970137105608\n",
      "Gradient Descent(371/499): loss=0.371279940035891, w0=-0.2981294859893324, w1=0.029961953195145156\n",
      "Gradient Descent(372/499): loss=0.37119956505258317, w0=-0.29829483112943933, w1=0.029866307115528522\n",
      "Gradient Descent(373/499): loss=0.3711196856541269, w0=-0.2984585228181452, w1=0.029771030975327274\n",
      "Gradient Descent(374/499): loss=0.37104029552549783, w0=-0.298620577589964, w1=0.029676123856548588\n",
      "Gradient Descent(375/499): loss=0.37096138845697463, w0=-0.29878101181406463, w1=0.029581584845953402\n",
      "Gradient Descent(376/499): loss=0.3708829583421871, w0=-0.29893984169592425, w1=0.029487413034879683\n",
      "Gradient Descent(377/499): loss=0.3708049991762024, w0=-0.2990970832789653, w1=0.029393607519070733\n",
      "Gradient Descent(378/499): loss=0.37072750505365043, w0=-0.2992527524461759, w1=0.029300167398508388\n",
      "Gradient Descent(379/499): loss=0.3706504701668854, w0=-0.29940686492171437, w1=0.029207091777251032\n",
      "Gradient Descent(380/499): loss=0.37057388880418407, w0=-0.2995594362724975, w1=0.0291143797632763\n",
      "Gradient Descent(381/499): loss=0.3704977553479817, w0=-0.2997104819097728, w1=0.02902203046832838\n",
      "Gradient Descent(382/499): loss=0.3704220642731398, w0=-0.29986001709067533, w1=0.02893004300776979\n",
      "Gradient Descent(383/499): loss=0.37034681014525095, w0=-0.30000805691976884, w1=0.028838416500437555\n",
      "Gradient Descent(384/499): loss=0.3702719876189751, w0=-0.3001546163505714, w1=0.028747150068503664\n",
      "Gradient Descent(385/499): loss=0.3701975914364105, w0=-0.3002997101870659, w1=0.028656242837339717\n",
      "Gradient Descent(386/499): loss=0.3701236164254951, w0=-0.3004433530851955, w1=0.028565693935385675\n",
      "Gradient Descent(387/499): loss=0.3700500574984404, w0=-0.3005855595543438, w1=0.028475502494022614\n",
      "Gradient Descent(388/499): loss=0.3699769096501955, w0=-0.3007263439588006, w1=0.028385667647449382\n",
      "Gradient Descent(389/499): loss=0.3699041679569421, w0=-0.30086572051921284, w1=0.028296188532563087\n",
      "Gradient Descent(390/499): loss=0.3698318275746178, w0=-0.301003703314021, w1=0.02820706428884331\n",
      "Gradient Descent(391/499): loss=0.3697598837374694, w0=-0.301140306280881, w1=0.028118294058239984\n",
      "Gradient Descent(392/499): loss=0.36968833175663307, w0=-0.3012755432180725, w1=0.028029876985064817\n",
      "Gradient Descent(393/499): loss=0.36961716701874425, w0=-0.301409427785892, w1=0.027941812215886198\n",
      "Gradient Descent(394/499): loss=0.3695463849845724, w0=-0.30154197350803336, w1=0.02785409889942753\n",
      "Gradient Descent(395/499): loss=0.3694759811876842, w0=-0.30167319377295326, w1=0.027766736186468848\n",
      "Gradient Descent(396/499): loss=0.3694059512331305, w0=-0.30180310183522396, w1=0.0276797232297517\n",
      "Gradient Descent(397/499): loss=0.369336290796162, w0=-0.301931710816872, w1=0.027593059183887193\n",
      "Gradient Descent(398/499): loss=0.3692669956209654, w0=-0.3020590337087035, w1=0.027506743205267123\n",
      "Gradient Descent(399/499): loss=0.3691980615194285, w0=-0.30218508337161676, w1=0.02742077445197813\n",
      "Gradient Descent(400/499): loss=0.36912948436992654, w0=-0.30230987253790087, w1=0.0273351520837188\n",
      "Gradient Descent(401/499): loss=0.3690612601161311, w0=-0.30243341381252214, w1=0.027249875261719642\n",
      "Gradient Descent(402/499): loss=0.36899338476584487, w0=-0.3025557196743972, w1=0.027164943148665858\n",
      "Gradient Descent(403/499): loss=0.36892585438985653, w0=-0.3026768024776535, w1=0.027080354908622883\n",
      "Gradient Descent(404/499): loss=0.3688586651208184, w0=-0.3027966744528772, w1=0.026996109706964564\n",
      "Gradient Descent(405/499): loss=0.3687918131521453, w0=-0.3029153477083487, w1=0.026912206710303976\n",
      "Gradient Descent(406/499): loss=0.36872529473693494, w0=-0.3030328342312655, w1=0.02682864508642676\n",
      "Gradient Descent(407/499): loss=0.3686591061869088, w0=-0.3031491458889531, w1=0.026745424004226952\n",
      "Gradient Descent(408/499): loss=0.36859324387137216, w0=-0.3032642944300638, w1=0.026662542633645245\n",
      "Gradient Descent(409/499): loss=0.3685277042161964, w0=-0.30337829148576345, w1=0.026580000145609586\n",
      "Gradient Descent(410/499): loss=0.3684624837028183, w0=-0.3034911485709061, w1=0.02649779571197811\n",
      "Gradient Descent(411/499): loss=0.36839757886725893, w0=-0.3036028770851973, w1=0.02641592850548428\n",
      "Gradient Descent(412/499): loss=0.3683329862991627, w0=-0.3037134883143456, w1=0.02633439769968425\n",
      "Gradient Descent(413/499): loss=0.3682687026408523, w0=-0.3038229934312024, w1=0.026253202468906348\n",
      "Gradient Descent(414/499): loss=0.36820472458640274, w0=-0.30393140349689063, w1=0.026172341988202633\n",
      "Gradient Descent(415/499): loss=0.3681410488807341, w0=-0.30403872946192195, w1=0.026091815433302492\n",
      "Gradient Descent(416/499): loss=0.3680776723187185, w0=-0.304144982167303, w1=0.026011621980568212\n",
      "Gradient Descent(417/499): loss=0.3680145917443066, w0=-0.30425017234563023, w1=0.02593176080695247\n",
      "Gradient Descent(418/499): loss=0.36795180404966976, w0=-0.30435431062217416, w1=0.02585223108995769\n",
      "Gradient Descent(419/499): loss=0.36788930617435683, w0=-0.3044574075159527, w1=0.02577303200759726\n",
      "Gradient Descent(420/499): loss=0.3678270951044691, w0=-0.3045594734407934, w1=0.025694162738358492\n",
      "Gradient Descent(421/499): loss=0.36776516787185004, w0=-0.3046605187063858, w1=0.02561562246116734\n",
      "Gradient Descent(422/499): loss=0.3677035215532892, w0=-0.3047605535193222, w1=0.025537410355354794\n",
      "Gradient Descent(423/499): loss=0.3676421532697415, w0=-0.3048595879841292, w1=0.025459525600624915\n",
      "Gradient Descent(424/499): loss=0.3675810601855625, w0=-0.30495763210428817, w1=0.02538196737702448\n",
      "Gradient Descent(425/499): loss=0.36752023950775536, w0=-0.30505469578324557, w1=0.025304734864914184\n",
      "Gradient Descent(426/499): loss=0.3674596884852345, w0=-0.30515078882541335, w1=0.025227827244941343\n",
      "Gradient Descent(427/499): loss=0.3673994044081013, w0=-0.30524592093715947, w1=0.025151243698014077\n",
      "Gradient Descent(428/499): loss=0.36733938460693333, w0=-0.30534010172778814, w1=0.025074983405276943\n",
      "Gradient Descent(429/499): loss=0.367279626452088, w0=-0.30543334071051054, w1=0.02499904554808794\n",
      "Gradient Descent(430/499): loss=0.3672201273530177, w0=-0.3055256473034057, w1=0.024923429307996885\n",
      "Gradient Descent(431/499): loss=0.3671608847575987, w0=-0.3056170308303719, w1=0.02484813386672511\n",
      "Gradient Descent(432/499): loss=0.36710189615147204, w0=-0.3057075005220684, w1=0.02477315840614642\n",
      "Gradient Descent(433/499): loss=0.3670431590573964, w0=-0.305797065516848, w1=0.02469850210826933\n",
      "Gradient Descent(434/499): loss=0.3669846710346127, w0=-0.30588573486167975, w1=0.0246241641552205\n",
      "Gradient Descent(435/499): loss=0.3669264296782223, w0=-0.30597351751306323, w1=0.024550143729229326\n",
      "Gradient Descent(436/499): loss=0.3668684326185742, w0=-0.30606042233793285, w1=0.02447644001261371\n",
      "Gradient Descent(437/499): loss=0.36681067752066443, w0=-0.30614645811455377, w1=0.024403052187766925\n",
      "Gradient Descent(438/499): loss=0.3667531620835471, w0=-0.3062316335334085, w1=0.02432997943714555\n",
      "Gradient Descent(439/499): loss=0.3666958840397552, w0=-0.30631595719807464, w1=0.02425722094325847\n",
      "Gradient Descent(440/499): loss=0.36663884115473405, w0=-0.3063994376260942, w1=0.02418477588865688\n",
      "Gradient Descent(441/499): loss=0.36658203122628186, w0=-0.3064820832498335, w1=0.024112643455925276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(442/499): loss=0.3665254520840032, w0=-0.30656390241733544, w1=0.024040822827673423\n",
      "Gradient Descent(443/499): loss=0.36646910158877205, w0=-0.30664490339316236, w1=0.02396931318652921\n",
      "Gradient Descent(444/499): loss=0.3664129776322035, w0=-0.306725094359231, w1=0.023898113715132462\n",
      "Gradient Descent(445/499): loss=0.36635707813613605, w0=-0.30680448341563893, w1=0.02382722359612957\n",
      "Gradient Descent(446/499): loss=0.3663014010521227, w0=-0.3068830785814828, w1=0.023756642012169007\n",
      "Gradient Descent(447/499): loss=0.3662459443609333, w0=-0.3069608877956682, w1=0.023686368145897637\n",
      "Gradient Descent(448/499): loss=0.3661907060720621, w0=-0.30703791891771176, w1=0.023616401179957846\n",
      "Gradient Descent(449/499): loss=0.3661356842232476, w0=-0.3071141797285349, w1=0.02354674029698542\n",
      "Gradient Descent(450/499): loss=0.3660808768800009, w0=-0.3071896779312498, w1=0.02347738467960817\n",
      "Gradient Descent(451/499): loss=0.36602628213513944, w0=-0.30726442115193753, w1=0.023408333510445287\n",
      "Gradient Descent(452/499): loss=0.3659718981083336, w0=-0.3073384169404184, w1=0.0233395859721074\n",
      "Gradient Descent(453/499): loss=0.365917722945658, w0=-0.3074116727710145, w1=0.023271141247197293\n",
      "Gradient Descent(454/499): loss=0.36586375481915273, w0=-0.3074841960433046, w1=0.02320299851831128\n",
      "Gradient Descent(455/499): loss=0.3658099919263924, w0=-0.3075559940828718, w1=0.02313515696804121\n",
      "Gradient Descent(456/499): loss=0.3657564324900608, w0=-0.3076270741420434, w1=0.023067615778977098\n",
      "Gradient Descent(457/499): loss=0.3657030747575372, w0=-0.3076974434006232, w1=0.02300037413371032\n",
      "Gradient Descent(458/499): loss=0.36564991700048655, w0=-0.30776710896661724, w1=0.0229334312148374\n",
      "Gradient Descent(459/499): loss=0.3655969575144581, w0=-0.30783607787695133, w1=0.02286678620496431\n",
      "Gradient Descent(460/499): loss=0.3655441946184924, w0=-0.3079043570981821, w1=0.022800438286711352\n",
      "Gradient Descent(461/499): loss=0.36549162665473345, w0=-0.3079719535272005, w1=0.022734386642718506\n",
      "Gradient Descent(462/499): loss=0.36543925198804944, w0=-0.30803887399192875, w1=0.022668630455651283\n",
      "Gradient Descent(463/499): loss=0.36538706900565837, w0=-0.3081051252520097, w1=0.02260316890820707\n",
      "Gradient Descent(464/499): loss=0.36533507611676347, w0=-0.30817071399948986, w1=0.022538001183121898\n",
      "Gradient Descent(465/499): loss=0.36528327175219105, w0=-0.3082356468594952, w1=0.022473126463177687\n",
      "Gradient Descent(466/499): loss=0.3652316543640385, w0=-0.30829993039090053, w1=0.022408543931209876\n",
      "Gradient Descent(467/499): loss=0.365180222425326, w0=-0.3083635710869918, w1=0.022344252770115498\n",
      "Gradient Descent(468/499): loss=0.3651289744296559, w0=-0.30842657537612217, w1=0.022280252162861603\n",
      "Gradient Descent(469/499): loss=0.3650779088908773, w0=-0.30848894962236123, w1=0.0222165412924941\n",
      "Gradient Descent(470/499): loss=0.36502702434275747, w0=-0.3085507001261379, w1=0.02215311934214693\n",
      "Gradient Descent(471/499): loss=0.3649763193386581, w0=-0.3086118331248768, w1=0.022089985495051583\n",
      "Gradient Descent(472/499): loss=0.3649257924512169, w0=-0.30867235479362826, w1=0.02202713893454696\n",
      "Gradient Descent(473/499): loss=0.36487544227203833, w0=-0.30873227124569225, w1=0.021964578844089547\n",
      "Gradient Descent(474/499): loss=0.36482526741138377, w0=-0.3087915885332356, w1=0.021902304407263887\n",
      "Gradient Descent(475/499): loss=0.3647752664978724, w0=-0.30885031264790347, w1=0.02184031480779335\n",
      "Gradient Descent(476/499): loss=0.3647254381781855, w0=-0.3089084495214247, w1=0.02177860922955116\n",
      "Gradient Descent(477/499): loss=0.3646757811167751, w0=-0.30896600502621074, w1=0.02171718685657172\n",
      "Gradient Descent(478/499): loss=0.3646262939955792, w0=-0.3090229849759489, w1=0.02165604687306215\n",
      "Gradient Descent(479/499): loss=0.3645769755137408, w0=-0.30907939512618965, w1=0.021595188463414083\n",
      "Gradient Descent(480/499): loss=0.3645278243873328, w0=-0.309135241174928, w1=0.021534610812215704\n",
      "Gradient Descent(481/499): loss=0.3644788393490872, w0=-0.309190528763179, w1=0.021474313104263967\n",
      "Gradient Descent(482/499): loss=0.36443001914812895, w0=-0.3092452634755474, w1=0.021414294524577052\n",
      "Gradient Descent(483/499): loss=0.3643813625497152, w0=-0.3092994508407922, w1=0.021354554258406995\n",
      "Gradient Descent(484/499): loss=0.36433286833497786, w0=-0.3093530963323845, w1=0.021295091491252516\n",
      "Gradient Descent(485/499): loss=0.36428453530067206, w0=-0.30940620536906094, w1=0.021235905408872\n",
      "Gradient Descent(486/499): loss=0.36423636225892775, w0=-0.3094587833153706, w1=0.02117699519729667\n",
      "Gradient Descent(487/499): loss=0.3641883480370061, w0=-0.30951083548221714, w1=0.02111836004284388\n",
      "Gradient Descent(488/499): loss=0.36414049147706135, w0=-0.3095623671273952, w1=0.02105999913213058\n",
      "Gradient Descent(489/499): loss=0.3640927914359037, w0=-0.3096133834561215, w1=0.021001911652086896\n",
      "Gradient Descent(490/499): loss=0.36404524678477, w0=-0.30966388962156055, w1=0.020944096789969856\n",
      "Gradient Descent(491/499): loss=0.363997856409096, w0=-0.3097138907253452, w1=0.0208865537333772\n",
      "Gradient Descent(492/499): loss=0.3639506192082931, w0=-0.30976339181809204, w1=0.020829281670261338\n",
      "Gradient Descent(493/499): loss=0.363903534095529, w0=-0.3098123978999114, w1=0.020772279788943377\n",
      "Gradient Descent(494/499): loss=0.3638565999975132, w0=-0.30986091392091253, w1=0.020715547278127255\n",
      "Gradient Descent(495/499): loss=0.3638098158542834, w0=-0.30990894478170367, w1=0.020659083326913952\n",
      "Gradient Descent(496/499): loss=0.36376318061899976, w0=-0.30995649533388686, w1=0.020602887124815786\n",
      "Gradient Descent(497/499): loss=0.3637166932577377, w0=-0.31000357038054827, w1=0.020546957861770763\n",
      "Gradient Descent(498/499): loss=0.3636703527492896, w0=-0.31005017467674306, w1=0.020491294728157007\n",
      "Gradient Descent(499/499): loss=0.3636241580849651, w0=-0.3100963129299759, w1=0.020435896914807232\n"
     ]
    }
   ],
   "source": [
    "w = np.array(np.random.rand((tX.shape[1]))) #initializing the weights to zero\n",
    "max_iters = 500 #choosing number of iterations\n",
    "gamma = 5/max_iters #choosing gamma \n",
    "(w, loss) = least_squares_GD(y,tX,w,max_iters,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss - loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
